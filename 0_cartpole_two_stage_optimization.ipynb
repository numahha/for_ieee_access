{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f49dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "import datetime\n",
    "import torch\n",
    "import gym\n",
    "import custom_gym\n",
    "import random\n",
    "from config import cfg_seed, cfg_env, cfg_z_dim, cfg_default_lr, cfg_default_early\n",
    "seed = cfg_seed\n",
    "env_str=cfg_env\n",
    "default_lr=cfg_default_lr\n",
    "default_early=cfg_default_early\n",
    "\n",
    "if cfg_env == \"pendulum\":\n",
    "    env_name = \"CustomPendulum-v0\"\n",
    "if cfg_env == \"cartpole\":\n",
    "    env_name = \"CustomCartPole-v0\"\n",
    "\n",
    "figfilenamehead = \"fig_policy_evaluation_\"+cfg_env+\"_\"\n",
    "\n",
    "num_iter_max=200000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf77b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "s_dim = env.reset().flatten().shape[0]\n",
    "a_dim = env.action_space.sample().flatten().shape[0]\n",
    "z_dim = cfg_z_dim\n",
    "print(s_dim, a_dim, z_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7c5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sac import SAC\n",
    "# agent = SAC(env.observation_space.shape[0], env.action_space)\n",
    "# agent.load_checkpoint(ckpt_path=\"checkpoints/sac_checkpoint_custom_\"+env_str+\"_mdp_\", evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9c09c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# データ生成\n",
    "from get_offline_data import getOfflineData\n",
    "getOfflineData(env_name=env_name, episode_num=100, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d57437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offline_data[1].sum() tensor(1771.3790)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "offline_data = pickle.load(open(\"offline_data_\"+env_str+\".pkl\",\"rb\"))\n",
    "\n",
    "debug_info = pickle.load(open(\"offline_data_debug_info_\"+env_str+\".pkl\",\"rb\"))\n",
    "debug_info = np.array(debug_info)\n",
    "print(\"offline_data[1].sum()\", offline_data[1].sum())\n",
    "print(debug_info.shape)\n",
    "# c_array = debug_info[:,1]\n",
    "\n",
    "# episode_index = 9\n",
    "# plt.plot(offline_data[episode_index][:,0],offline_data[episode_index][:,1])\n",
    "# print(\"env_param: \",debug_info[episode_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767b8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec: h_dim 48\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(vi_base)\n",
    "import vi_base\n",
    "\n",
    "args_init_dict = {\n",
    "    \"offline_data\": offline_data,\n",
    "    \"s_dim\": s_dim,\n",
    "    \"a_dim\": a_dim,\n",
    "    \"z_dim\": z_dim,\n",
    "    #              \"policy\":agent.select_action,\n",
    "    \"mdp_policy\":None,\n",
    "    \"bamdp_policy\":None,\n",
    "    \"debug_info\":debug_info,\n",
    "    \"env\" : env,\n",
    "    \"ckpt_suffix\" : env_str,\n",
    "    }\n",
    "\n",
    "vi = vi_base.baseVI(args_init_dict)\n",
    "# print(np.random.randn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c06f958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vae: enc_dec\n",
      "train: iter 0  trainloss 682.03392  validloss 1003.36592±0.00000  bestvalidloss 1003.36592  last_update 0\n",
      "train: iter 1  trainloss 219.36365  validloss 899.99265±0.00000  bestvalidloss 899.99265  last_update 0\n",
      "train: iter 2  trainloss -14.13473  validloss 758.67945±0.00000  bestvalidloss 758.67945  last_update 0\n",
      "train: iter 3  trainloss -322.77369  validloss -127.03998±0.00000  bestvalidloss -127.03998  last_update 0\n",
      "train: iter 4  trainloss -543.87513  validloss -315.40305±0.00000  bestvalidloss -315.40305  last_update 0\n",
      "train: iter 5  trainloss -726.03220  validloss -514.09119±0.00000  bestvalidloss -514.09119  last_update 0\n",
      "train: iter 6  trainloss -784.12798  validloss -611.07515±0.00000  bestvalidloss -611.07515  last_update 0\n",
      "train: iter 7  trainloss -659.49106  validloss -697.26160±0.00000  bestvalidloss -697.26160  last_update 0\n",
      "train: iter 8  trainloss -942.96487  validloss -719.93590±0.00000  bestvalidloss -719.93590  last_update 0\n",
      "train: iter 9  trainloss -922.67801  validloss -496.61725±0.00000  bestvalidloss -719.93590  last_update 1\n",
      "train: iter 10  trainloss -1066.94343  validloss -901.49264±0.00000  bestvalidloss -901.49264  last_update 0\n",
      "train: iter 11  trainloss -1097.97930  validloss -928.56062±0.00000  bestvalidloss -928.56062  last_update 0\n",
      "train: iter 12  trainloss -1102.96248  validloss -974.24190±0.00000  bestvalidloss -974.24190  last_update 0\n",
      "train: iter 13  trainloss -1058.08688  validloss -747.71969±0.00000  bestvalidloss -974.24190  last_update 1\n",
      "train: iter 14  trainloss -1216.00565  validloss -1064.39971±0.00000  bestvalidloss -1064.39971  last_update 0\n",
      "train: iter 15  trainloss -1235.48699  validloss -1080.72198±0.00000  bestvalidloss -1080.72198  last_update 0\n",
      "train: iter 16  trainloss -1317.99559  validloss -1038.65050±0.00000  bestvalidloss -1080.72198  last_update 1\n",
      "train: iter 17  trainloss -1270.69366  validloss -1131.83319±0.00000  bestvalidloss -1131.83319  last_update 0\n",
      "train: iter 18  trainloss -1405.15788  validloss -1144.87877±0.00000  bestvalidloss -1144.87877  last_update 0\n",
      "train: iter 19  trainloss -1423.70107  validloss -1148.40793±0.00000  bestvalidloss -1148.40793  last_update 0\n",
      "train: iter 20  trainloss -1392.63221  validloss -1251.61911±0.00000  bestvalidloss -1251.61911  last_update 0\n",
      "train: iter 21  trainloss -1456.46280  validloss -1225.51199±0.00000  bestvalidloss -1251.61911  last_update 1\n",
      "train: iter 22  trainloss -1499.69823  validloss -1308.98694±0.00000  bestvalidloss -1308.98694  last_update 0\n",
      "train: iter 23  trainloss -1489.48940  validloss -1278.94646±0.00000  bestvalidloss -1308.98694  last_update 1\n",
      "train: iter 24  trainloss -1483.96826  validloss -1385.12180±0.00000  bestvalidloss -1385.12180  last_update 0\n",
      "train: iter 25  trainloss -1506.86677  validloss -1009.23061±0.00000  bestvalidloss -1385.12180  last_update 1\n",
      "train: iter 26  trainloss -1573.75501  validloss -1380.88679±0.00000  bestvalidloss -1385.12180  last_update 2\n",
      "train: iter 27  trainloss -1546.64159  validloss -1329.94476±0.00000  bestvalidloss -1385.12180  last_update 3\n",
      "train: iter 28  trainloss -1588.52717  validloss -1350.51379±0.00000  bestvalidloss -1385.12180  last_update 4\n",
      "train: iter 29  trainloss -1562.16205  validloss -1414.03802±0.00000  bestvalidloss -1414.03802  last_update 0\n",
      "train: iter 30  trainloss -1255.77453  validloss -1431.10768±0.00000  bestvalidloss -1431.10768  last_update 0\n",
      "train: iter 31  trainloss -1603.84779  validloss -1297.16085±0.00000  bestvalidloss -1431.10768  last_update 1\n",
      "train: iter 32  trainloss -1612.75689  validloss -1416.47800±0.00000  bestvalidloss -1431.10768  last_update 2\n",
      "train: iter 33  trainloss -1648.20956  validloss -1497.15575±0.00000  bestvalidloss -1497.15575  last_update 0\n",
      "train: iter 34  trainloss -1648.70115  validloss -1434.95027±0.00000  bestvalidloss -1497.15575  last_update 1\n",
      "train: iter 35  trainloss -1654.54981  validloss -1502.70583±0.00000  bestvalidloss -1502.70583  last_update 0\n",
      "train: iter 36  trainloss -1644.95353  validloss -1259.82464±0.00000  bestvalidloss -1502.70583  last_update 1\n",
      "train: iter 37  trainloss -1668.07863  validloss -1459.92210±0.00000  bestvalidloss -1502.70583  last_update 2\n",
      "train: iter 38  trainloss -1699.14796  validloss -1516.01475±0.00000  bestvalidloss -1516.01475  last_update 0\n",
      "train: iter 39  trainloss -1615.88416  validloss -1455.49442±0.00000  bestvalidloss -1516.01475  last_update 1\n",
      "train: iter 40  trainloss -1624.43624  validloss -1405.00433±0.00000  bestvalidloss -1516.01475  last_update 2\n",
      "train: iter 41  trainloss -1650.33587  validloss -1489.28829±0.00000  bestvalidloss -1516.01475  last_update 3\n",
      "train: iter 42  trainloss -1731.21531  validloss -1485.55925±0.00000  bestvalidloss -1516.01475  last_update 4\n",
      "train: iter 43  trainloss -1733.11267  validloss -1470.03388±0.00000  bestvalidloss -1516.01475  last_update 5\n",
      "train: iter 44  trainloss -1771.61753  validloss -1573.37646±0.00000  bestvalidloss -1573.37646  last_update 0\n",
      "train: iter 45  trainloss -1671.70421  validloss -1572.14858±0.00000  bestvalidloss -1573.37646  last_update 1\n",
      "train: iter 46  trainloss -1773.36347  validloss -1339.08824±0.00000  bestvalidloss -1573.37646  last_update 2\n",
      "train: iter 47  trainloss -1738.68015  validloss -1473.51666±0.00000  bestvalidloss -1573.37646  last_update 3\n",
      "train: iter 48  trainloss -1673.97112  validloss -1450.94534±0.00000  bestvalidloss -1573.37646  last_update 4\n",
      "train: iter 49  trainloss -1774.42536  validloss -1374.23266±0.00000  bestvalidloss -1573.37646  last_update 5\n",
      "train: iter 50  trainloss -1777.19781  validloss -1552.03265±0.00000  bestvalidloss -1573.37646  last_update 6\n",
      "train: iter 51  trainloss -1755.63178  validloss -1525.00249±0.00000  bestvalidloss -1573.37646  last_update 7\n",
      "train: iter 52  trainloss -1817.31662  validloss -1579.15430±0.00000  bestvalidloss -1579.15430  last_update 0\n",
      "train: iter 53  trainloss -1841.77260  validloss -1614.16671±0.00000  bestvalidloss -1614.16671  last_update 0\n",
      "train: iter 54  trainloss -1833.23599  validloss -1620.29548±0.00000  bestvalidloss -1620.29548  last_update 0\n",
      "train: iter 55  trainloss -1626.85664  validloss -1545.18485±0.00000  bestvalidloss -1620.29548  last_update 1\n",
      "train: iter 56  trainloss -1771.90829  validloss -1497.12277±0.00000  bestvalidloss -1620.29548  last_update 2\n",
      "train: iter 57  trainloss -1824.36304  validloss -1486.27877±0.00000  bestvalidloss -1620.29548  last_update 3\n",
      "train: iter 58  trainloss -1876.59654  validloss -1582.61313±0.00000  bestvalidloss -1620.29548  last_update 4\n",
      "train: iter 59  trainloss -1861.85931  validloss -1660.07687±0.00000  bestvalidloss -1660.07687  last_update 0\n",
      "train: iter 60  trainloss -1854.81121  validloss -1593.67942±0.00000  bestvalidloss -1660.07687  last_update 1\n",
      "train: iter 61  trainloss -1839.91967  validloss -1586.77816±0.00000  bestvalidloss -1660.07687  last_update 2\n",
      "train: iter 62  trainloss -1781.38591  validloss -1674.20053±0.00000  bestvalidloss -1674.20053  last_update 0\n",
      "train: iter 63  trainloss -1780.08655  validloss -1597.18813±0.00000  bestvalidloss -1674.20053  last_update 1\n",
      "train: iter 64  trainloss -1896.56922  validloss -1625.06651±0.00000  bestvalidloss -1674.20053  last_update 2\n",
      "train: iter 65  trainloss -1913.23249  validloss -1626.64382±0.00000  bestvalidloss -1674.20053  last_update 3\n",
      "train: iter 66  trainloss -1929.56679  validloss -1678.50215±0.00000  bestvalidloss -1678.50215  last_update 0\n",
      "train: iter 67  trainloss -1831.99859  validloss -1699.89441±0.00000  bestvalidloss -1699.89441  last_update 0\n",
      "train: iter 68  trainloss -1782.98015  validloss -1623.37385±0.00000  bestvalidloss -1699.89441  last_update 1\n",
      "train: iter 69  trainloss -1908.34418  validloss -1636.16272±0.00000  bestvalidloss -1699.89441  last_update 2\n",
      "train: iter 70  trainloss -1866.88069  validloss -1633.92225±0.00000  bestvalidloss -1699.89441  last_update 3\n",
      "train: iter 71  trainloss -1920.00233  validloss -1643.97925±0.00000  bestvalidloss -1699.89441  last_update 4\n",
      "train: iter 72  trainloss -1884.04509  validloss -1604.50533±0.00000  bestvalidloss -1699.89441  last_update 5\n",
      "train: iter 73  trainloss -1903.57934  validloss -1599.62300±0.00000  bestvalidloss -1699.89441  last_update 6\n",
      "train: iter 74  trainloss -1766.15665  validloss -1585.66115±0.00000  bestvalidloss -1699.89441  last_update 7\n",
      "train: iter 75  trainloss -1906.87953  validloss -1624.81491±0.00000  bestvalidloss -1699.89441  last_update 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 76  trainloss -1888.96090  validloss -1674.47191±0.00000  bestvalidloss -1699.89441  last_update 9\n",
      "train: iter 77  trainloss -1880.26901  validloss -1714.00986±0.00000  bestvalidloss -1714.00986  last_update 0\n",
      "train: iter 78  trainloss -1895.45163  validloss -1489.33383±0.00000  bestvalidloss -1714.00986  last_update 1\n",
      "train: iter 79  trainloss -1931.57691  validloss -1720.84748±0.00000  bestvalidloss -1720.84748  last_update 0\n",
      "train: iter 80  trainloss -1965.22980  validloss -1718.64826±0.00000  bestvalidloss -1720.84748  last_update 1\n",
      "train: iter 81  trainloss -1940.35887  validloss -1744.66918±0.00000  bestvalidloss -1744.66918  last_update 0\n",
      "train: iter 82  trainloss -1942.31918  validloss -1573.92587±0.00000  bestvalidloss -1744.66918  last_update 1\n",
      "train: iter 83  trainloss -1979.81920  validloss -1796.67866±0.00000  bestvalidloss -1796.67866  last_update 0\n",
      "train: iter 84  trainloss -1928.47016  validloss -1788.89837±0.00000  bestvalidloss -1796.67866  last_update 1\n",
      "train: iter 85  trainloss -1786.22599  validloss -1718.40034±0.00000  bestvalidloss -1796.67866  last_update 2\n",
      "train: iter 86  trainloss -733.36829  validloss 1192.54807±0.00000  bestvalidloss -1796.67866  last_update 3\n",
      "train: iter 87  trainloss -1432.10910  validloss -951.83975±0.00000  bestvalidloss -1796.67866  last_update 4\n",
      "train: iter 88  trainloss -1904.69923  validloss -1660.46135±0.00000  bestvalidloss -1796.67866  last_update 5\n",
      "train: iter 89  trainloss -1955.97821  validloss -1774.29876±0.00000  bestvalidloss -1796.67866  last_update 6\n",
      "train: iter 90  trainloss -1977.77609  validloss -1776.93014±0.00000  bestvalidloss -1796.67866  last_update 7\n",
      "train: iter 91  trainloss -1989.52236  validloss -1822.21118±0.00000  bestvalidloss -1822.21118  last_update 0\n",
      "train: iter 92  trainloss -1969.76818  validloss -1780.04354±0.00000  bestvalidloss -1822.21118  last_update 1\n",
      "train: iter 93  trainloss -1980.30263  validloss -1731.34575±0.00000  bestvalidloss -1822.21118  last_update 2\n",
      "train: iter 94  trainloss -1955.48937  validloss -1808.33782±0.00000  bestvalidloss -1822.21118  last_update 3\n",
      "train: iter 95  trainloss -1947.60401  validloss -1744.29639±0.00000  bestvalidloss -1822.21118  last_update 4\n",
      "train: iter 96  trainloss -1963.58845  validloss -1784.92197±0.00000  bestvalidloss -1822.21118  last_update 5\n",
      "train: iter 97  trainloss -1959.73256  validloss -1740.63525±0.00000  bestvalidloss -1822.21118  last_update 6\n",
      "train: iter 98  trainloss -1981.49716  validloss -1684.59274±0.00000  bestvalidloss -1822.21118  last_update 7\n",
      "train: iter 99  trainloss -1876.53104  validloss -1802.14831±0.00000  bestvalidloss -1822.21118  last_update 8\n",
      "train: iter 100  trainloss -1862.33277  validloss -1813.35923±0.00000  bestvalidloss -1822.21118  last_update 9\n",
      "train: iter 101  trainloss -1907.50812  validloss -1565.27269±0.00000  bestvalidloss -1822.21118  last_update 10\n",
      "train: iter 102  trainloss -1945.34207  validloss -1729.63144±0.00000  bestvalidloss -1822.21118  last_update 11\n",
      "train: iter 103  trainloss -1978.16459  validloss -1820.55215±0.00000  bestvalidloss -1822.21118  last_update 12\n",
      "train: iter 104  trainloss -2001.68362  validloss -1830.63808±0.00000  bestvalidloss -1830.63808  last_update 0\n",
      "train: iter 105  trainloss -1991.64610  validloss -1822.74002±0.00000  bestvalidloss -1830.63808  last_update 1\n",
      "train: iter 106  trainloss -1993.45220  validloss -1857.32052±0.00000  bestvalidloss -1857.32052  last_update 0\n",
      "train: iter 107  trainloss -1905.92761  validloss -1738.40534±0.00000  bestvalidloss -1857.32052  last_update 1\n",
      "train: iter 108  trainloss -2003.79862  validloss -1657.93067±0.00000  bestvalidloss -1857.32052  last_update 2\n",
      "train: iter 109  trainloss -1926.46680  validloss -1774.45872±0.00000  bestvalidloss -1857.32052  last_update 3\n",
      "train: iter 110  trainloss -1957.42944  validloss -1738.66282±0.00000  bestvalidloss -1857.32052  last_update 4\n",
      "train: iter 111  trainloss -1970.04046  validloss -1807.25847±0.00000  bestvalidloss -1857.32052  last_update 5\n",
      "train: iter 112  trainloss -1949.48750  validloss -1831.55465±0.00000  bestvalidloss -1857.32052  last_update 6\n",
      "train: iter 113  trainloss -1899.46602  validloss -1763.78393±0.00000  bestvalidloss -1857.32052  last_update 7\n",
      "train: iter 114  trainloss -1991.88933  validloss -1766.65776±0.00000  bestvalidloss -1857.32052  last_update 8\n",
      "train: iter 115  trainloss -1965.99546  validloss -1828.13432±0.00000  bestvalidloss -1857.32052  last_update 9\n",
      "train: iter 116  trainloss -1995.70531  validloss -1851.51574±0.00000  bestvalidloss -1857.32052  last_update 10\n",
      "train: iter 117  trainloss -1940.61328  validloss -1723.06730±0.00000  bestvalidloss -1857.32052  last_update 11\n",
      "train: iter 118  trainloss -1960.17267  validloss -1806.10154±0.00000  bestvalidloss -1857.32052  last_update 12\n",
      "train: iter 119  trainloss -1932.42241  validloss -1863.42229±0.00000  bestvalidloss -1863.42229  last_update 0\n",
      "train: iter 120  trainloss -1944.48598  validloss -1841.14540±0.00000  bestvalidloss -1863.42229  last_update 1\n",
      "train: iter 121  trainloss -2011.20064  validloss -1855.10636±0.00000  bestvalidloss -1863.42229  last_update 2\n",
      "train: iter 122  trainloss -1971.75809  validloss -1725.43404±0.00000  bestvalidloss -1863.42229  last_update 3\n",
      "train: iter 123  trainloss -2009.18093  validloss -1853.29768±0.00000  bestvalidloss -1863.42229  last_update 4\n",
      "train: iter 124  trainloss -1974.14149  validloss -1772.76754±0.00000  bestvalidloss -1863.42229  last_update 5\n",
      "train: iter 125  trainloss -1972.78492  validloss -1769.67672±0.00000  bestvalidloss -1863.42229  last_update 6\n",
      "train: iter 126  trainloss -1998.34959  validloss -1846.95961±0.00000  bestvalidloss -1863.42229  last_update 7\n",
      "train: iter 127  trainloss -2007.83070  validloss -1846.18418±0.00000  bestvalidloss -1863.42229  last_update 8\n",
      "train: iter 128  trainloss -2019.99295  validloss -1824.14625±0.00000  bestvalidloss -1863.42229  last_update 9\n",
      "train: iter 129  trainloss -2040.49406  validloss -1886.71679±0.00000  bestvalidloss -1886.71679  last_update 0\n",
      "train: iter 130  trainloss -2005.97867  validloss -1785.12340±0.00000  bestvalidloss -1886.71679  last_update 1\n",
      "train: iter 131  trainloss -1924.15605  validloss -1811.69985±0.00000  bestvalidloss -1886.71679  last_update 2\n",
      "train: iter 132  trainloss -1975.41310  validloss -1837.98803±0.00000  bestvalidloss -1886.71679  last_update 3\n",
      "train: iter 133  trainloss -2018.33484  validloss -1820.31963±0.00000  bestvalidloss -1886.71679  last_update 4\n",
      "train: iter 134  trainloss -2032.94468  validloss -1882.79701±0.00000  bestvalidloss -1886.71679  last_update 5\n",
      "train: iter 135  trainloss -2021.43584  validloss -1843.37350±0.00000  bestvalidloss -1886.71679  last_update 6\n",
      "train: iter 136  trainloss -2016.77844  validloss -1781.87174±0.00000  bestvalidloss -1886.71679  last_update 7\n",
      "train: iter 137  trainloss -1932.74752  validloss -1810.06257±0.00000  bestvalidloss -1886.71679  last_update 8\n",
      "train: iter 138  trainloss -2011.76115  validloss -1882.14751±0.00000  bestvalidloss -1886.71679  last_update 9\n",
      "train: iter 139  trainloss -2002.61302  validloss -1877.44551±0.00000  bestvalidloss -1886.71679  last_update 10\n",
      "train: iter 140  trainloss -1901.06654  validloss -1854.39182±0.00000  bestvalidloss -1886.71679  last_update 11\n",
      "train: iter 141  trainloss -2003.97016  validloss -1797.55379±0.00000  bestvalidloss -1886.71679  last_update 12\n",
      "train: iter 142  trainloss -1981.87708  validloss -1791.14466±0.00000  bestvalidloss -1886.71679  last_update 13\n",
      "train: iter 143  trainloss -2039.00415  validloss -1886.90255±0.00000  bestvalidloss -1886.90255  last_update 0\n",
      "train: iter 144  trainloss -2059.21338  validloss -1887.64983±0.00000  bestvalidloss -1887.64983  last_update 0\n",
      "train: iter 145  trainloss -2013.68971  validloss -1905.37813±0.00000  bestvalidloss -1905.37813  last_update 0\n",
      "train: iter 146  trainloss -2041.32647  validloss -1893.50865±0.00000  bestvalidloss -1905.37813  last_update 1\n",
      "train: iter 147  trainloss -1956.44973  validloss -1868.55331±0.00000  bestvalidloss -1905.37813  last_update 2\n",
      "train: iter 148  trainloss -2014.22739  validloss -1825.97095±0.00000  bestvalidloss -1905.37813  last_update 3\n",
      "train: iter 149  trainloss -2044.90951  validloss -1898.86904±0.00000  bestvalidloss -1905.37813  last_update 4\n",
      "train: iter 150  trainloss -2015.14209  validloss -1767.71770±0.00000  bestvalidloss -1905.37813  last_update 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 151  trainloss -1964.93866  validloss -1577.67768±0.00000  bestvalidloss -1905.37813  last_update 6\n",
      "train: iter 152  trainloss -2054.13115  validloss -1862.93017±0.00000  bestvalidloss -1905.37813  last_update 7\n",
      "train: iter 153  trainloss -2052.31664  validloss -1912.96824±0.00000  bestvalidloss -1912.96824  last_update 0\n",
      "train: iter 154  trainloss -2036.74040  validloss -1933.37368±0.00000  bestvalidloss -1933.37368  last_update 0\n",
      "train: iter 155  trainloss -2039.87981  validloss -1906.10079±0.00000  bestvalidloss -1933.37368  last_update 1\n",
      "train: iter 156  trainloss -2044.07268  validloss -1923.32144±0.00000  bestvalidloss -1933.37368  last_update 2\n",
      "train: iter 157  trainloss -2000.38577  validloss -1821.53442±0.00000  bestvalidloss -1933.37368  last_update 3\n",
      "train: iter 158  trainloss -2031.36374  validloss -1892.68667±0.00000  bestvalidloss -1933.37368  last_update 4\n",
      "train: iter 159  trainloss -1958.40039  validloss -1891.22162±0.00000  bestvalidloss -1933.37368  last_update 5\n",
      "train: iter 160  trainloss -2018.88699  validloss -1825.68143±0.00000  bestvalidloss -1933.37368  last_update 6\n",
      "train: iter 161  trainloss -2058.49106  validloss -1846.00272±0.00000  bestvalidloss -1933.37368  last_update 7\n",
      "train: iter 162  trainloss -2050.92170  validloss -1896.72254±0.00000  bestvalidloss -1933.37368  last_update 8\n",
      "train: iter 163  trainloss -1951.03312  validloss -1924.36546±0.00000  bestvalidloss -1933.37368  last_update 9\n",
      "train: iter 164  trainloss -2036.57335  validloss -1712.53856±0.00000  bestvalidloss -1933.37368  last_update 10\n",
      "train: iter 165  trainloss -2037.54685  validloss -1925.83988±0.00000  bestvalidloss -1933.37368  last_update 11\n",
      "train: iter 166  trainloss -2047.25334  validloss -1839.29656±0.00000  bestvalidloss -1933.37368  last_update 12\n",
      "train: iter 167  trainloss -2047.41910  validloss -1840.83556±0.00000  bestvalidloss -1933.37368  last_update 13\n",
      "train: iter 168  trainloss -2051.47324  validloss -1931.01052±0.00000  bestvalidloss -1933.37368  last_update 14\n",
      "train: iter 169  trainloss -2043.40782  validloss -1769.67435±0.00000  bestvalidloss -1933.37368  last_update 15\n",
      "train: iter 170  trainloss -1972.26127  validloss -1870.93404±0.00000  bestvalidloss -1933.37368  last_update 16\n",
      "train: iter 171  trainloss -1951.54942  validloss -1570.54026±0.00000  bestvalidloss -1933.37368  last_update 17\n",
      "train: iter 172  trainloss -1950.23602  validloss -1661.12338±0.00000  bestvalidloss -1933.37368  last_update 18\n",
      "train: iter 173  trainloss -2071.60261  validloss -1888.19895±0.00000  bestvalidloss -1933.37368  last_update 19\n",
      "train: iter 174  trainloss -2090.52343  validloss -1885.79208±0.00000  bestvalidloss -1933.37368  last_update 20\n",
      "train: iter 175  trainloss -2040.50426  validloss -1958.12408±0.00000  bestvalidloss -1958.12408  last_update 0\n",
      "train: iter 176  trainloss -2068.85193  validloss -1924.59304±0.00000  bestvalidloss -1958.12408  last_update 1\n",
      "train: iter 177  trainloss -2095.40311  validloss -1911.19394±0.00000  bestvalidloss -1958.12408  last_update 2\n",
      "train: iter 178  trainloss -2065.92257  validloss -1939.20404±0.00000  bestvalidloss -1958.12408  last_update 3\n",
      "train: iter 179  trainloss -2046.95113  validloss -1922.32613±0.00000  bestvalidloss -1958.12408  last_update 4\n",
      "train: iter 180  trainloss -1998.12947  validloss -1757.10603±0.00000  bestvalidloss -1958.12408  last_update 5\n",
      "train: iter 181  trainloss -2054.72721  validloss -1756.54035±0.00000  bestvalidloss -1958.12408  last_update 6\n",
      "train: iter 182  trainloss -2086.26764  validloss -1967.09030±0.00000  bestvalidloss -1967.09030  last_update 0\n",
      "train: iter 183  trainloss -2031.67331  validloss -1983.40413±0.00000  bestvalidloss -1983.40413  last_update 0\n",
      "train: iter 184  trainloss -2058.16450  validloss -1969.07981±0.00000  bestvalidloss -1983.40413  last_update 1\n",
      "train: iter 185  trainloss -2069.36020  validloss -1959.98837±0.00000  bestvalidloss -1983.40413  last_update 2\n",
      "train: iter 186  trainloss -2047.83812  validloss -1842.34496±0.00000  bestvalidloss -1983.40413  last_update 3\n",
      "train: iter 187  trainloss -2095.44921  validloss -1904.02516±0.00000  bestvalidloss -1983.40413  last_update 4\n",
      "train: iter 188  trainloss -2011.38691  validloss -1870.03303±0.00000  bestvalidloss -1983.40413  last_update 5\n",
      "train: iter 189  trainloss -2060.47217  validloss -1868.83466±0.00000  bestvalidloss -1983.40413  last_update 6\n",
      "train: iter 190  trainloss -2064.33813  validloss -1957.44045±0.00000  bestvalidloss -1983.40413  last_update 7\n",
      "train: iter 191  trainloss -2031.85112  validloss -1936.24064±0.00000  bestvalidloss -1983.40413  last_update 8\n",
      "train: iter 192  trainloss -2085.12537  validloss -1971.17088±0.00000  bestvalidloss -1983.40413  last_update 9\n",
      "train: iter 193  trainloss -2100.10709  validloss -1942.98925±0.00000  bestvalidloss -1983.40413  last_update 10\n",
      "train: iter 194  trainloss -2034.16205  validloss -1923.26570±0.00000  bestvalidloss -1983.40413  last_update 11\n",
      "train: iter 195  trainloss -2057.10229  validloss -1847.05811±0.00000  bestvalidloss -1983.40413  last_update 12\n",
      "train: iter 196  trainloss -2069.23542  validloss -1956.28046±0.00000  bestvalidloss -1983.40413  last_update 13\n",
      "train: iter 197  trainloss -2043.44910  validloss -1948.75801±0.00000  bestvalidloss -1983.40413  last_update 14\n",
      "train: iter 198  trainloss -2083.96846  validloss -1887.39640±0.00000  bestvalidloss -1983.40413  last_update 15\n",
      "train: iter 199  trainloss -2113.55212  validloss -1987.03274±0.00000  bestvalidloss -1987.03274  last_update 0\n",
      "train: iter 200  trainloss -2095.10753  validloss -1975.98123±0.00000  bestvalidloss -1987.03274  last_update 1\n",
      "train: iter 201  trainloss -2035.27614  validloss -1962.57625±0.00000  bestvalidloss -1987.03274  last_update 2\n",
      "train: iter 202  trainloss -2114.18893  validloss -2019.42379±0.00000  bestvalidloss -2019.42379  last_update 0\n",
      "train: iter 203  trainloss -2076.43169  validloss -1945.78568±0.00000  bestvalidloss -2019.42379  last_update 1\n",
      "train: iter 204  trainloss -2103.09169  validloss -1936.72253±0.00000  bestvalidloss -2019.42379  last_update 2\n",
      "train: iter 205  trainloss -2048.46162  validloss -1932.54173±0.00000  bestvalidloss -2019.42379  last_update 3\n",
      "train: iter 206  trainloss -2051.77180  validloss -1938.39167±0.00000  bestvalidloss -2019.42379  last_update 4\n",
      "train: iter 207  trainloss -2102.12368  validloss -1954.33091±0.00000  bestvalidloss -2019.42379  last_update 5\n",
      "train: iter 208  trainloss -2129.03546  validloss -1954.86882±0.00000  bestvalidloss -2019.42379  last_update 6\n",
      "train: iter 209  trainloss -2126.26498  validloss -1952.36870±0.00000  bestvalidloss -2019.42379  last_update 7\n",
      "train: iter 210  trainloss -2119.25665  validloss -1931.06340±0.00000  bestvalidloss -2019.42379  last_update 8\n",
      "train: iter 211  trainloss -2066.61404  validloss -1903.49488±0.00000  bestvalidloss -2019.42379  last_update 9\n",
      "train: iter 212  trainloss -2082.72725  validloss -1905.68981±0.00000  bestvalidloss -2019.42379  last_update 10\n",
      "train: iter 213  trainloss -2119.43107  validloss -1901.14643±0.00000  bestvalidloss -2019.42379  last_update 11\n",
      "train: iter 214  trainloss -2135.00842  validloss -1916.79824±0.00000  bestvalidloss -2019.42379  last_update 12\n",
      "train: iter 215  trainloss -2101.88542  validloss -1904.59705±0.00000  bestvalidloss -2019.42379  last_update 13\n",
      "train: iter 216  trainloss -2098.76006  validloss -1803.66441±0.00000  bestvalidloss -2019.42379  last_update 14\n",
      "train: iter 217  trainloss -2080.16311  validloss -1861.77277±0.00000  bestvalidloss -2019.42379  last_update 15\n",
      "train: iter 218  trainloss -2004.54574  validloss -1776.38270±0.00000  bestvalidloss -2019.42379  last_update 16\n",
      "train: iter 219  trainloss -2138.57028  validloss -1973.12004±0.00000  bestvalidloss -2019.42379  last_update 17\n",
      "train: iter 220  trainloss -2137.66324  validloss -1935.52421±0.00000  bestvalidloss -2019.42379  last_update 18\n",
      "train: iter 221  trainloss -2082.18596  validloss -1914.09420±0.00000  bestvalidloss -2019.42379  last_update 19\n",
      "train: iter 222  trainloss -2116.74517  validloss -1944.82591±0.00000  bestvalidloss -2019.42379  last_update 20\n",
      "train: iter 223  trainloss -2128.83332  validloss -1907.94393±0.00000  bestvalidloss -2019.42379  last_update 21\n",
      "train: iter 224  trainloss -2118.26262  validloss -1875.83745±0.00000  bestvalidloss -2019.42379  last_update 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 225  trainloss -2148.65974  validloss -1879.99930±0.00000  bestvalidloss -2019.42379  last_update 23\n",
      "train: iter 226  trainloss -2074.33439  validloss -1955.66955±0.00000  bestvalidloss -2019.42379  last_update 24\n",
      "train: iter 227  trainloss -1998.75104  validloss -1854.80893±0.00000  bestvalidloss -2019.42379  last_update 25\n",
      "train: iter 228  trainloss -2088.54859  validloss -1873.55898±0.00000  bestvalidloss -2019.42379  last_update 26\n",
      "train: iter 229  trainloss -2145.47979  validloss -1855.02175±0.00000  bestvalidloss -2019.42379  last_update 27\n",
      "train: iter 230  trainloss -2109.24239  validloss -1890.44620±0.00000  bestvalidloss -2019.42379  last_update 28\n",
      "train: iter 231  trainloss -2110.10565  validloss -1646.21038±0.00000  bestvalidloss -2019.42379  last_update 29\n",
      "train: iter 232  trainloss -2020.38390  validloss -1923.94761±0.00000  bestvalidloss -2019.42379  last_update 30\n",
      "train: iter 233  trainloss -2023.22233  validloss -1567.43646±0.00000  bestvalidloss -2019.42379  last_update 31\n",
      "train: iter 234  trainloss -2134.51262  validloss -1908.96594±0.00000  bestvalidloss -2019.42379  last_update 32\n",
      "train: iter 235  trainloss -2146.01394  validloss -1929.48142±0.00000  bestvalidloss -2019.42379  last_update 33\n",
      "train: iter 236  trainloss -2127.70125  validloss -1954.29272±0.00000  bestvalidloss -2019.42379  last_update 34\n",
      "train: iter 237  trainloss -2146.35938  validloss -1948.16813±0.00000  bestvalidloss -2019.42379  last_update 35\n",
      "train: iter 238  trainloss -2140.45297  validloss -1958.52729±0.00000  bestvalidloss -2019.42379  last_update 36\n",
      "train: iter 239  trainloss -2012.45144  validloss -1869.44764±0.00000  bestvalidloss -2019.42379  last_update 37\n",
      "train: iter 240  trainloss -2113.18605  validloss -1928.18596±0.00000  bestvalidloss -2019.42379  last_update 38\n",
      "train: iter 241  trainloss -2144.73374  validloss -1939.08862±0.00000  bestvalidloss -2019.42379  last_update 39\n",
      "train: iter 242  trainloss -2119.32440  validloss -1982.74539±0.00000  bestvalidloss -2019.42379  last_update 40\n",
      "train: iter 243  trainloss -2152.81522  validloss -1948.35218±0.00000  bestvalidloss -2019.42379  last_update 41\n",
      "train: iter 244  trainloss -2154.20844  validloss -1929.15913±0.00000  bestvalidloss -2019.42379  last_update 42\n",
      "train: iter 245  trainloss -2150.76540  validloss -1925.27767±0.00000  bestvalidloss -2019.42379  last_update 43\n",
      "train: iter 246  trainloss -2051.44707  validloss -1859.89439±0.00000  bestvalidloss -2019.42379  last_update 44\n",
      "train: iter 247  trainloss -1997.13906  validloss -1762.13607±0.00000  bestvalidloss -2019.42379  last_update 45\n",
      "train: iter 248  trainloss -2130.90716  validloss -1804.01747±0.00000  bestvalidloss -2019.42379  last_update 46\n",
      "train: iter 249  trainloss -2148.75012  validloss -1961.90089±0.00000  bestvalidloss -2019.42379  last_update 47\n",
      "train: iter 250  trainloss -2120.44917  validloss -1899.03165±0.00000  bestvalidloss -2019.42379  last_update 48\n",
      "train: iter 251  trainloss -2133.24793  validloss -1944.02840±0.00000  bestvalidloss -2019.42379  last_update 49\n",
      "train: iter 252  trainloss -2054.90844  validloss -1920.41484±0.00000  bestvalidloss -2019.42379  last_update 50\n",
      "train: iter 253  trainloss -2135.05244  validloss -1938.04653±0.00000  bestvalidloss -2019.42379  last_update 51\n",
      "train: iter 254  trainloss -2141.40390  validloss -2010.14082±0.00000  bestvalidloss -2019.42379  last_update 52\n",
      "train: iter 255  trainloss -2148.09554  validloss -1938.19214±0.00000  bestvalidloss -2019.42379  last_update 53\n",
      "train: iter 256  trainloss -2156.40323  validloss -2027.26765±0.00000  bestvalidloss -2027.26765  last_update 0\n",
      "train: iter 257  trainloss -2147.98183  validloss -1994.70883±0.00000  bestvalidloss -2027.26765  last_update 1\n",
      "train: iter 258  trainloss -2151.97999  validloss -1936.07603±0.00000  bestvalidloss -2027.26765  last_update 2\n",
      "train: iter 259  trainloss -2110.28056  validloss -1955.39965±0.00000  bestvalidloss -2027.26765  last_update 3\n",
      "train: iter 260  trainloss -2147.06271  validloss -2009.65709±0.00000  bestvalidloss -2027.26765  last_update 4\n",
      "train: iter 261  trainloss -2015.21350  validloss -1889.82432±0.00000  bestvalidloss -2027.26765  last_update 5\n",
      "train: iter 262  trainloss -2081.52576  validloss -1870.48996±0.00000  bestvalidloss -2027.26765  last_update 6\n",
      "train: iter 263  trainloss -2130.14014  validloss -1935.89369±0.00000  bestvalidloss -2027.26765  last_update 7\n",
      "train: iter 264  trainloss -2158.35642  validloss -1993.80472±0.00000  bestvalidloss -2027.26765  last_update 8\n",
      "train: iter 265  trainloss -2154.60495  validloss -1921.88463±0.00000  bestvalidloss -2027.26765  last_update 9\n",
      "train: iter 266  trainloss -2137.74232  validloss -1969.75340±0.00000  bestvalidloss -2027.26765  last_update 10\n",
      "train: iter 267  trainloss -2173.26443  validloss -1950.46003±0.00000  bestvalidloss -2027.26765  last_update 11\n",
      "train: iter 268  trainloss -2132.66466  validloss -1994.32448±0.00000  bestvalidloss -2027.26765  last_update 12\n",
      "train: iter 269  trainloss -2182.19547  validloss -1948.38228±0.00000  bestvalidloss -2027.26765  last_update 13\n",
      "train: iter 270  trainloss -2171.02032  validloss -2038.93332±0.00000  bestvalidloss -2038.93332  last_update 0\n",
      "train: iter 271  trainloss -2152.97813  validloss -1987.21861±0.00000  bestvalidloss -2038.93332  last_update 1\n",
      "train: iter 272  trainloss -2152.38100  validloss -2022.55479±0.00000  bestvalidloss -2038.93332  last_update 2\n",
      "train: iter 273  trainloss -2162.99024  validloss -1958.85241±0.00000  bestvalidloss -2038.93332  last_update 3\n",
      "train: iter 274  trainloss -2128.74624  validloss -2016.66880±0.00000  bestvalidloss -2038.93332  last_update 4\n",
      "train: iter 275  trainloss -2121.81003  validloss -2008.79650±0.00000  bestvalidloss -2038.93332  last_update 5\n",
      "train: iter 276  trainloss -2124.18957  validloss -1995.04793±0.00000  bestvalidloss -2038.93332  last_update 6\n",
      "train: iter 277  trainloss -2122.16821  validloss -2020.80993±0.00000  bestvalidloss -2038.93332  last_update 7\n",
      "train: iter 278  trainloss -2131.91417  validloss -1921.87259±0.00000  bestvalidloss -2038.93332  last_update 8\n",
      "train: iter 279  trainloss -2087.31601  validloss -1906.52296±0.00000  bestvalidloss -2038.93332  last_update 9\n",
      "train: iter 280  trainloss -2089.51684  validloss -2001.54716±0.00000  bestvalidloss -2038.93332  last_update 10\n",
      "train: iter 281  trainloss -2147.20898  validloss -1976.23357±0.00000  bestvalidloss -2038.93332  last_update 11\n",
      "train: iter 282  trainloss -2112.73583  validloss -1864.19142±0.00000  bestvalidloss -2038.93332  last_update 12\n",
      "train: iter 283  trainloss -2148.42531  validloss -1995.16510±0.00000  bestvalidloss -2038.93332  last_update 13\n",
      "train: iter 284  trainloss -2172.72063  validloss -2032.08173±0.00000  bestvalidloss -2038.93332  last_update 14\n",
      "train: iter 285  trainloss -2074.86796  validloss -1981.32805±0.00000  bestvalidloss -2038.93332  last_update 15\n",
      "train: iter 286  trainloss -2164.11976  validloss -2008.55240±0.00000  bestvalidloss -2038.93332  last_update 16\n",
      "train: iter 287  trainloss -2181.43452  validloss -2031.03792±0.00000  bestvalidloss -2038.93332  last_update 17\n",
      "train: iter 288  trainloss -1962.10471  validloss -2014.54792±0.00000  bestvalidloss -2038.93332  last_update 18\n",
      "train: iter 289  trainloss -1995.79474  validloss -1750.52979±0.00000  bestvalidloss -2038.93332  last_update 19\n",
      "train: iter 290  trainloss -2144.76724  validloss -2045.39668±0.00000  bestvalidloss -2045.39668  last_update 0\n",
      "train: iter 291  trainloss -2173.66842  validloss -2065.13555±0.00000  bestvalidloss -2065.13555  last_update 0\n",
      "train: iter 292  trainloss -2175.83020  validloss -2065.58875±0.00000  bestvalidloss -2065.58875  last_update 0\n",
      "train: iter 293  trainloss -2170.41646  validloss -2070.73804±0.00000  bestvalidloss -2070.73804  last_update 0\n",
      "train: iter 294  trainloss -2149.12176  validloss -2052.85493±0.00000  bestvalidloss -2070.73804  last_update 1\n",
      "train: iter 295  trainloss -2167.23798  validloss -2054.72444±0.00000  bestvalidloss -2070.73804  last_update 2\n",
      "train: iter 296  trainloss -2178.45824  validloss -2040.10657±0.00000  bestvalidloss -2070.73804  last_update 3\n",
      "train: iter 297  trainloss -2171.22187  validloss -2056.41841±0.00000  bestvalidloss -2070.73804  last_update 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 298  trainloss -2156.83220  validloss -2060.26533±0.00000  bestvalidloss -2070.73804  last_update 5\n",
      "train: iter 299  trainloss -2133.96638  validloss -2078.26636±0.00000  bestvalidloss -2078.26636  last_update 0\n",
      "train: iter 300  trainloss -2041.85853  validloss -1895.93429±0.00000  bestvalidloss -2078.26636  last_update 1\n",
      "train: iter 301  trainloss -2185.80311  validloss -2063.88571±0.00000  bestvalidloss -2078.26636  last_update 2\n",
      "train: iter 302  trainloss -2149.13371  validloss -2064.55702±0.00000  bestvalidloss -2078.26636  last_update 3\n",
      "train: iter 303  trainloss -2177.19356  validloss -2057.05426±0.00000  bestvalidloss -2078.26636  last_update 4\n",
      "train: iter 304  trainloss -2176.57240  validloss -2064.62794±0.00000  bestvalidloss -2078.26636  last_update 5\n",
      "train: iter 305  trainloss -2172.91975  validloss -2089.09861±0.00000  bestvalidloss -2089.09861  last_update 0\n",
      "train: iter 306  trainloss -2100.56187  validloss -2067.04096±0.00000  bestvalidloss -2089.09861  last_update 1\n",
      "train: iter 307  trainloss -2162.05318  validloss -2044.33111±0.00000  bestvalidloss -2089.09861  last_update 2\n",
      "train: iter 308  trainloss -2171.06049  validloss -1982.91179±0.00000  bestvalidloss -2089.09861  last_update 3\n",
      "train: iter 309  trainloss -2169.36598  validloss -2049.43491±0.00000  bestvalidloss -2089.09861  last_update 4\n",
      "train: iter 310  trainloss -2169.67865  validloss -2085.44390±0.00000  bestvalidloss -2089.09861  last_update 5\n",
      "train: iter 311  trainloss -2116.65150  validloss -2049.41554±0.00000  bestvalidloss -2089.09861  last_update 6\n",
      "train: iter 312  trainloss -2133.61411  validloss -1902.33749±0.00000  bestvalidloss -2089.09861  last_update 7\n",
      "train: iter 313  trainloss -2148.99807  validloss -2100.44001±0.00000  bestvalidloss -2100.44001  last_update 0\n",
      "train: iter 314  trainloss -2152.39943  validloss -2030.66052±0.00000  bestvalidloss -2100.44001  last_update 1\n",
      "train: iter 315  trainloss -2169.59050  validloss -2056.68409±0.00000  bestvalidloss -2100.44001  last_update 2\n",
      "train: iter 316  trainloss -2065.36522  validloss -1975.65351±0.00000  bestvalidloss -2100.44001  last_update 3\n",
      "train: iter 317  trainloss -2160.55816  validloss -2031.82608±0.00000  bestvalidloss -2100.44001  last_update 4\n",
      "train: iter 318  trainloss -2153.02663  validloss -2084.76827±0.00000  bestvalidloss -2100.44001  last_update 5\n",
      "train: iter 319  trainloss -2105.85161  validloss -2020.57307±0.00000  bestvalidloss -2100.44001  last_update 6\n",
      "train: iter 320  trainloss -2124.66918  validloss -1932.42499±0.00000  bestvalidloss -2100.44001  last_update 7\n",
      "train: iter 321  trainloss -2172.08718  validloss -2082.44395±0.00000  bestvalidloss -2100.44001  last_update 8\n",
      "train: iter 322  trainloss -2189.32540  validloss -2075.56473±0.00000  bestvalidloss -2100.44001  last_update 9\n",
      "train: iter 323  trainloss -2160.99196  validloss -2082.44723±0.00000  bestvalidloss -2100.44001  last_update 10\n",
      "train: iter 324  trainloss -2129.14782  validloss -2014.62044±0.00000  bestvalidloss -2100.44001  last_update 11\n",
      "train: iter 325  trainloss -2189.92735  validloss -2066.59489±0.00000  bestvalidloss -2100.44001  last_update 12\n",
      "train: iter 326  trainloss -2184.16502  validloss -2056.58280±0.00000  bestvalidloss -2100.44001  last_update 13\n",
      "train: iter 327  trainloss -2180.27529  validloss -2029.90497±0.00000  bestvalidloss -2100.44001  last_update 14\n",
      "train: iter 328  trainloss -2145.23364  validloss -2033.13946±0.00000  bestvalidloss -2100.44001  last_update 15\n",
      "train: iter 329  trainloss -2138.10695  validloss -2089.10374±0.00000  bestvalidloss -2100.44001  last_update 16\n",
      "train: iter 330  trainloss -2048.62597  validloss -1868.56945±0.00000  bestvalidloss -2100.44001  last_update 17\n",
      "train: iter 331  trainloss -2182.29710  validloss -2066.87901±0.00000  bestvalidloss -2100.44001  last_update 18\n",
      "train: iter 332  trainloss -2175.77145  validloss -2107.73693±0.00000  bestvalidloss -2107.73693  last_update 0\n",
      "train: iter 333  trainloss -2183.76250  validloss -2088.25820±0.00000  bestvalidloss -2107.73693  last_update 1\n",
      "train: iter 334  trainloss -2137.11328  validloss -2015.51734±0.00000  bestvalidloss -2107.73693  last_update 2\n",
      "train: iter 335  trainloss -2109.57167  validloss -2010.55674±0.00000  bestvalidloss -2107.73693  last_update 3\n",
      "train: iter 336  trainloss -2164.87129  validloss -2023.13044±0.00000  bestvalidloss -2107.73693  last_update 4\n",
      "train: iter 337  trainloss -2191.60323  validloss -2104.83552±0.00000  bestvalidloss -2107.73693  last_update 5\n",
      "train: iter 338  trainloss -2181.52706  validloss -1974.27980±0.00000  bestvalidloss -2107.73693  last_update 6\n",
      "train: iter 339  trainloss -2166.50838  validloss -2095.88645±0.00000  bestvalidloss -2107.73693  last_update 7\n",
      "train: iter 340  trainloss -2163.02890  validloss -2032.41707±0.00000  bestvalidloss -2107.73693  last_update 8\n",
      "train: iter 341  trainloss -2112.34088  validloss -2001.53480±0.00000  bestvalidloss -2107.73693  last_update 9\n",
      "train: iter 342  trainloss -2180.02352  validloss -2028.53253±0.00000  bestvalidloss -2107.73693  last_update 10\n",
      "train: iter 343  trainloss -2147.59275  validloss -2071.88260±0.00000  bestvalidloss -2107.73693  last_update 11\n",
      "train: iter 344  trainloss -2168.88353  validloss -1965.83212±0.00000  bestvalidloss -2107.73693  last_update 12\n",
      "train: iter 345  trainloss -2165.09896  validloss -2072.37629±0.00000  bestvalidloss -2107.73693  last_update 13\n",
      "train: iter 346  trainloss -2164.04474  validloss -2112.48437±0.00000  bestvalidloss -2112.48437  last_update 0\n",
      "train: iter 347  trainloss -2174.35266  validloss -2128.14839±0.00000  bestvalidloss -2128.14839  last_update 0\n",
      "train: iter 348  trainloss -2185.43149  validloss -2107.59249±0.00000  bestvalidloss -2128.14839  last_update 1\n",
      "train: iter 349  trainloss -2184.04416  validloss -2071.49532±0.00000  bestvalidloss -2128.14839  last_update 2\n",
      "train: iter 350  trainloss -2179.90955  validloss -2095.60728±0.00000  bestvalidloss -2128.14839  last_update 3\n",
      "train: iter 351  trainloss -2117.52209  validloss -2065.79375±0.00000  bestvalidloss -2128.14839  last_update 4\n",
      "train: iter 352  trainloss -2116.42631  validloss -1996.25479±0.00000  bestvalidloss -2128.14839  last_update 5\n",
      "train: iter 353  trainloss -2108.36960  validloss -1909.90650±0.00000  bestvalidloss -2128.14839  last_update 6\n",
      "train: iter 354  trainloss -2182.92395  validloss -2056.97464±0.00000  bestvalidloss -2128.14839  last_update 7\n",
      "train: iter 355  trainloss -2178.14152  validloss -2075.11218±0.00000  bestvalidloss -2128.14839  last_update 8\n",
      "train: iter 356  trainloss -2187.43482  validloss -2075.54775±0.00000  bestvalidloss -2128.14839  last_update 9\n",
      "train: iter 357  trainloss -2153.60327  validloss -2016.40545±0.00000  bestvalidloss -2128.14839  last_update 10\n",
      "train: iter 358  trainloss -2134.35842  validloss -2053.27746±0.00000  bestvalidloss -2128.14839  last_update 11\n",
      "train: iter 359  trainloss -2193.55727  validloss -2062.68997±0.00000  bestvalidloss -2128.14839  last_update 12\n",
      "train: iter 360  trainloss -2193.03094  validloss -2073.46745±0.00000  bestvalidloss -2128.14839  last_update 13\n",
      "train: iter 361  trainloss -2208.68775  validloss -2128.90990±0.00000  bestvalidloss -2128.90990  last_update 0\n",
      "train: iter 362  trainloss -2144.91073  validloss -2102.64146±0.00000  bestvalidloss -2128.90990  last_update 1\n",
      "train: iter 363  trainloss -2148.71600  validloss -1996.27986±0.00000  bestvalidloss -2128.90990  last_update 2\n",
      "train: iter 364  trainloss -2153.26671  validloss -1945.65612±0.00000  bestvalidloss -2128.90990  last_update 3\n",
      "train: iter 365  trainloss -2152.46412  validloss -2020.08104±0.00000  bestvalidloss -2128.90990  last_update 4\n",
      "train: iter 366  trainloss -2170.98319  validloss -2013.77348±0.00000  bestvalidloss -2128.90990  last_update 5\n",
      "train: iter 367  trainloss -2211.67347  validloss -2092.86169±0.00000  bestvalidloss -2128.90990  last_update 6\n",
      "train: iter 368  trainloss -2155.89492  validloss -2076.87791±0.00000  bestvalidloss -2128.90990  last_update 7\n",
      "train: iter 369  trainloss -2152.97434  validloss -2093.04691±0.00000  bestvalidloss -2128.90990  last_update 8\n",
      "train: iter 370  trainloss -2174.54427  validloss -2085.78587±0.00000  bestvalidloss -2128.90990  last_update 9\n",
      "train: iter 371  trainloss -2195.09787  validloss -2116.37449±0.00000  bestvalidloss -2128.90990  last_update 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 372  trainloss -2189.83167  validloss -2104.26921±0.00000  bestvalidloss -2128.90990  last_update 11\n",
      "train: iter 373  trainloss -2116.94904  validloss -1997.47338±0.00000  bestvalidloss -2128.90990  last_update 12\n",
      "train: iter 374  trainloss -2169.47184  validloss -1957.89430±0.00000  bestvalidloss -2128.90990  last_update 13\n",
      "train: iter 375  trainloss -2175.59901  validloss -2111.37259±0.00000  bestvalidloss -2128.90990  last_update 14\n",
      "train: iter 376  trainloss -2163.83605  validloss -2122.98288±0.00000  bestvalidloss -2128.90990  last_update 15\n",
      "train: iter 377  trainloss -2197.41066  validloss -2070.57500±0.00000  bestvalidloss -2128.90990  last_update 16\n",
      "train: iter 378  trainloss -2161.13143  validloss -2091.15804±0.00000  bestvalidloss -2128.90990  last_update 17\n",
      "train: iter 379  trainloss -2080.69583  validloss -2036.83682±0.00000  bestvalidloss -2128.90990  last_update 18\n",
      "train: iter 380  trainloss -2195.06313  validloss -2026.74360±0.00000  bestvalidloss -2128.90990  last_update 19\n",
      "train: iter 381  trainloss -2197.15770  validloss -2107.74847±0.00000  bestvalidloss -2128.90990  last_update 20\n",
      "train: iter 382  trainloss -2201.86183  validloss -2108.05715±0.00000  bestvalidloss -2128.90990  last_update 21\n",
      "train: iter 383  trainloss -2025.32242  validloss -2092.01427±0.00000  bestvalidloss -2128.90990  last_update 22\n",
      "train: iter 384  trainloss -2163.02954  validloss -2082.61182±0.00000  bestvalidloss -2128.90990  last_update 23\n",
      "train: iter 385  trainloss -2157.54734  validloss -2054.72126±0.00000  bestvalidloss -2128.90990  last_update 24\n",
      "train: iter 386  trainloss -2155.05876  validloss -1979.74377±0.00000  bestvalidloss -2128.90990  last_update 25\n",
      "train: iter 387  trainloss -2185.98909  validloss -2099.73163±0.00000  bestvalidloss -2128.90990  last_update 26\n",
      "train: iter 388  trainloss -2184.43172  validloss -2131.40074±0.00000  bestvalidloss -2131.40074  last_update 0\n",
      "train: iter 389  trainloss -2182.46649  validloss -2035.11994±0.00000  bestvalidloss -2131.40074  last_update 1\n",
      "train: iter 390  trainloss -2124.04022  validloss -2012.81235±0.00000  bestvalidloss -2131.40074  last_update 2\n",
      "train: iter 391  trainloss -2185.56585  validloss -2124.85188±0.00000  bestvalidloss -2131.40074  last_update 3\n",
      "train: iter 392  trainloss -2200.65080  validloss -2116.99106±0.00000  bestvalidloss -2131.40074  last_update 4\n",
      "train: iter 393  trainloss -2198.38530  validloss -2089.36657±0.00000  bestvalidloss -2131.40074  last_update 5\n",
      "train: iter 394  trainloss -2215.38885  validloss -2130.10989±0.00000  bestvalidloss -2131.40074  last_update 6\n",
      "train: iter 395  trainloss -2200.39371  validloss -2081.40784±0.00000  bestvalidloss -2131.40074  last_update 7\n",
      "train: iter 396  trainloss -2169.30268  validloss -2028.15740±0.00000  bestvalidloss -2131.40074  last_update 8\n",
      "train: iter 397  trainloss -2203.14967  validloss -2077.58479±0.00000  bestvalidloss -2131.40074  last_update 9\n",
      "train: iter 398  trainloss -2126.57951  validloss -2117.80377±0.00000  bestvalidloss -2131.40074  last_update 10\n",
      "train: iter 399  trainloss -2105.47269  validloss -2001.62782±0.00000  bestvalidloss -2131.40074  last_update 11\n",
      "train: iter 400  trainloss -2164.17644  validloss -2111.31652±0.00000  bestvalidloss -2131.40074  last_update 12\n",
      "train: iter 401  trainloss -2176.89366  validloss -2070.13580±0.00000  bestvalidloss -2131.40074  last_update 13\n",
      "train: iter 402  trainloss -2200.39420  validloss -2091.49752±0.00000  bestvalidloss -2131.40074  last_update 14\n",
      "train: iter 403  trainloss -2171.46055  validloss -2095.30099±0.00000  bestvalidloss -2131.40074  last_update 15\n",
      "train: iter 404  trainloss -2204.42727  validloss -2128.77624±0.00000  bestvalidloss -2131.40074  last_update 16\n",
      "train: iter 405  trainloss -2205.49978  validloss -2134.21622±0.00000  bestvalidloss -2134.21622  last_update 0\n",
      "train: iter 406  trainloss -2192.77628  validloss -2132.55714±0.00000  bestvalidloss -2134.21622  last_update 1\n",
      "train: iter 407  trainloss -2208.49699  validloss -2089.89415±0.00000  bestvalidloss -2134.21622  last_update 2\n",
      "train: iter 408  trainloss -2211.44930  validloss -2111.05218±0.00000  bestvalidloss -2134.21622  last_update 3\n",
      "train: iter 409  trainloss -2212.68905  validloss -2107.48386±0.00000  bestvalidloss -2134.21622  last_update 4\n",
      "train: iter 410  trainloss -2108.32404  validloss -2135.05037±0.00000  bestvalidloss -2135.05037  last_update 0\n",
      "train: iter 411  trainloss -2036.35284  validloss -1770.94403±0.00000  bestvalidloss -2135.05037  last_update 1\n",
      "train: iter 412  trainloss -2212.39203  validloss -2092.27858±0.00000  bestvalidloss -2135.05037  last_update 2\n",
      "train: iter 413  trainloss -2167.71922  validloss -2075.95819±0.00000  bestvalidloss -2135.05037  last_update 3\n",
      "train: iter 414  trainloss -2189.19486  validloss -2063.32065±0.00000  bestvalidloss -2135.05037  last_update 4\n",
      "train: iter 415  trainloss -2200.48049  validloss -2147.09757±0.00000  bestvalidloss -2147.09757  last_update 0\n",
      "train: iter 416  trainloss -2212.95961  validloss -2085.43470±0.00000  bestvalidloss -2147.09757  last_update 1\n",
      "train: iter 417  trainloss -2209.66701  validloss -2136.95620±0.00000  bestvalidloss -2147.09757  last_update 2\n",
      "train: iter 418  trainloss -2204.96445  validloss -2145.29625±0.00000  bestvalidloss -2147.09757  last_update 3\n",
      "train: iter 419  trainloss -2206.51189  validloss -2106.69599±0.00000  bestvalidloss -2147.09757  last_update 4\n",
      "train: iter 420  trainloss -2210.82467  validloss -2141.55968±0.00000  bestvalidloss -2147.09757  last_update 5\n",
      "train: iter 421  trainloss -2155.78654  validloss -2023.30273±0.00000  bestvalidloss -2147.09757  last_update 6\n",
      "train: iter 422  trainloss -2144.18214  validloss -2075.18097±0.00000  bestvalidloss -2147.09757  last_update 7\n",
      "train: iter 423  trainloss -1666.59667  validloss -2021.35181±0.00000  bestvalidloss -2147.09757  last_update 8\n",
      "train: iter 424  trainloss -2068.22807  validloss -1382.85599±0.00000  bestvalidloss -2147.09757  last_update 9\n",
      "train: iter 425  trainloss -2210.69015  validloss -2121.51929±0.00000  bestvalidloss -2147.09757  last_update 10\n",
      "train: iter 426  trainloss -2212.34941  validloss -2098.03267±0.00000  bestvalidloss -2147.09757  last_update 11\n",
      "train: iter 427  trainloss -2201.07648  validloss -2061.10560±0.00000  bestvalidloss -2147.09757  last_update 12\n",
      "train: iter 428  trainloss -2197.86419  validloss -2136.60183±0.00000  bestvalidloss -2147.09757  last_update 13\n",
      "train: iter 429  trainloss -2200.89585  validloss -2133.06168±0.00000  bestvalidloss -2147.09757  last_update 14\n",
      "train: iter 430  trainloss -2193.62415  validloss -2089.40652±0.00000  bestvalidloss -2147.09757  last_update 15\n",
      "train: iter 431  trainloss -2179.28686  validloss -2139.73642±0.00000  bestvalidloss -2147.09757  last_update 16\n",
      "train: iter 432  trainloss -2204.56129  validloss -2141.58724±0.00000  bestvalidloss -2147.09757  last_update 17\n",
      "train: iter 433  trainloss -2202.26061  validloss -2123.31035±0.00000  bestvalidloss -2147.09757  last_update 18\n",
      "train: iter 434  trainloss -2208.81134  validloss -2112.61502±0.00000  bestvalidloss -2147.09757  last_update 19\n",
      "train: iter 435  trainloss -2200.22324  validloss -2160.95739±0.00000  bestvalidloss -2160.95739  last_update 0\n",
      "train: iter 436  trainloss -2188.70435  validloss -2068.68295±0.00000  bestvalidloss -2160.95739  last_update 1\n",
      "train: iter 437  trainloss -2171.75631  validloss -2101.12460±0.00000  bestvalidloss -2160.95739  last_update 2\n",
      "train: iter 438  trainloss -2132.93379  validloss -2057.80682±0.00000  bestvalidloss -2160.95739  last_update 3\n",
      "train: iter 439  trainloss -2209.16513  validloss -2146.50097±0.00000  bestvalidloss -2160.95739  last_update 4\n",
      "train: iter 440  trainloss -2217.57281  validloss -2108.96478±0.00000  bestvalidloss -2160.95739  last_update 5\n",
      "train: iter 441  trainloss -2210.39259  validloss -2156.27864±0.00000  bestvalidloss -2160.95739  last_update 6\n",
      "train: iter 442  trainloss -2122.60764  validloss -2105.72591±0.00000  bestvalidloss -2160.95739  last_update 7\n",
      "train: iter 443  trainloss -2191.88030  validloss -2103.97559±0.00000  bestvalidloss -2160.95739  last_update 8\n",
      "train: iter 444  trainloss -2191.13228  validloss -2065.06030±0.00000  bestvalidloss -2160.95739  last_update 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 445  trainloss -2115.56578  validloss -2097.65093±0.00000  bestvalidloss -2160.95739  last_update 10\n",
      "train: iter 446  trainloss -2110.50567  validloss -1897.61888±0.00000  bestvalidloss -2160.95739  last_update 11\n",
      "train: iter 447  trainloss -2163.46763  validloss -2028.10880±0.00000  bestvalidloss -2160.95739  last_update 12\n",
      "train: iter 448  trainloss -2208.83823  validloss -2139.13089±0.00000  bestvalidloss -2160.95739  last_update 13\n",
      "train: iter 449  trainloss -2198.31972  validloss -2104.23143±0.00000  bestvalidloss -2160.95739  last_update 14\n",
      "train: iter 450  trainloss -2202.79197  validloss -2146.87078±0.00000  bestvalidloss -2160.95739  last_update 15\n",
      "train: iter 451  trainloss -2217.08612  validloss -2132.31467±0.00000  bestvalidloss -2160.95739  last_update 16\n",
      "train: iter 452  trainloss -2201.94670  validloss -2123.70346±0.00000  bestvalidloss -2160.95739  last_update 17\n",
      "train: iter 453  trainloss -2102.58678  validloss -2112.77818±0.00000  bestvalidloss -2160.95739  last_update 18\n",
      "train: iter 454  trainloss -2188.55826  validloss -2072.35142±0.00000  bestvalidloss -2160.95739  last_update 19\n",
      "train: iter 455  trainloss -2193.06381  validloss -2146.08613±0.00000  bestvalidloss -2160.95739  last_update 20\n",
      "train: iter 456  trainloss -2176.53395  validloss -2001.98417±0.00000  bestvalidloss -2160.95739  last_update 21\n",
      "train: iter 457  trainloss -2195.53012  validloss -2158.98054±0.00000  bestvalidloss -2160.95739  last_update 22\n",
      "train: iter 458  trainloss -2157.55869  validloss -2097.44954±0.00000  bestvalidloss -2160.95739  last_update 23\n",
      "train: iter 459  trainloss -2083.60292  validloss -2098.78304±0.00000  bestvalidloss -2160.95739  last_update 24\n",
      "train: iter 460  trainloss -2136.39790  validloss -2056.48490±0.00000  bestvalidloss -2160.95739  last_update 25\n",
      "train: iter 461  trainloss -2208.74594  validloss -2117.19700±0.00000  bestvalidloss -2160.95739  last_update 26\n",
      "train: iter 462  trainloss -2197.44326  validloss -2090.62999±0.00000  bestvalidloss -2160.95739  last_update 27\n",
      "train: iter 463  trainloss -2181.22069  validloss -2117.76806±0.00000  bestvalidloss -2160.95739  last_update 28\n",
      "train: iter 464  trainloss -2181.60395  validloss -2094.78800±0.00000  bestvalidloss -2160.95739  last_update 29\n",
      "train: iter 465  trainloss -2212.50301  validloss -2103.83320±0.00000  bestvalidloss -2160.95739  last_update 30\n",
      "train: iter 466  trainloss -2211.84050  validloss -2119.96674±0.00000  bestvalidloss -2160.95739  last_update 31\n",
      "train: iter 467  trainloss -2194.61535  validloss -2121.85558±0.00000  bestvalidloss -2160.95739  last_update 32\n",
      "train: iter 468  trainloss -2223.53667  validloss -2168.88073±0.00000  bestvalidloss -2168.88073  last_update 0\n",
      "train: iter 469  trainloss -2196.18106  validloss -2140.58062±0.00000  bestvalidloss -2168.88073  last_update 1\n",
      "train: iter 470  trainloss -2180.16768  validloss -2123.01127±0.00000  bestvalidloss -2168.88073  last_update 2\n",
      "train: iter 471  trainloss -2023.61787  validloss -2032.60175±0.00000  bestvalidloss -2168.88073  last_update 3\n",
      "train: iter 472  trainloss -2193.18036  validloss -2130.89435±0.00000  bestvalidloss -2168.88073  last_update 4\n",
      "train: iter 473  trainloss -2213.28895  validloss -2099.83235±0.00000  bestvalidloss -2168.88073  last_update 5\n",
      "train: iter 474  trainloss -2152.52708  validloss -2131.33401±0.00000  bestvalidloss -2168.88073  last_update 6\n",
      "train: iter 475  trainloss -2197.84437  validloss -2016.02707±0.00000  bestvalidloss -2168.88073  last_update 7\n",
      "train: iter 476  trainloss -2205.83478  validloss -2151.36447±0.00000  bestvalidloss -2168.88073  last_update 8\n",
      "train: iter 477  trainloss -2209.17079  validloss -2146.88214±0.00000  bestvalidloss -2168.88073  last_update 9\n",
      "train: iter 478  trainloss -2187.88452  validloss -2112.01031±0.00000  bestvalidloss -2168.88073  last_update 10\n",
      "train: iter 479  trainloss -2177.18265  validloss -2119.75079±0.00000  bestvalidloss -2168.88073  last_update 11\n",
      "train: iter 480  trainloss -2211.49514  validloss -2116.08372±0.00000  bestvalidloss -2168.88073  last_update 12\n",
      "train: iter 481  trainloss -2173.36202  validloss -2106.47311±0.00000  bestvalidloss -2168.88073  last_update 13\n",
      "train: iter 482  trainloss -2185.15804  validloss -2026.16057±0.00000  bestvalidloss -2168.88073  last_update 14\n",
      "train: iter 483  trainloss -2220.52081  validloss -2132.39085±0.00000  bestvalidloss -2168.88073  last_update 15\n",
      "train: iter 484  trainloss -2221.51523  validloss -2143.81621±0.00000  bestvalidloss -2168.88073  last_update 16\n",
      "train: iter 485  trainloss -2225.70366  validloss -2133.70532±0.00000  bestvalidloss -2168.88073  last_update 17\n",
      "train: iter 486  trainloss -2204.61203  validloss -2150.27946±0.00000  bestvalidloss -2168.88073  last_update 18\n",
      "train: iter 487  trainloss -2201.17359  validloss -2139.38823±0.00000  bestvalidloss -2168.88073  last_update 19\n",
      "train: iter 488  trainloss -2192.27366  validloss -2077.48973±0.00000  bestvalidloss -2168.88073  last_update 20\n",
      "train: iter 489  trainloss -2164.43543  validloss -2048.29612±0.00000  bestvalidloss -2168.88073  last_update 21\n",
      "train: iter 490  trainloss -2050.43049  validloss -2126.48879±0.00000  bestvalidloss -2168.88073  last_update 22\n",
      "train: iter 491  trainloss -2109.47723  validloss -1690.03679±0.00000  bestvalidloss -2168.88073  last_update 23\n",
      "train: iter 492  trainloss -2196.42656  validloss -2111.31650±0.00000  bestvalidloss -2168.88073  last_update 24\n",
      "train: iter 493  trainloss -2213.50322  validloss -2108.27457±0.00000  bestvalidloss -2168.88073  last_update 25\n",
      "train: iter 494  trainloss -2221.40320  validloss -2120.41579±0.00000  bestvalidloss -2168.88073  last_update 26\n",
      "train: iter 495  trainloss -2219.16657  validloss -2160.93459±0.00000  bestvalidloss -2168.88073  last_update 27\n",
      "train: iter 496  trainloss -2225.13345  validloss -2142.95916±0.00000  bestvalidloss -2168.88073  last_update 28\n",
      "train: iter 497  trainloss -2204.91518  validloss -2145.40362±0.00000  bestvalidloss -2168.88073  last_update 29\n",
      "train: iter 498  trainloss -2197.27488  validloss -2086.06846±0.00000  bestvalidloss -2168.88073  last_update 30\n",
      "train: iter 499  trainloss -2222.30891  validloss -2144.36590±0.00000  bestvalidloss -2168.88073  last_update 31\n",
      "train: iter 500  trainloss -2202.53998  validloss -2153.34415±0.00000  bestvalidloss -2168.88073  last_update 32\n",
      "train: iter 501  trainloss -2209.60378  validloss -2143.94373±0.00000  bestvalidloss -2168.88073  last_update 33\n",
      "train: iter 502  trainloss -2227.21832  validloss -2164.87316±0.00000  bestvalidloss -2168.88073  last_update 34\n",
      "train: iter 503  trainloss -2221.20837  validloss -2160.26539±0.00000  bestvalidloss -2168.88073  last_update 35\n",
      "train: iter 504  trainloss -2202.10111  validloss -2156.14429±0.00000  bestvalidloss -2168.88073  last_update 36\n",
      "train: iter 505  trainloss -2182.99718  validloss -2083.37788±0.00000  bestvalidloss -2168.88073  last_update 37\n",
      "train: iter 506  trainloss -2166.04183  validloss -2031.65576±0.00000  bestvalidloss -2168.88073  last_update 38\n",
      "train: iter 507  trainloss -2126.97546  validloss -2087.37422±0.00000  bestvalidloss -2168.88073  last_update 39\n",
      "train: iter 508  trainloss -2199.37871  validloss -2080.40411±0.00000  bestvalidloss -2168.88073  last_update 40\n",
      "train: iter 509  trainloss -2170.43322  validloss -2133.63974±0.00000  bestvalidloss -2168.88073  last_update 41\n",
      "train: iter 510  trainloss -2198.15668  validloss -2123.67711±0.00000  bestvalidloss -2168.88073  last_update 42\n",
      "train: iter 511  trainloss -2223.48815  validloss -2129.44838±0.00000  bestvalidloss -2168.88073  last_update 43\n",
      "train: iter 512  trainloss -2190.31676  validloss -2138.19370±0.00000  bestvalidloss -2168.88073  last_update 44\n",
      "train: iter 513  trainloss -2206.85828  validloss -2060.22391±0.00000  bestvalidloss -2168.88073  last_update 45\n",
      "train: iter 514  trainloss -2215.29510  validloss -2128.24036±0.00000  bestvalidloss -2168.88073  last_update 46\n",
      "train: iter 515  trainloss -2206.42738  validloss -2096.24868±0.00000  bestvalidloss -2168.88073  last_update 47\n",
      "train: iter 516  trainloss -2203.29977  validloss -2139.21061±0.00000  bestvalidloss -2168.88073  last_update 48\n",
      "train: iter 517  trainloss -2119.44981  validloss -2089.40201±0.00000  bestvalidloss -2168.88073  last_update 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 518  trainloss -2175.16770  validloss -2093.64701±0.00000  bestvalidloss -2168.88073  last_update 50\n",
      "train: iter 519  trainloss -2201.49355  validloss -2094.78091±0.00000  bestvalidloss -2168.88073  last_update 51\n",
      "train: iter 520  trainloss -2225.48952  validloss -2145.32438±0.00000  bestvalidloss -2168.88073  last_update 52\n",
      "train: iter 521  trainloss -2216.33583  validloss -2172.79704±0.00000  bestvalidloss -2172.79704  last_update 0\n",
      "train: iter 522  trainloss -2227.34151  validloss -2145.80768±0.00000  bestvalidloss -2172.79704  last_update 1\n",
      "train: iter 523  trainloss -2215.11893  validloss -2141.95367±0.00000  bestvalidloss -2172.79704  last_update 2\n",
      "train: iter 524  trainloss -2208.50339  validloss -2140.52512±0.00000  bestvalidloss -2172.79704  last_update 3\n",
      "train: iter 525  trainloss -2149.77852  validloss -1967.54622±0.00000  bestvalidloss -2172.79704  last_update 4\n",
      "train: iter 526  trainloss -2160.62890  validloss -2049.70569±0.00000  bestvalidloss -2172.79704  last_update 5\n",
      "train: iter 527  trainloss -2205.48596  validloss -2124.26685±0.00000  bestvalidloss -2172.79704  last_update 6\n",
      "train: iter 528  trainloss -2225.09186  validloss -2168.00038±0.00000  bestvalidloss -2172.79704  last_update 7\n",
      "train: iter 529  trainloss -2231.50067  validloss -2155.47066±0.00000  bestvalidloss -2172.79704  last_update 8\n",
      "train: iter 530  trainloss -2172.76635  validloss -2151.33223±0.00000  bestvalidloss -2172.79704  last_update 9\n",
      "train: iter 531  trainloss -2201.36888  validloss -2131.98463±0.00000  bestvalidloss -2172.79704  last_update 10\n",
      "train: iter 532  trainloss -2036.76987  validloss -2104.92500±0.00000  bestvalidloss -2172.79704  last_update 11\n",
      "train: iter 533  trainloss -2206.93588  validloss -2092.74919±0.00000  bestvalidloss -2172.79704  last_update 12\n",
      "train: iter 534  trainloss -2224.16926  validloss -2166.84595±0.00000  bestvalidloss -2172.79704  last_update 13\n",
      "train: iter 535  trainloss -2197.24249  validloss -2140.96451±0.00000  bestvalidloss -2172.79704  last_update 14\n",
      "train: iter 536  trainloss -2212.42008  validloss -2144.07050±0.00000  bestvalidloss -2172.79704  last_update 15\n",
      "train: iter 537  trainloss -2228.99518  validloss -2166.08074±0.00000  bestvalidloss -2172.79704  last_update 16\n",
      "train: iter 538  trainloss -2220.73922  validloss -2168.87977±0.00000  bestvalidloss -2172.79704  last_update 17\n",
      "train: iter 539  trainloss -2218.89369  validloss -2157.01485±0.00000  bestvalidloss -2172.79704  last_update 18\n",
      "train: iter 540  trainloss -2215.77905  validloss -2157.45677±0.00000  bestvalidloss -2172.79704  last_update 19\n",
      "train: iter 541  trainloss -2201.02674  validloss -2158.23714±0.00000  bestvalidloss -2172.79704  last_update 20\n",
      "train: iter 542  trainloss -2181.80090  validloss -2128.03737±0.00000  bestvalidloss -2172.79704  last_update 21\n",
      "train: iter 543  trainloss -2208.65670  validloss -2155.68243±0.00000  bestvalidloss -2172.79704  last_update 22\n",
      "train: iter 544  trainloss -2068.40933  validloss -2141.73680±0.00000  bestvalidloss -2172.79704  last_update 23\n",
      "train: iter 545  trainloss -2117.65804  validloss -1953.80310±0.00000  bestvalidloss -2172.79704  last_update 24\n",
      "train: iter 546  trainloss -2150.90653  validloss -2101.00635±0.00000  bestvalidloss -2172.79704  last_update 25\n",
      "train: iter 547  trainloss -2216.15792  validloss -2151.70552±0.00000  bestvalidloss -2172.79704  last_update 26\n",
      "train: iter 548  trainloss -2208.86856  validloss -2137.49854±0.00000  bestvalidloss -2172.79704  last_update 27\n",
      "train: iter 549  trainloss -2195.48395  validloss -2159.36196±0.00000  bestvalidloss -2172.79704  last_update 28\n",
      "train: iter 550  trainloss -2222.97303  validloss -2140.06788±0.00000  bestvalidloss -2172.79704  last_update 29\n",
      "train: iter 551  trainloss -2216.66869  validloss -2147.21036±0.00000  bestvalidloss -2172.79704  last_update 30\n",
      "train: iter 552  trainloss -2224.41203  validloss -2141.10555±0.00000  bestvalidloss -2172.79704  last_update 31\n",
      "train: iter 553  trainloss -2227.66108  validloss -2164.91323±0.00000  bestvalidloss -2172.79704  last_update 32\n",
      "train: iter 554  trainloss -2168.80429  validloss -2105.02371±0.00000  bestvalidloss -2172.79704  last_update 33\n",
      "train: iter 555  trainloss -2198.69125  validloss -2111.19440±0.00000  bestvalidloss -2172.79704  last_update 34\n",
      "train: iter 556  trainloss -2202.44162  validloss -2060.66938±0.00000  bestvalidloss -2172.79704  last_update 35\n",
      "train: iter 557  trainloss -2211.67963  validloss -2162.43388±0.00000  bestvalidloss -2172.79704  last_update 36\n",
      "train: iter 558  trainloss -2204.53225  validloss -2141.10753±0.00000  bestvalidloss -2172.79704  last_update 37\n",
      "train: iter 559  trainloss -2219.48529  validloss -2147.43226±0.00000  bestvalidloss -2172.79704  last_update 38\n",
      "train: iter 560  trainloss -2198.10436  validloss -2046.73449±0.00000  bestvalidloss -2172.79704  last_update 39\n",
      "train: iter 561  trainloss -2210.93253  validloss -2178.63671±0.00000  bestvalidloss -2178.63671  last_update 0\n",
      "train: iter 562  trainloss -2227.27607  validloss -2140.78740±0.00000  bestvalidloss -2178.63671  last_update 1\n",
      "train: iter 563  trainloss -2227.07465  validloss -2185.48354±0.00000  bestvalidloss -2185.48354  last_update 0\n",
      "train: iter 564  trainloss -2222.72820  validloss -2146.21246±0.00000  bestvalidloss -2185.48354  last_update 1\n",
      "train: iter 565  trainloss -2231.90821  validloss -2147.55192±0.00000  bestvalidloss -2185.48354  last_update 2\n",
      "train: iter 566  trainloss -2203.66882  validloss -2152.87688±0.00000  bestvalidloss -2185.48354  last_update 3\n",
      "train: iter 567  trainloss -2221.12941  validloss -2119.72626±0.00000  bestvalidloss -2185.48354  last_update 4\n",
      "train: iter 568  trainloss -1977.07968  validloss -2057.52908±0.00000  bestvalidloss -2185.48354  last_update 5\n",
      "train: iter 569  trainloss -2087.19855  validloss -1896.37250±0.00000  bestvalidloss -2185.48354  last_update 6\n",
      "train: iter 570  trainloss -2232.38991  validloss -2160.73344±0.00000  bestvalidloss -2185.48354  last_update 7\n",
      "train: iter 571  trainloss -2214.06465  validloss -2156.35535±0.00000  bestvalidloss -2185.48354  last_update 8\n",
      "train: iter 572  trainloss -2188.95643  validloss -2123.55022±0.00000  bestvalidloss -2185.48354  last_update 9\n",
      "train: iter 573  trainloss -2221.72380  validloss -2146.07223±0.00000  bestvalidloss -2185.48354  last_update 10\n",
      "train: iter 574  trainloss -2209.57511  validloss -2183.70001±0.00000  bestvalidloss -2185.48354  last_update 11\n",
      "train: iter 575  trainloss -2235.58473  validloss -2170.72230±0.00000  bestvalidloss -2185.48354  last_update 12\n",
      "train: iter 576  trainloss -2231.22697  validloss -2192.64424±0.00000  bestvalidloss -2192.64424  last_update 0\n",
      "train: iter 577  trainloss -2224.21976  validloss -2113.06591±0.00000  bestvalidloss -2192.64424  last_update 1\n",
      "train: iter 578  trainloss -2205.76547  validloss -2168.70651±0.00000  bestvalidloss -2192.64424  last_update 2\n",
      "train: iter 579  trainloss -2210.58496  validloss -2140.57765±0.00000  bestvalidloss -2192.64424  last_update 3\n",
      "train: iter 580  trainloss -2221.25961  validloss -2178.67460±0.00000  bestvalidloss -2192.64424  last_update 4\n",
      "train: iter 581  trainloss -2213.68756  validloss -2163.03445±0.00000  bestvalidloss -2192.64424  last_update 5\n",
      "train: iter 582  trainloss -2203.24791  validloss -2164.90663±0.00000  bestvalidloss -2192.64424  last_update 6\n",
      "train: iter 583  trainloss -2181.52906  validloss -2156.34096±0.00000  bestvalidloss -2192.64424  last_update 7\n",
      "train: iter 584  trainloss -2197.27319  validloss -2171.09942±0.00000  bestvalidloss -2192.64424  last_update 8\n",
      "train: iter 585  trainloss -2226.42442  validloss -2126.27404±0.00000  bestvalidloss -2192.64424  last_update 9\n",
      "train: iter 586  trainloss -2213.03240  validloss -2178.65800±0.00000  bestvalidloss -2192.64424  last_update 10\n",
      "train: iter 587  trainloss -2201.81521  validloss -2035.59738±0.00000  bestvalidloss -2192.64424  last_update 11\n",
      "train: iter 588  trainloss -2180.13889  validloss -2130.83900±0.00000  bestvalidloss -2192.64424  last_update 12\n",
      "train: iter 589  trainloss -2220.35704  validloss -2172.32166±0.00000  bestvalidloss -2192.64424  last_update 13\n",
      "train: iter 590  trainloss -2219.93875  validloss -2147.26917±0.00000  bestvalidloss -2192.64424  last_update 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 591  trainloss -2198.86118  validloss -2200.65443±0.00000  bestvalidloss -2200.65443  last_update 0\n",
      "train: iter 592  trainloss -2205.81354  validloss -2153.83973±0.00000  bestvalidloss -2200.65443  last_update 1\n",
      "train: iter 593  trainloss -2205.78013  validloss -2117.31196±0.00000  bestvalidloss -2200.65443  last_update 2\n",
      "train: iter 594  trainloss -2198.55502  validloss -2168.37632±0.00000  bestvalidloss -2200.65443  last_update 3\n",
      "train: iter 595  trainloss -2162.94713  validloss -2071.47480±0.00000  bestvalidloss -2200.65443  last_update 4\n",
      "train: iter 596  trainloss -2180.82496  validloss -2165.88754±0.00000  bestvalidloss -2200.65443  last_update 5\n",
      "train: iter 597  trainloss -2215.77679  validloss -2174.05067±0.00000  bestvalidloss -2200.65443  last_update 6\n",
      "train: iter 598  trainloss -2208.73276  validloss -2141.26099±0.00000  bestvalidloss -2200.65443  last_update 7\n",
      "train: iter 599  trainloss -2195.73808  validloss -2047.71173±0.00000  bestvalidloss -2200.65443  last_update 8\n",
      "train: iter 600  trainloss -2209.49377  validloss -2161.59071±0.00000  bestvalidloss -2200.65443  last_update 9\n",
      "train: iter 601  trainloss -2226.40187  validloss -2140.28889±0.00000  bestvalidloss -2200.65443  last_update 10\n",
      "train: iter 602  trainloss -2228.74784  validloss -2160.72332±0.00000  bestvalidloss -2200.65443  last_update 11\n",
      "train: iter 603  trainloss -2211.75988  validloss -2159.70986±0.00000  bestvalidloss -2200.65443  last_update 12\n",
      "train: iter 604  trainloss -2201.38411  validloss -2167.99337±0.00000  bestvalidloss -2200.65443  last_update 13\n",
      "train: iter 605  trainloss -2166.44430  validloss -2134.04769±0.00000  bestvalidloss -2200.65443  last_update 14\n",
      "train: iter 606  trainloss -2184.81730  validloss -2124.91899±0.00000  bestvalidloss -2200.65443  last_update 15\n",
      "train: iter 607  trainloss -2197.17182  validloss -2144.01708±0.00000  bestvalidloss -2200.65443  last_update 16\n",
      "train: iter 608  trainloss -2205.26828  validloss -2056.72416±0.00000  bestvalidloss -2200.65443  last_update 17\n",
      "train: iter 609  trainloss -2210.90083  validloss -2181.68710±0.00000  bestvalidloss -2200.65443  last_update 18\n",
      "train: iter 610  trainloss -2180.23389  validloss -2145.36606±0.00000  bestvalidloss -2200.65443  last_update 19\n",
      "train: iter 611  trainloss -2151.60464  validloss -2041.43055±0.00000  bestvalidloss -2200.65443  last_update 20\n",
      "train: iter 612  trainloss -2225.80600  validloss -2152.87430±0.00000  bestvalidloss -2200.65443  last_update 21\n",
      "train: iter 613  trainloss -2242.14092  validloss -2197.71388±0.00000  bestvalidloss -2200.65443  last_update 22\n",
      "train: iter 614  trainloss -2231.33855  validloss -2185.42142±0.00000  bestvalidloss -2200.65443  last_update 23\n",
      "train: iter 615  trainloss -2186.16643  validloss -2170.24146±0.00000  bestvalidloss -2200.65443  last_update 24\n",
      "train: iter 616  trainloss -2191.54729  validloss -2132.91913±0.00000  bestvalidloss -2200.65443  last_update 25\n",
      "train: iter 617  trainloss -2184.99429  validloss -1998.07693±0.00000  bestvalidloss -2200.65443  last_update 26\n",
      "train: iter 618  trainloss -2234.04302  validloss -2168.62887±0.00000  bestvalidloss -2200.65443  last_update 27\n",
      "train: iter 619  trainloss -2240.78105  validloss -2191.58708±0.00000  bestvalidloss -2200.65443  last_update 28\n",
      "train: iter 620  trainloss -2236.76129  validloss -2161.80507±0.00000  bestvalidloss -2200.65443  last_update 29\n",
      "train: iter 621  trainloss -2223.02410  validloss -2150.18019±0.00000  bestvalidloss -2200.65443  last_update 30\n",
      "train: iter 622  trainloss -2235.21500  validloss -2193.72129±0.00000  bestvalidloss -2200.65443  last_update 31\n",
      "train: iter 623  trainloss -2187.21339  validloss -2189.22686±0.00000  bestvalidloss -2200.65443  last_update 32\n",
      "train: iter 624  trainloss -2233.68733  validloss -2177.97737±0.00000  bestvalidloss -2200.65443  last_update 33\n",
      "train: iter 625  trainloss -2225.75948  validloss -2192.81052±0.00000  bestvalidloss -2200.65443  last_update 34\n",
      "train: iter 626  trainloss -2240.96825  validloss -2184.38088±0.00000  bestvalidloss -2200.65443  last_update 35\n",
      "train: iter 627  trainloss -2216.58438  validloss -2170.81948±0.00000  bestvalidloss -2200.65443  last_update 36\n",
      "train: iter 628  trainloss -2225.64729  validloss -2171.45215±0.00000  bestvalidloss -2200.65443  last_update 37\n",
      "train: iter 629  trainloss -2216.64597  validloss -2177.82481±0.00000  bestvalidloss -2200.65443  last_update 38\n",
      "train: iter 630  trainloss -2080.89489  validloss -2150.49611±0.00000  bestvalidloss -2200.65443  last_update 39\n",
      "train: iter 631  trainloss -2153.59958  validloss -2114.08705±0.00000  bestvalidloss -2200.65443  last_update 40\n",
      "train: iter 632  trainloss -2159.74209  validloss -1919.25585±0.00000  bestvalidloss -2200.65443  last_update 41\n",
      "train: iter 633  trainloss -2200.13648  validloss -2130.96234±0.00000  bestvalidloss -2200.65443  last_update 42\n",
      "train: iter 634  trainloss -2235.24499  validloss -2160.55177±0.00000  bestvalidloss -2200.65443  last_update 43\n",
      "train: iter 635  trainloss -2229.37568  validloss -2194.56359±0.00000  bestvalidloss -2200.65443  last_update 44\n",
      "train: iter 636  trainloss -2238.12146  validloss -2188.80901±0.00000  bestvalidloss -2200.65443  last_update 45\n",
      "train: iter 637  trainloss -2222.29443  validloss -2182.96914±0.00000  bestvalidloss -2200.65443  last_update 46\n",
      "train: iter 638  trainloss -2216.51152  validloss -2179.48385±0.00000  bestvalidloss -2200.65443  last_update 47\n",
      "train: iter 639  trainloss -2245.84912  validloss -2185.89561±0.00000  bestvalidloss -2200.65443  last_update 48\n",
      "train: iter 640  trainloss -2235.99044  validloss -2192.28864±0.00000  bestvalidloss -2200.65443  last_update 49\n",
      "train: iter 641  trainloss -2225.42491  validloss -2194.10461±0.00000  bestvalidloss -2200.65443  last_update 50\n",
      "train: iter 642  trainloss -2219.51853  validloss -2178.99416±0.00000  bestvalidloss -2200.65443  last_update 51\n",
      "train: iter 643  trainloss -2242.27946  validloss -2175.67171±0.00000  bestvalidloss -2200.65443  last_update 52\n",
      "train: iter 644  trainloss -2207.13212  validloss -2196.03570±0.00000  bestvalidloss -2200.65443  last_update 53\n",
      "train: iter 645  trainloss -2031.70718  validloss -2122.57427±0.00000  bestvalidloss -2200.65443  last_update 54\n",
      "train: iter 646  trainloss -2200.94986  validloss -2077.71921±0.00000  bestvalidloss -2200.65443  last_update 55\n",
      "train: iter 647  trainloss -2181.32093  validloss -2147.66061±0.00000  bestvalidloss -2200.65443  last_update 56\n",
      "train: iter 648  trainloss -2239.28874  validloss -2154.88173±0.00000  bestvalidloss -2200.65443  last_update 57\n",
      "train: iter 649  trainloss -2230.33043  validloss -2155.69644±0.00000  bestvalidloss -2200.65443  last_update 58\n",
      "train: iter 650  trainloss -2226.55156  validloss -2160.24931±0.00000  bestvalidloss -2200.65443  last_update 59\n",
      "train: iter 651  trainloss -2225.88561  validloss -2187.18178±0.00000  bestvalidloss -2200.65443  last_update 60\n",
      "train: iter 652  trainloss -2195.59951  validloss -2156.20787±0.00000  bestvalidloss -2200.65443  last_update 61\n",
      "train: iter 653  trainloss -2234.27464  validloss -2161.14489±0.00000  bestvalidloss -2200.65443  last_update 62\n",
      "train: iter 654  trainloss -2231.67991  validloss -2159.09260±0.00000  bestvalidloss -2200.65443  last_update 63\n",
      "train: iter 655  trainloss -2218.41318  validloss -2181.56179±0.00000  bestvalidloss -2200.65443  last_update 64\n",
      "train: iter 656  trainloss -2217.68925  validloss -2196.21659±0.00000  bestvalidloss -2200.65443  last_update 65\n",
      "train: iter 657  trainloss -2231.42197  validloss -2152.94235±0.00000  bestvalidloss -2200.65443  last_update 66\n",
      "train: iter 658  trainloss -2221.64704  validloss -2185.11595±0.00000  bestvalidloss -2200.65443  last_update 67\n",
      "train: iter 659  trainloss -2203.87460  validloss -2138.12950±0.00000  bestvalidloss -2200.65443  last_update 68\n",
      "train: iter 660  trainloss -2214.07136  validloss -2117.15823±0.00000  bestvalidloss -2200.65443  last_update 69\n",
      "train: iter 661  trainloss -2192.74609  validloss -2179.55952±0.00000  bestvalidloss -2200.65443  last_update 70\n",
      "train: iter 662  trainloss -2175.43486  validloss -2023.79871±0.00000  bestvalidloss -2200.65443  last_update 71\n",
      "train: iter 663  trainloss -2191.29937  validloss -2102.49413±0.00000  bestvalidloss -2200.65443  last_update 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 664  trainloss -2226.57253  validloss -2147.70820±0.00000  bestvalidloss -2200.65443  last_update 73\n",
      "train: iter 665  trainloss -2242.33705  validloss -2189.09240±0.00000  bestvalidloss -2200.65443  last_update 74\n",
      "train: iter 666  trainloss -2228.62498  validloss -2190.04686±0.00000  bestvalidloss -2200.65443  last_update 75\n",
      "train: iter 667  trainloss -2219.45770  validloss -2165.72397±0.00000  bestvalidloss -2200.65443  last_update 76\n",
      "train: iter 668  trainloss -2229.96911  validloss -2175.51041±0.00000  bestvalidloss -2200.65443  last_update 77\n",
      "train: iter 669  trainloss -2230.14344  validloss -2159.57975±0.00000  bestvalidloss -2200.65443  last_update 78\n",
      "train: iter 670  trainloss -2211.60903  validloss -2188.43558±0.00000  bestvalidloss -2200.65443  last_update 79\n",
      "train: iter 671  trainloss -2227.02695  validloss -2188.11417±0.00000  bestvalidloss -2200.65443  last_update 80\n",
      "train: iter 672  trainloss -2219.19868  validloss -2091.12083±0.00000  bestvalidloss -2200.65443  last_update 81\n",
      "train: iter 673  trainloss -2177.81014  validloss -2148.11639±0.00000  bestvalidloss -2200.65443  last_update 82\n",
      "train: iter 674  trainloss -2197.52470  validloss -2114.65063±0.00000  bestvalidloss -2200.65443  last_update 83\n",
      "train: iter 675  trainloss -2223.40352  validloss -2181.65880±0.00000  bestvalidloss -2200.65443  last_update 84\n",
      "train: iter 676  trainloss -2175.86445  validloss -2172.28742±0.00000  bestvalidloss -2200.65443  last_update 85\n",
      "train: iter 677  trainloss -2173.86364  validloss -2028.72748±0.00000  bestvalidloss -2200.65443  last_update 86\n",
      "train: iter 678  trainloss -2234.75325  validloss -2173.23927±0.00000  bestvalidloss -2200.65443  last_update 87\n",
      "train: iter 679  trainloss -2210.68556  validloss -2169.99863±0.00000  bestvalidloss -2200.65443  last_update 88\n",
      "train: iter 680  trainloss -2196.23722  validloss -2113.64988±0.00000  bestvalidloss -2200.65443  last_update 89\n",
      "train: iter 681  trainloss -2239.89260  validloss -2119.17073±0.00000  bestvalidloss -2200.65443  last_update 90\n",
      "train: iter 682  trainloss -2236.34163  validloss -2188.51816±0.00000  bestvalidloss -2200.65443  last_update 91\n",
      "train: iter 683  trainloss -2233.26802  validloss -2179.25583±0.00000  bestvalidloss -2200.65443  last_update 92\n",
      "train: iter 684  trainloss -2221.27880  validloss -2176.63986±0.00000  bestvalidloss -2200.65443  last_update 93\n",
      "train: iter 685  trainloss -2204.60633  validloss -2150.79619±0.00000  bestvalidloss -2200.65443  last_update 94\n",
      "train: iter 686  trainloss -2066.96891  validloss -2174.27762±0.00000  bestvalidloss -2200.65443  last_update 95\n",
      "train: iter 687  trainloss -2195.58477  validloss -2081.38572±0.00000  bestvalidloss -2200.65443  last_update 96\n",
      "train: iter 688  trainloss -2240.99545  validloss -2188.91766±0.00000  bestvalidloss -2200.65443  last_update 97\n",
      "train: iter 689  trainloss -2232.54681  validloss -2176.99690±0.00000  bestvalidloss -2200.65443  last_update 98\n",
      "train: iter 690  trainloss -2227.04868  validloss -2176.57769±0.00000  bestvalidloss -2200.65443  last_update 99\n",
      "train: iter 691  trainloss -2228.73316  validloss -2188.47191±0.00000  bestvalidloss -2200.65443  last_update 100\n",
      "train: fin\n",
      "penalty_target_min tensor(-12.3208) penalty_target_max tensor(2.6361)\n"
     ]
    }
   ],
   "source": [
    "train_curve1, valid_curve1 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=1)\n",
    "vi.update_mulogvar_offlinedata()\n",
    "\n",
    "# train_curve2, valid_curve2 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=2)\n",
    "# vi.update_mulogvar_offlinedata()\n",
    "\n",
    "# train_curve3, valid_curve3 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=3)\n",
    "# vi.update_mulogvar_offlinedata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c548788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_curve1)\n",
    "plt.plot(valid_curve1)\n",
    "# plt.ylim([-2000, 1000])\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(train_curve2)\n",
    "# plt.plot(valid_curve2)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_curve3)\n",
    "# plt.plot(valid_curve3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2656b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd778499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 0  trainloss 7.13203  validloss 7.46258±0.00000  bestvalidloss 7.46258  last_update 0\n",
      "train: iter 1  trainloss 6.54064  validloss 6.81418±0.00000  bestvalidloss 6.81418  last_update 0\n",
      "train: iter 2  trainloss 6.04491  validloss 6.27394±0.00000  bestvalidloss 6.27394  last_update 0\n",
      "train: iter 3  trainloss 5.63252  validloss 5.82609±0.00000  bestvalidloss 5.82609  last_update 0\n",
      "train: iter 4  trainloss 5.25744  validloss 5.43858±0.00000  bestvalidloss 5.43858  last_update 0\n",
      "train: iter 5  trainloss 4.93743  validloss 5.09524±0.00000  bestvalidloss 5.09524  last_update 0\n",
      "train: iter 6  trainloss 4.65609  validloss 4.77735±0.00000  bestvalidloss 4.77735  last_update 0\n",
      "train: iter 7  trainloss 4.39796  validloss 4.51505±0.00000  bestvalidloss 4.51505  last_update 0\n",
      "train: iter 8  trainloss 4.15979  validloss 4.27511±0.00000  bestvalidloss 4.27511  last_update 0\n",
      "train: iter 9  trainloss 3.94414  validloss 4.02852±0.00000  bestvalidloss 4.02852  last_update 0\n",
      "train: iter 10  trainloss 3.75113  validloss 3.83291±0.00000  bestvalidloss 3.83291  last_update 0\n",
      "train: iter 11  trainloss 3.57389  validloss 3.64731±0.00000  bestvalidloss 3.64731  last_update 0\n",
      "train: iter 12  trainloss 3.40540  validloss 3.47991±0.00000  bestvalidloss 3.47991  last_update 0\n",
      "train: iter 13  trainloss 3.25611  validloss 3.32787±0.00000  bestvalidloss 3.32787  last_update 0\n",
      "train: iter 14  trainloss 3.12768  validloss 3.18663±0.00000  bestvalidloss 3.18663  last_update 0\n",
      "train: iter 15  trainloss 3.00526  validloss 3.06011±0.00000  bestvalidloss 3.06011  last_update 0\n",
      "train: iter 16  trainloss 2.88317  validloss 2.94238±0.00000  bestvalidloss 2.94238  last_update 0\n",
      "train: iter 17  trainloss 2.78231  validloss 2.84082±0.00000  bestvalidloss 2.84082  last_update 0\n",
      "train: iter 18  trainloss 2.69207  validloss 2.74422±0.00000  bestvalidloss 2.74422  last_update 0\n",
      "train: iter 19  trainloss 2.60163  validloss 2.64116±0.00000  bestvalidloss 2.64116  last_update 0\n",
      "train: iter 20  trainloss 2.52543  validloss 2.56610±0.00000  bestvalidloss 2.56610  last_update 0\n",
      "train: iter 21  trainloss 2.45362  validloss 2.50461±0.00000  bestvalidloss 2.50461  last_update 0\n",
      "train: iter 22  trainloss 2.38117  validloss 2.42591±0.00000  bestvalidloss 2.42591  last_update 0\n",
      "train: iter 23  trainloss 2.32197  validloss 2.36209±0.00000  bestvalidloss 2.36209  last_update 0\n",
      "train: iter 24  trainloss 2.25872  validloss 2.29084±0.00000  bestvalidloss 2.29084  last_update 0\n",
      "train: iter 25  trainloss 2.20244  validloss 2.24321±0.00000  bestvalidloss 2.24321  last_update 0\n",
      "train: iter 26  trainloss 2.15287  validloss 2.18939±0.00000  bestvalidloss 2.18939  last_update 0\n",
      "train: iter 27  trainloss 2.09915  validloss 2.13488±0.00000  bestvalidloss 2.13488  last_update 0\n",
      "train: iter 28  trainloss 2.04321  validloss 2.08886±0.00000  bestvalidloss 2.08886  last_update 0\n",
      "train: iter 29  trainloss 2.00059  validloss 2.01028±0.00000  bestvalidloss 2.01028  last_update 0\n",
      "train: iter 30  trainloss 1.95347  validloss 1.98513±0.00000  bestvalidloss 1.98513  last_update 0\n",
      "train: iter 31  trainloss 1.92560  validloss 1.92313±0.00000  bestvalidloss 1.92313  last_update 0\n",
      "train: iter 32  trainloss 1.87124  validloss 1.88926±0.00000  bestvalidloss 1.88926  last_update 0\n",
      "train: iter 33  trainloss 1.84114  validloss 1.84371±0.00000  bestvalidloss 1.84371  last_update 0\n",
      "train: iter 34  trainloss 1.79939  validloss 1.80129±0.00000  bestvalidloss 1.80129  last_update 0\n",
      "train: iter 35  trainloss 1.75328  validloss 1.77875±0.00000  bestvalidloss 1.77875  last_update 0\n",
      "train: iter 36  trainloss 1.71037  validloss 1.73171±0.00000  bestvalidloss 1.73171  last_update 0\n",
      "train: iter 37  trainloss 1.68334  validloss 1.69451±0.00000  bestvalidloss 1.69451  last_update 0\n",
      "train: iter 38  trainloss 1.65177  validloss 1.65407±0.00000  bestvalidloss 1.65407  last_update 0\n",
      "train: iter 39  trainloss 1.61147  validloss 1.63558±0.00000  bestvalidloss 1.63558  last_update 0\n",
      "train: iter 40  trainloss 1.57590  validloss 1.57291±0.00000  bestvalidloss 1.57291  last_update 0\n",
      "train: iter 41  trainloss 1.54123  validloss 1.54295±0.00000  bestvalidloss 1.54295  last_update 0\n",
      "train: iter 42  trainloss 1.50463  validloss 1.48144±0.00000  bestvalidloss 1.48144  last_update 0\n",
      "train: iter 43  trainloss 1.45832  validloss 1.46817±0.00000  bestvalidloss 1.46817  last_update 0\n",
      "train: iter 44  trainloss 1.43177  validloss 1.42418±0.00000  bestvalidloss 1.42418  last_update 0\n",
      "train: iter 45  trainloss 1.38846  validloss 1.41486±0.00000  bestvalidloss 1.41486  last_update 0\n",
      "train: iter 46  trainloss 1.34509  validloss 1.35879±0.00000  bestvalidloss 1.35879  last_update 0\n",
      "train: iter 47  trainloss 1.29655  validloss 1.31345±0.00000  bestvalidloss 1.31345  last_update 0\n",
      "train: iter 48  trainloss 1.25535  validloss 1.26009±0.00000  bestvalidloss 1.26009  last_update 0\n",
      "train: iter 49  trainloss 1.19918  validloss 1.20529±0.00000  bestvalidloss 1.20529  last_update 0\n",
      "train: iter 50  trainloss 1.16748  validloss 1.13615±0.00000  bestvalidloss 1.13615  last_update 0\n",
      "train: iter 51  trainloss 1.10906  validloss 1.11177±0.00000  bestvalidloss 1.11177  last_update 0\n",
      "train: iter 52  trainloss 1.06634  validloss 1.04901±0.00000  bestvalidloss 1.04901  last_update 0\n",
      "train: iter 53  trainloss 1.03069  validloss 1.03159±0.00000  bestvalidloss 1.03159  last_update 0\n",
      "train: iter 54  trainloss 0.95974  validloss 0.96268±0.00000  bestvalidloss 0.96268  last_update 0\n",
      "train: iter 55  trainloss 0.91647  validloss 0.93743±0.00000  bestvalidloss 0.93743  last_update 0\n",
      "train: iter 56  trainloss 0.86180  validloss 0.84708±0.00000  bestvalidloss 0.84708  last_update 0\n",
      "train: iter 57  trainloss 0.81877  validloss 0.80512±0.00000  bestvalidloss 0.80512  last_update 0\n",
      "train: iter 58  trainloss 0.77657  validloss 0.77039±0.00000  bestvalidloss 0.77039  last_update 0\n",
      "train: iter 59  trainloss 0.75292  validloss 0.74031±0.00000  bestvalidloss 0.74031  last_update 0\n",
      "train: iter 60  trainloss 0.69049  validloss 0.66692±0.00000  bestvalidloss 0.66692  last_update 0\n",
      "train: iter 61  trainloss 0.63893  validloss 0.61137±0.00000  bestvalidloss 0.61137  last_update 0\n",
      "train: iter 62  trainloss 0.59246  validloss 0.63184±0.00000  bestvalidloss 0.61137  last_update 1\n",
      "train: iter 63  trainloss 0.56096  validloss 0.54036±0.00000  bestvalidloss 0.54036  last_update 0\n",
      "train: iter 64  trainloss 0.50993  validloss 0.49814±0.00000  bestvalidloss 0.49814  last_update 0\n",
      "train: iter 65  trainloss 0.46663  validloss 0.45142±0.00000  bestvalidloss 0.45142  last_update 0\n",
      "train: iter 66  trainloss 0.41447  validloss 0.41228±0.00000  bestvalidloss 0.41228  last_update 0\n",
      "train: iter 67  trainloss 0.38948  validloss 0.36490±0.00000  bestvalidloss 0.36490  last_update 0\n",
      "train: iter 68  trainloss 0.35099  validloss 0.33649±0.00000  bestvalidloss 0.33649  last_update 0\n",
      "train: iter 69  trainloss 0.30972  validloss 0.27988±0.00000  bestvalidloss 0.27988  last_update 0\n",
      "train: iter 70  trainloss 0.25238  validloss 0.25487±0.00000  bestvalidloss 0.25487  last_update 0\n",
      "train: iter 71  trainloss 0.22716  validloss 0.20507±0.00000  bestvalidloss 0.20507  last_update 0\n",
      "train: iter 72  trainloss 0.18334  validloss 0.17740±0.00000  bestvalidloss 0.17740  last_update 0\n",
      "train: iter 73  trainloss 0.14242  validloss 0.13409±0.00000  bestvalidloss 0.13409  last_update 0\n",
      "train: iter 74  trainloss 0.12339  validloss 0.12000±0.00000  bestvalidloss 0.12000  last_update 0\n",
      "train: iter 75  trainloss 0.06774  validloss 0.04862±0.00000  bestvalidloss 0.04862  last_update 0\n",
      "train: iter 76  trainloss 0.04523  validloss 0.02944±0.00000  bestvalidloss 0.02944  last_update 0\n",
      "train: iter 77  trainloss 0.00463  validloss -0.00090±0.00000  bestvalidloss -0.00090  last_update 0\n",
      "train: iter 78  trainloss -0.03186  validloss -0.03241±0.00000  bestvalidloss -0.03241  last_update 0\n",
      "train: iter 79  trainloss -0.07126  validloss -0.09115±0.00000  bestvalidloss -0.09115  last_update 0\n",
      "train: iter 80  trainloss -0.11313  validloss -0.12133±0.00000  bestvalidloss -0.12133  last_update 0\n",
      "train: iter 81  trainloss -0.15843  validloss -0.15969±0.00000  bestvalidloss -0.15969  last_update 0\n",
      "train: iter 82  trainloss -0.17447  validloss -0.23996±0.00000  bestvalidloss -0.23996  last_update 0\n",
      "train: iter 83  trainloss -0.20727  validloss -0.24880±0.00000  bestvalidloss -0.24880  last_update 0\n",
      "train: iter 84  trainloss -0.25663  validloss -0.25414±0.00000  bestvalidloss -0.25414  last_update 0\n",
      "train: iter 85  trainloss -0.28897  validloss -0.28552±0.00000  bestvalidloss -0.28552  last_update 0\n",
      "train: iter 86  trainloss -0.31544  validloss -0.35021±0.00000  bestvalidloss -0.35021  last_update 0\n",
      "train: iter 87  trainloss -0.36735  validloss -0.37185±0.00000  bestvalidloss -0.37185  last_update 0\n",
      "train: iter 88  trainloss -0.37648  validloss -0.38262±0.00000  bestvalidloss -0.38262  last_update 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 89  trainloss -0.42634  validloss -0.44586±0.00000  bestvalidloss -0.44586  last_update 0\n",
      "train: iter 90  trainloss -0.44775  validloss -0.46214±0.00000  bestvalidloss -0.46214  last_update 0\n",
      "train: iter 91  trainloss -0.50928  validloss -0.48342±0.00000  bestvalidloss -0.48342  last_update 0\n",
      "train: iter 92  trainloss -0.52667  validloss -0.51483±0.00000  bestvalidloss -0.51483  last_update 0\n",
      "train: iter 93  trainloss -0.54062  validloss -0.57754±0.00000  bestvalidloss -0.57754  last_update 0\n",
      "train: iter 94  trainloss -0.57628  validloss -0.61653±0.00000  bestvalidloss -0.61653  last_update 0\n",
      "train: iter 95  trainloss -0.60260  validloss -0.61597±0.00000  bestvalidloss -0.61653  last_update 1\n",
      "train: iter 96  trainloss -0.64854  validloss -0.64927±0.00000  bestvalidloss -0.64927  last_update 0\n",
      "train: iter 97  trainloss -0.66577  validloss -0.73709±0.00000  bestvalidloss -0.73709  last_update 0\n",
      "train: iter 98  trainloss -0.67637  validloss -0.69564±0.00000  bestvalidloss -0.73709  last_update 1\n",
      "train: iter 99  trainloss -0.72174  validloss -0.70150±0.00000  bestvalidloss -0.73709  last_update 2\n",
      "train: iter 100  trainloss -0.74603  validloss -0.74120±0.00000  bestvalidloss -0.74120  last_update 0\n",
      "train: iter 101  trainloss -0.75739  validloss -0.74450±0.00000  bestvalidloss -0.74450  last_update 0\n",
      "train: iter 102  trainloss -0.81029  validloss -0.82273±0.00000  bestvalidloss -0.82273  last_update 0\n",
      "train: iter 103  trainloss -0.81003  validloss -0.81549±0.00000  bestvalidloss -0.82273  last_update 1\n",
      "train: iter 104  trainloss -0.84569  validloss -0.84505±0.00000  bestvalidloss -0.84505  last_update 0\n",
      "train: iter 105  trainloss -0.85196  validloss -0.86850±0.00000  bestvalidloss -0.86850  last_update 0\n",
      "train: iter 106  trainloss -0.85654  validloss -0.84555±0.00000  bestvalidloss -0.86850  last_update 1\n",
      "train: iter 107  trainloss -0.89693  validloss -0.89912±0.00000  bestvalidloss -0.89912  last_update 0\n",
      "train: iter 108  trainloss -0.89033  validloss -0.83010±0.00000  bestvalidloss -0.89912  last_update 1\n",
      "train: iter 109  trainloss -0.88574  validloss -0.93648±0.00000  bestvalidloss -0.93648  last_update 0\n",
      "train: iter 110  trainloss -0.95131  validloss -0.95529±0.00000  bestvalidloss -0.95529  last_update 0\n",
      "train: iter 111  trainloss -0.96692  validloss -1.00731±0.00000  bestvalidloss -1.00731  last_update 0\n",
      "train: iter 112  trainloss -0.98581  validloss -0.93442±0.00000  bestvalidloss -1.00731  last_update 1\n",
      "train: iter 113  trainloss -0.97965  validloss -1.00185±0.00000  bestvalidloss -1.00731  last_update 2\n",
      "train: iter 114  trainloss -0.94156  validloss -0.94660±0.00000  bestvalidloss -1.00731  last_update 3\n",
      "train: iter 115  trainloss -0.99372  validloss -1.03760±0.00000  bestvalidloss -1.03760  last_update 0\n",
      "train: iter 116  trainloss -0.97047  validloss -1.01718±0.00000  bestvalidloss -1.03760  last_update 1\n",
      "train: iter 117  trainloss -0.99087  validloss -1.04182±0.00000  bestvalidloss -1.04182  last_update 0\n",
      "train: iter 118  trainloss -0.99876  validloss -1.02137±0.00000  bestvalidloss -1.04182  last_update 1\n",
      "train: iter 119  trainloss -0.96358  validloss -0.94041±0.00000  bestvalidloss -1.04182  last_update 2\n",
      "train: iter 120  trainloss -1.00115  validloss -1.12089±0.00000  bestvalidloss -1.12089  last_update 0\n",
      "train: iter 121  trainloss -1.06850  validloss -1.00332±0.00000  bestvalidloss -1.12089  last_update 1\n",
      "train: iter 122  trainloss -1.04313  validloss -1.03658±0.00000  bestvalidloss -1.12089  last_update 2\n",
      "train: iter 123  trainloss -0.97712  validloss -1.01203±0.00000  bestvalidloss -1.12089  last_update 3\n",
      "train: iter 124  trainloss -1.03975  validloss -1.08526±0.00000  bestvalidloss -1.12089  last_update 4\n",
      "train: iter 125  trainloss -1.03097  validloss -1.03455±0.00000  bestvalidloss -1.12089  last_update 5\n",
      "train: iter 126  trainloss -1.02617  validloss -1.09950±0.00000  bestvalidloss -1.12089  last_update 6\n",
      "train: iter 127  trainloss -1.03464  validloss -1.00754±0.00000  bestvalidloss -1.12089  last_update 7\n",
      "train: iter 128  trainloss -1.00669  validloss -1.09841±0.00000  bestvalidloss -1.12089  last_update 8\n",
      "train: iter 129  trainloss -1.01071  validloss -0.95713±0.00000  bestvalidloss -1.12089  last_update 9\n",
      "train: iter 130  trainloss -1.00713  validloss -0.95826±0.00000  bestvalidloss -1.12089  last_update 10\n",
      "train: iter 131  trainloss -1.00773  validloss -1.05946±0.00000  bestvalidloss -1.12089  last_update 11\n",
      "train: iter 132  trainloss -1.02251  validloss -1.05950±0.00000  bestvalidloss -1.12089  last_update 12\n",
      "train: iter 133  trainloss -1.00962  validloss -1.09426±0.00000  bestvalidloss -1.12089  last_update 13\n",
      "train: iter 134  trainloss -1.01077  validloss -1.02549±0.00000  bestvalidloss -1.12089  last_update 14\n",
      "train: iter 135  trainloss -1.00092  validloss -1.02359±0.00000  bestvalidloss -1.12089  last_update 15\n",
      "train: iter 136  trainloss -1.02827  validloss -1.13716±0.00000  bestvalidloss -1.13716  last_update 0\n",
      "train: iter 137  trainloss -1.04821  validloss -1.12675±0.00000  bestvalidloss -1.13716  last_update 1\n",
      "train: iter 138  trainloss -1.01895  validloss -1.06970±0.00000  bestvalidloss -1.13716  last_update 2\n",
      "train: iter 139  trainloss -1.03303  validloss -1.02340±0.00000  bestvalidloss -1.13716  last_update 3\n",
      "train: iter 140  trainloss -1.03949  validloss -1.02553±0.00000  bestvalidloss -1.13716  last_update 4\n",
      "train: iter 141  trainloss -1.03779  validloss -0.94324±0.00000  bestvalidloss -1.13716  last_update 5\n",
      "train: iter 142  trainloss -1.04248  validloss -1.05881±0.00000  bestvalidloss -1.13716  last_update 6\n",
      "train: iter 143  trainloss -1.02531  validloss -0.98826±0.00000  bestvalidloss -1.13716  last_update 7\n",
      "train: iter 144  trainloss -1.03870  validloss -1.10684±0.00000  bestvalidloss -1.13716  last_update 8\n",
      "train: iter 145  trainloss -1.03801  validloss -1.01738±0.00000  bestvalidloss -1.13716  last_update 9\n",
      "train: iter 146  trainloss -1.01681  validloss -1.03003±0.00000  bestvalidloss -1.13716  last_update 10\n",
      "train: iter 147  trainloss -1.05795  validloss -1.02866±0.00000  bestvalidloss -1.13716  last_update 11\n",
      "train: iter 148  trainloss -1.04175  validloss -1.06876±0.00000  bestvalidloss -1.13716  last_update 12\n",
      "train: iter 149  trainloss -1.05449  validloss -1.04670±0.00000  bestvalidloss -1.13716  last_update 13\n",
      "train: iter 150  trainloss -1.00284  validloss -1.01595±0.00000  bestvalidloss -1.13716  last_update 14\n",
      "train: iter 151  trainloss -1.04721  validloss -1.06721±0.00000  bestvalidloss -1.13716  last_update 15\n",
      "train: iter 152  trainloss -1.03244  validloss -1.10015±0.00000  bestvalidloss -1.13716  last_update 16\n",
      "train: iter 153  trainloss -1.05306  validloss -1.06347±0.00000  bestvalidloss -1.13716  last_update 17\n",
      "train: iter 154  trainloss -1.02019  validloss -1.09386±0.00000  bestvalidloss -1.13716  last_update 18\n",
      "train: iter 155  trainloss -1.04637  validloss -1.03669±0.00000  bestvalidloss -1.13716  last_update 19\n",
      "train: iter 156  trainloss -0.99627  validloss -1.04652±0.00000  bestvalidloss -1.13716  last_update 20\n",
      "train: iter 157  trainloss -1.06648  validloss -1.05699±0.00000  bestvalidloss -1.13716  last_update 21\n",
      "train: iter 158  trainloss -1.03588  validloss -0.98497±0.00000  bestvalidloss -1.13716  last_update 22\n",
      "train: iter 159  trainloss -0.99392  validloss -1.02595±0.00000  bestvalidloss -1.13716  last_update 23\n",
      "train: iter 160  trainloss -0.98926  validloss -1.01253±0.00000  bestvalidloss -1.13716  last_update 24\n",
      "train: iter 161  trainloss -1.00920  validloss -0.95617±0.00000  bestvalidloss -1.13716  last_update 25\n",
      "train: iter 162  trainloss -1.00465  validloss -0.90168±0.00000  bestvalidloss -1.13716  last_update 26\n",
      "train: iter 163  trainloss -1.02413  validloss -1.01670±0.00000  bestvalidloss -1.13716  last_update 27\n",
      "train: iter 164  trainloss -0.97691  validloss -0.98845±0.00000  bestvalidloss -1.13716  last_update 28\n",
      "train: iter 165  trainloss -0.99678  validloss -0.97608±0.00000  bestvalidloss -1.13716  last_update 29\n",
      "train: iter 166  trainloss -1.02804  validloss -1.01584±0.00000  bestvalidloss -1.13716  last_update 30\n",
      "train: iter 167  trainloss -1.01966  validloss -1.04028±0.00000  bestvalidloss -1.13716  last_update 31\n",
      "train: iter 168  trainloss -1.04472  validloss -1.00102±0.00000  bestvalidloss -1.13716  last_update 32\n",
      "train: iter 169  trainloss -1.04620  validloss -1.00849±0.00000  bestvalidloss -1.13716  last_update 33\n",
      "train: iter 170  trainloss -1.05423  validloss -1.05561±0.00000  bestvalidloss -1.13716  last_update 34\n",
      "train: iter 171  trainloss -1.05617  validloss -1.01867±0.00000  bestvalidloss -1.13716  last_update 35\n",
      "train: iter 172  trainloss -1.03751  validloss -0.96927±0.00000  bestvalidloss -1.13716  last_update 36\n",
      "train: iter 173  trainloss -1.04141  validloss -1.00843±0.00000  bestvalidloss -1.13716  last_update 37\n",
      "train: iter 174  trainloss -1.06440  validloss -1.06494±0.00000  bestvalidloss -1.13716  last_update 38\n",
      "train: iter 175  trainloss -1.04339  validloss -1.07573±0.00000  bestvalidloss -1.13716  last_update 39\n",
      "train: iter 176  trainloss -1.02778  validloss -0.90241±0.00000  bestvalidloss -1.13716  last_update 40\n",
      "train: iter 177  trainloss -1.02684  validloss -0.98149±0.00000  bestvalidloss -1.13716  last_update 41\n",
      "train: iter 178  trainloss -1.03703  validloss -1.05552±0.00000  bestvalidloss -1.13716  last_update 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 179  trainloss -1.08323  validloss -1.11102±0.00000  bestvalidloss -1.13716  last_update 43\n",
      "train: iter 180  trainloss -1.05697  validloss -1.07364±0.00000  bestvalidloss -1.13716  last_update 44\n",
      "train: iter 181  trainloss -1.01393  validloss -1.13976±0.00000  bestvalidloss -1.13976  last_update 0\n",
      "train: iter 182  trainloss -1.03397  validloss -1.08090±0.00000  bestvalidloss -1.13976  last_update 1\n",
      "train: iter 183  trainloss -1.03355  validloss -1.01135±0.00000  bestvalidloss -1.13976  last_update 2\n",
      "train: iter 184  trainloss -1.00996  validloss -1.15230±0.00000  bestvalidloss -1.15230  last_update 0\n",
      "train: iter 185  trainloss -1.08979  validloss -0.99547±0.00000  bestvalidloss -1.15230  last_update 1\n",
      "train: iter 186  trainloss -1.07898  validloss -0.95765±0.00000  bestvalidloss -1.15230  last_update 2\n",
      "train: iter 187  trainloss -1.08682  validloss -1.02124±0.00000  bestvalidloss -1.15230  last_update 3\n",
      "train: iter 188  trainloss -1.01749  validloss -1.01442±0.00000  bestvalidloss -1.15230  last_update 4\n",
      "train: iter 189  trainloss -1.03062  validloss -1.00089±0.00000  bestvalidloss -1.15230  last_update 5\n",
      "train: iter 190  trainloss -1.03724  validloss -0.97838±0.00000  bestvalidloss -1.15230  last_update 6\n",
      "train: iter 191  trainloss -1.02647  validloss -0.94399±0.00000  bestvalidloss -1.15230  last_update 7\n",
      "train: iter 192  trainloss -0.99157  validloss -1.05161±0.00000  bestvalidloss -1.15230  last_update 8\n",
      "train: iter 193  trainloss -1.03124  validloss -1.00962±0.00000  bestvalidloss -1.15230  last_update 9\n",
      "train: iter 194  trainloss -1.06572  validloss -1.02006±0.00000  bestvalidloss -1.15230  last_update 10\n",
      "train: iter 195  trainloss -1.02619  validloss -1.03862±0.00000  bestvalidloss -1.15230  last_update 11\n",
      "train: iter 196  trainloss -1.08543  validloss -1.14802±0.00000  bestvalidloss -1.15230  last_update 12\n",
      "train: iter 197  trainloss -1.04950  validloss -1.04495±0.00000  bestvalidloss -1.15230  last_update 13\n",
      "train: iter 198  trainloss -1.04415  validloss -1.02915±0.00000  bestvalidloss -1.15230  last_update 14\n",
      "train: iter 199  trainloss -1.02641  validloss -1.01980±0.00000  bestvalidloss -1.15230  last_update 15\n",
      "train: iter 200  trainloss -1.05394  validloss -1.00014±0.00000  bestvalidloss -1.15230  last_update 16\n",
      "train: iter 201  trainloss -1.07773  validloss -1.00837±0.00000  bestvalidloss -1.15230  last_update 17\n",
      "train: iter 202  trainloss -1.03010  validloss -1.14828±0.00000  bestvalidloss -1.15230  last_update 18\n",
      "train: iter 203  trainloss -1.10954  validloss -0.95059±0.00000  bestvalidloss -1.15230  last_update 19\n",
      "train: iter 204  trainloss -0.97155  validloss -1.04011±0.00000  bestvalidloss -1.15230  last_update 20\n",
      "train: iter 205  trainloss -1.03631  validloss -1.05395±0.00000  bestvalidloss -1.15230  last_update 21\n",
      "train: iter 206  trainloss -1.04864  validloss -1.06419±0.00000  bestvalidloss -1.15230  last_update 22\n",
      "train: iter 207  trainloss -1.05212  validloss -1.03146±0.00000  bestvalidloss -1.15230  last_update 23\n",
      "train: iter 208  trainloss -1.02007  validloss -1.04013±0.00000  bestvalidloss -1.15230  last_update 24\n",
      "train: iter 209  trainloss -1.04755  validloss -1.10970±0.00000  bestvalidloss -1.15230  last_update 25\n",
      "train: iter 210  trainloss -1.06297  validloss -1.09047±0.00000  bestvalidloss -1.15230  last_update 26\n",
      "train: iter 211  trainloss -1.02620  validloss -1.02978±0.00000  bestvalidloss -1.15230  last_update 27\n",
      "train: iter 212  trainloss -1.02242  validloss -1.03081±0.00000  bestvalidloss -1.15230  last_update 28\n",
      "train: iter 213  trainloss -1.03071  validloss -1.12890±0.00000  bestvalidloss -1.15230  last_update 29\n",
      "train: iter 214  trainloss -1.02976  validloss -1.03224±0.00000  bestvalidloss -1.15230  last_update 30\n",
      "train: iter 215  trainloss -1.06571  validloss -0.98102±0.00000  bestvalidloss -1.15230  last_update 31\n",
      "train: iter 216  trainloss -1.04830  validloss -1.05412±0.00000  bestvalidloss -1.15230  last_update 32\n",
      "train: iter 217  trainloss -0.99733  validloss -1.02363±0.00000  bestvalidloss -1.15230  last_update 33\n",
      "train: iter 218  trainloss -1.05635  validloss -1.07254±0.00000  bestvalidloss -1.15230  last_update 34\n",
      "train: iter 219  trainloss -1.04690  validloss -1.05082±0.00000  bestvalidloss -1.15230  last_update 35\n",
      "train: iter 220  trainloss -1.03624  validloss -1.05797±0.00000  bestvalidloss -1.15230  last_update 36\n",
      "train: iter 221  trainloss -1.01314  validloss -1.03754±0.00000  bestvalidloss -1.15230  last_update 37\n",
      "train: iter 222  trainloss -1.08235  validloss -0.96462±0.00000  bestvalidloss -1.15230  last_update 38\n",
      "train: iter 223  trainloss -1.06173  validloss -0.97975±0.00000  bestvalidloss -1.15230  last_update 39\n",
      "train: iter 224  trainloss -1.01463  validloss -1.07072±0.00000  bestvalidloss -1.15230  last_update 40\n",
      "train: iter 225  trainloss -1.03483  validloss -0.98612±0.00000  bestvalidloss -1.15230  last_update 41\n",
      "train: iter 226  trainloss -1.00778  validloss -0.99566±0.00000  bestvalidloss -1.15230  last_update 42\n",
      "train: iter 227  trainloss -1.06640  validloss -0.97825±0.00000  bestvalidloss -1.15230  last_update 43\n",
      "train: iter 228  trainloss -1.06145  validloss -1.04594±0.00000  bestvalidloss -1.15230  last_update 44\n",
      "train: iter 229  trainloss -1.05309  validloss -1.07570±0.00000  bestvalidloss -1.15230  last_update 45\n",
      "train: iter 230  trainloss -0.99566  validloss -1.09413±0.00000  bestvalidloss -1.15230  last_update 46\n",
      "train: iter 231  trainloss -1.04087  validloss -1.04247±0.00000  bestvalidloss -1.15230  last_update 47\n",
      "train: iter 232  trainloss -1.06451  validloss -1.15404±0.00000  bestvalidloss -1.15404  last_update 0\n",
      "train: iter 233  trainloss -0.97459  validloss -0.99209±0.00000  bestvalidloss -1.15404  last_update 1\n",
      "train: iter 234  trainloss -1.03165  validloss -0.98247±0.00000  bestvalidloss -1.15404  last_update 2\n",
      "train: iter 235  trainloss -1.01736  validloss -1.03039±0.00000  bestvalidloss -1.15404  last_update 3\n",
      "train: iter 236  trainloss -1.02235  validloss -1.10309±0.00000  bestvalidloss -1.15404  last_update 4\n",
      "train: iter 237  trainloss -1.01043  validloss -1.03797±0.00000  bestvalidloss -1.15404  last_update 5\n",
      "train: iter 238  trainloss -1.04526  validloss -1.08306±0.00000  bestvalidloss -1.15404  last_update 6\n",
      "train: iter 239  trainloss -1.04059  validloss -1.13184±0.00000  bestvalidloss -1.15404  last_update 7\n",
      "train: iter 240  trainloss -1.02945  validloss -1.01585±0.00000  bestvalidloss -1.15404  last_update 8\n",
      "train: iter 241  trainloss -1.04002  validloss -0.98623±0.00000  bestvalidloss -1.15404  last_update 9\n",
      "train: iter 242  trainloss -1.04649  validloss -0.96529±0.00000  bestvalidloss -1.15404  last_update 10\n",
      "train: iter 243  trainloss -1.00239  validloss -1.00866±0.00000  bestvalidloss -1.15404  last_update 11\n",
      "train: iter 244  trainloss -1.05729  validloss -1.02204±0.00000  bestvalidloss -1.15404  last_update 12\n",
      "train: iter 245  trainloss -1.06521  validloss -1.12579±0.00000  bestvalidloss -1.15404  last_update 13\n",
      "train: iter 246  trainloss -1.01202  validloss -1.05371±0.00000  bestvalidloss -1.15404  last_update 14\n",
      "train: iter 247  trainloss -1.07979  validloss -1.03333±0.00000  bestvalidloss -1.15404  last_update 15\n",
      "train: iter 248  trainloss -1.00701  validloss -1.04967±0.00000  bestvalidloss -1.15404  last_update 16\n",
      "train: iter 249  trainloss -1.03867  validloss -1.03207±0.00000  bestvalidloss -1.15404  last_update 17\n",
      "train: iter 250  trainloss -1.04088  validloss -1.08349±0.00000  bestvalidloss -1.15404  last_update 18\n",
      "train: iter 251  trainloss -1.04611  validloss -1.09509±0.00000  bestvalidloss -1.15404  last_update 19\n",
      "train: iter 252  trainloss -1.03981  validloss -1.00212±0.00000  bestvalidloss -1.15404  last_update 20\n",
      "train: iter 253  trainloss -0.98794  validloss -1.09834±0.00000  bestvalidloss -1.15404  last_update 21\n",
      "train: iter 254  trainloss -1.05193  validloss -1.00343±0.00000  bestvalidloss -1.15404  last_update 22\n",
      "train: iter 255  trainloss -1.07070  validloss -1.04471±0.00000  bestvalidloss -1.15404  last_update 23\n",
      "train: iter 256  trainloss -1.04181  validloss -1.02606±0.00000  bestvalidloss -1.15404  last_update 24\n",
      "train: iter 257  trainloss -1.05163  validloss -1.06070±0.00000  bestvalidloss -1.15404  last_update 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 258  trainloss -0.98037  validloss -0.95724±0.00000  bestvalidloss -1.15404  last_update 26\n",
      "train: iter 259  trainloss -1.03880  validloss -1.03352±0.00000  bestvalidloss -1.15404  last_update 27\n",
      "train: iter 260  trainloss -1.01767  validloss -1.02736±0.00000  bestvalidloss -1.15404  last_update 28\n",
      "train: iter 261  trainloss -0.99775  validloss -1.04361±0.00000  bestvalidloss -1.15404  last_update 29\n",
      "train: iter 262  trainloss -1.01995  validloss -1.11689±0.00000  bestvalidloss -1.15404  last_update 30\n",
      "train: iter 263  trainloss -1.01896  validloss -0.98351±0.00000  bestvalidloss -1.15404  last_update 31\n",
      "train: iter 264  trainloss -1.02611  validloss -1.08362±0.00000  bestvalidloss -1.15404  last_update 32\n",
      "train: iter 265  trainloss -1.04683  validloss -1.05430±0.00000  bestvalidloss -1.15404  last_update 33\n",
      "train: iter 266  trainloss -1.03576  validloss -0.93309±0.00000  bestvalidloss -1.15404  last_update 34\n",
      "train: iter 267  trainloss -1.01193  validloss -1.06042±0.00000  bestvalidloss -1.15404  last_update 35\n",
      "train: iter 268  trainloss -1.01419  validloss -1.00761±0.00000  bestvalidloss -1.15404  last_update 36\n",
      "train: iter 269  trainloss -0.99323  validloss -0.98649±0.00000  bestvalidloss -1.15404  last_update 37\n",
      "train: iter 270  trainloss -1.03144  validloss -1.00790±0.00000  bestvalidloss -1.15404  last_update 38\n",
      "train: iter 271  trainloss -1.02463  validloss -1.10060±0.00000  bestvalidloss -1.15404  last_update 39\n",
      "train: iter 272  trainloss -1.03830  validloss -1.01816±0.00000  bestvalidloss -1.15404  last_update 40\n",
      "train: iter 273  trainloss -1.02114  validloss -0.95639±0.00000  bestvalidloss -1.15404  last_update 41\n",
      "train: iter 274  trainloss -1.03473  validloss -1.06483±0.00000  bestvalidloss -1.15404  last_update 42\n",
      "train: iter 275  trainloss -0.98010  validloss -0.98777±0.00000  bestvalidloss -1.15404  last_update 43\n",
      "train: iter 276  trainloss -1.00927  validloss -1.10317±0.00000  bestvalidloss -1.15404  last_update 44\n",
      "train: iter 277  trainloss -1.00614  validloss -1.08301±0.00000  bestvalidloss -1.15404  last_update 45\n",
      "train: iter 278  trainloss -1.01407  validloss -1.01014±0.00000  bestvalidloss -1.15404  last_update 46\n",
      "train: iter 279  trainloss -1.04926  validloss -0.96236±0.00000  bestvalidloss -1.15404  last_update 47\n",
      "train: iter 280  trainloss -1.07396  validloss -1.06401±0.00000  bestvalidloss -1.15404  last_update 48\n",
      "train: iter 281  trainloss -1.01459  validloss -1.02094±0.00000  bestvalidloss -1.15404  last_update 49\n",
      "train: iter 282  trainloss -1.05633  validloss -1.04626±0.00000  bestvalidloss -1.15404  last_update 50\n",
      "train: iter 283  trainloss -1.07110  validloss -1.11470±0.00000  bestvalidloss -1.15404  last_update 51\n",
      "train: iter 284  trainloss -1.04846  validloss -1.03027±0.00000  bestvalidloss -1.15404  last_update 52\n",
      "train: iter 285  trainloss -1.02912  validloss -1.06569±0.00000  bestvalidloss -1.15404  last_update 53\n",
      "train: iter 286  trainloss -1.00089  validloss -1.02179±0.00000  bestvalidloss -1.15404  last_update 54\n",
      "train: iter 287  trainloss -1.01335  validloss -0.95264±0.00000  bestvalidloss -1.15404  last_update 55\n",
      "train: iter 288  trainloss -1.05246  validloss -1.06471±0.00000  bestvalidloss -1.15404  last_update 56\n",
      "train: iter 289  trainloss -1.03555  validloss -1.03538±0.00000  bestvalidloss -1.15404  last_update 57\n",
      "train: iter 290  trainloss -1.02923  validloss -0.98843±0.00000  bestvalidloss -1.15404  last_update 58\n",
      "train: iter 291  trainloss -1.04360  validloss -1.08243±0.00000  bestvalidloss -1.15404  last_update 59\n",
      "train: iter 292  trainloss -1.03401  validloss -0.90627±0.00000  bestvalidloss -1.15404  last_update 60\n",
      "train: iter 293  trainloss -1.03047  validloss -1.04752±0.00000  bestvalidloss -1.15404  last_update 61\n",
      "train: iter 294  trainloss -1.04430  validloss -0.99895±0.00000  bestvalidloss -1.15404  last_update 62\n",
      "train: iter 295  trainloss -1.06265  validloss -0.99050±0.00000  bestvalidloss -1.15404  last_update 63\n",
      "train: iter 296  trainloss -1.01913  validloss -0.96944±0.00000  bestvalidloss -1.15404  last_update 64\n",
      "train: iter 297  trainloss -1.03520  validloss -1.10633±0.00000  bestvalidloss -1.15404  last_update 65\n",
      "train: iter 298  trainloss -1.04207  validloss -1.02592±0.00000  bestvalidloss -1.15404  last_update 66\n",
      "train: iter 299  trainloss -1.02011  validloss -1.02857±0.00000  bestvalidloss -1.15404  last_update 67\n",
      "train: iter 300  trainloss -1.08353  validloss -1.09326±0.00000  bestvalidloss -1.15404  last_update 68\n",
      "train: iter 301  trainloss -1.08648  validloss -1.00560±0.00000  bestvalidloss -1.15404  last_update 69\n",
      "train: iter 302  trainloss -1.00468  validloss -1.07367±0.00000  bestvalidloss -1.15404  last_update 70\n",
      "train: iter 303  trainloss -1.03482  validloss -0.97883±0.00000  bestvalidloss -1.15404  last_update 71\n",
      "train: iter 304  trainloss -1.06038  validloss -0.96008±0.00000  bestvalidloss -1.15404  last_update 72\n",
      "train: iter 305  trainloss -1.02046  validloss -0.99218±0.00000  bestvalidloss -1.15404  last_update 73\n",
      "train: iter 306  trainloss -1.02079  validloss -1.04355±0.00000  bestvalidloss -1.15404  last_update 74\n",
      "train: iter 307  trainloss -1.03656  validloss -1.04028±0.00000  bestvalidloss -1.15404  last_update 75\n",
      "train: iter 308  trainloss -1.03864  validloss -1.13426±0.00000  bestvalidloss -1.15404  last_update 76\n",
      "train: iter 309  trainloss -1.06247  validloss -1.02575±0.00000  bestvalidloss -1.15404  last_update 77\n",
      "train: iter 310  trainloss -1.04957  validloss -0.99351±0.00000  bestvalidloss -1.15404  last_update 78\n",
      "train: iter 311  trainloss -1.02095  validloss -1.06602±0.00000  bestvalidloss -1.15404  last_update 79\n",
      "train: iter 312  trainloss -1.01921  validloss -0.98452±0.00000  bestvalidloss -1.15404  last_update 80\n",
      "train: iter 313  trainloss -1.05291  validloss -0.99337±0.00000  bestvalidloss -1.15404  last_update 81\n",
      "train: iter 314  trainloss -1.02948  validloss -1.08084±0.00000  bestvalidloss -1.15404  last_update 82\n",
      "train: iter 315  trainloss -1.08487  validloss -1.00398±0.00000  bestvalidloss -1.15404  last_update 83\n",
      "train: iter 316  trainloss -1.05208  validloss -0.97412±0.00000  bestvalidloss -1.15404  last_update 84\n",
      "train: iter 317  trainloss -1.06492  validloss -0.94976±0.00000  bestvalidloss -1.15404  last_update 85\n",
      "train: iter 318  trainloss -1.01766  validloss -0.90120±0.00000  bestvalidloss -1.15404  last_update 86\n",
      "train: iter 319  trainloss -1.04041  validloss -0.96187±0.00000  bestvalidloss -1.15404  last_update 87\n",
      "train: iter 320  trainloss -1.01574  validloss -1.11898±0.00000  bestvalidloss -1.15404  last_update 88\n",
      "train: iter 321  trainloss -1.07469  validloss -0.95774±0.00000  bestvalidloss -1.15404  last_update 89\n",
      "train: iter 322  trainloss -1.01279  validloss -1.07641±0.00000  bestvalidloss -1.15404  last_update 90\n",
      "train: iter 323  trainloss -0.99122  validloss -1.06362±0.00000  bestvalidloss -1.15404  last_update 91\n",
      "train: iter 324  trainloss -1.03898  validloss -1.02417±0.00000  bestvalidloss -1.15404  last_update 92\n",
      "train: iter 325  trainloss -1.03020  validloss -1.01898±0.00000  bestvalidloss -1.15404  last_update 93\n",
      "train: iter 326  trainloss -1.01780  validloss -1.01148±0.00000  bestvalidloss -1.15404  last_update 94\n",
      "train: iter 327  trainloss -1.00490  validloss -0.97882±0.00000  bestvalidloss -1.15404  last_update 95\n",
      "train: iter 328  trainloss -1.01825  validloss -0.99057±0.00000  bestvalidloss -1.15404  last_update 96\n",
      "train: iter 329  trainloss -1.04511  validloss -1.03381±0.00000  bestvalidloss -1.15404  last_update 97\n",
      "train: iter 330  trainloss -1.01871  validloss -0.95837±0.00000  bestvalidloss -1.15404  last_update 98\n",
      "train: iter 331  trainloss -1.04417  validloss -1.02258±0.00000  bestvalidloss -1.15404  last_update 99\n",
      "train: iter 332  trainloss -1.05173  validloss -1.00103±0.00000  bestvalidloss -1.15404  last_update 100\n",
      "train: fin\n"
     ]
    }
   ],
   "source": [
    "train_curve, valid_curve = vi.train_initial_belief(num_iter=100000, lr=1e-3, early_stop_step=default_early)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117822aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.1154, -3.1557, -2.8420, -4.9065], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(vi.initial_belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f505e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90db667c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 0  trainloss 116.12478  validloss 122.45556±0.00000  bestvalidloss 122.45556  last_update 0\n",
      "train: iter 1  trainloss 86.96439  validloss 97.53002±0.00000  bestvalidloss 97.53002  last_update 0\n",
      "train: iter 2  trainloss 64.16767  validloss 69.67127±0.00000  bestvalidloss 69.67127  last_update 0\n",
      "train: iter 3  trainloss 50.19860  validloss 53.10220±0.00000  bestvalidloss 53.10220  last_update 0\n",
      "train: iter 4  trainloss 39.81741  validloss 41.64225±0.00000  bestvalidloss 41.64225  last_update 0\n",
      "train: iter 5  trainloss 31.68016  validloss 32.88612±0.00000  bestvalidloss 32.88612  last_update 0\n",
      "train: iter 6  trainloss 25.24200  validloss 26.10964±0.00000  bestvalidloss 26.10964  last_update 0\n",
      "train: iter 7  trainloss 20.07032  validloss 20.72019±0.00000  bestvalidloss 20.72019  last_update 0\n",
      "train: iter 8  trainloss 15.87216  validloss 16.34154±0.00000  bestvalidloss 16.34154  last_update 0\n",
      "train: iter 9  trainloss 12.51997  validloss 12.88370±0.00000  bestvalidloss 12.88370  last_update 0\n",
      "train: iter 10  trainloss 9.87507  validloss 10.06224±0.00000  bestvalidloss 10.06224  last_update 0\n",
      "train: iter 11  trainloss 7.72458  validloss 7.85531±0.00000  bestvalidloss 7.85531  last_update 0\n",
      "train: iter 12  trainloss 6.05461  validloss 6.24059±0.00000  bestvalidloss 6.24059  last_update 0\n",
      "train: iter 13  trainloss 4.77253  validloss 4.90636±0.00000  bestvalidloss 4.90636  last_update 0\n",
      "train: iter 14  trainloss 3.77302  validloss 3.98483±0.00000  bestvalidloss 3.98483  last_update 0\n",
      "train: iter 15  trainloss 3.04360  validloss 3.25092±0.00000  bestvalidloss 3.25092  last_update 0\n",
      "train: iter 16  trainloss 2.50674  validloss 2.74827±0.00000  bestvalidloss 2.74827  last_update 0\n",
      "train: iter 17  trainloss 2.10961  validloss 2.37662±0.00000  bestvalidloss 2.37662  last_update 0\n",
      "train: iter 18  trainloss 1.83081  validloss 2.17515±0.00000  bestvalidloss 2.17515  last_update 0\n",
      "train: iter 19  trainloss 1.63777  validloss 2.01753±0.00000  bestvalidloss 2.01753  last_update 0\n",
      "train: iter 20  trainloss 1.50174  validloss 1.95470±0.00000  bestvalidloss 1.95470  last_update 0\n",
      "train: iter 21  trainloss 1.39787  validloss 1.85040±0.00000  bestvalidloss 1.85040  last_update 0\n",
      "train: iter 22  trainloss 1.33655  validloss 1.86914±0.00000  bestvalidloss 1.85040  last_update 1\n",
      "train: iter 23  trainloss 1.30237  validloss 1.79392±0.00000  bestvalidloss 1.79392  last_update 0\n",
      "train: iter 24  trainloss 1.28270  validloss 1.87100±0.00000  bestvalidloss 1.79392  last_update 1\n",
      "train: iter 25  trainloss 1.25915  validloss 1.81946±0.00000  bestvalidloss 1.79392  last_update 2\n",
      "train: iter 26  trainloss 1.25874  validloss 1.82293±0.00000  bestvalidloss 1.79392  last_update 3\n",
      "train: iter 27  trainloss 1.25287  validloss 1.85975±0.00000  bestvalidloss 1.79392  last_update 4\n",
      "train: iter 28  trainloss 1.24348  validloss 1.84598±0.00000  bestvalidloss 1.79392  last_update 5\n",
      "train: iter 29  trainloss 1.24757  validloss 1.86266±0.00000  bestvalidloss 1.79392  last_update 6\n",
      "train: iter 30  trainloss 1.24229  validloss 1.87732±0.00000  bestvalidloss 1.79392  last_update 7\n",
      "train: iter 31  trainloss 1.24884  validloss 1.85394±0.00000  bestvalidloss 1.79392  last_update 8\n",
      "train: iter 32  trainloss 1.25696  validloss 1.85269±0.00000  bestvalidloss 1.79392  last_update 9\n",
      "train: iter 33  trainloss 1.25385  validloss 1.87144±0.00000  bestvalidloss 1.79392  last_update 10\n",
      "train: iter 34  trainloss 1.23834  validloss 1.84732±0.00000  bestvalidloss 1.79392  last_update 11\n",
      "train: iter 35  trainloss 1.23901  validloss 1.91987±0.00000  bestvalidloss 1.79392  last_update 12\n",
      "train: iter 36  trainloss 1.23987  validloss 1.92514±0.00000  bestvalidloss 1.79392  last_update 13\n",
      "train: iter 37  trainloss 1.23045  validloss 1.91569±0.00000  bestvalidloss 1.79392  last_update 14\n",
      "train: iter 38  trainloss 1.21694  validloss 1.86987±0.00000  bestvalidloss 1.79392  last_update 15\n",
      "train: iter 39  trainloss 1.27264  validloss 1.82047±0.00000  bestvalidloss 1.79392  last_update 16\n",
      "train: iter 40  trainloss 1.21326  validloss 1.83833±0.00000  bestvalidloss 1.79392  last_update 17\n",
      "train: iter 41  trainloss 1.20403  validloss 1.84518±0.00000  bestvalidloss 1.79392  last_update 18\n",
      "train: iter 42  trainloss 1.17080  validloss 1.83795±0.00000  bestvalidloss 1.79392  last_update 19\n",
      "train: iter 43  trainloss 1.15066  validloss 1.79727±0.00000  bestvalidloss 1.79392  last_update 20\n",
      "train: iter 44  trainloss 1.12607  validloss 1.74895±0.00000  bestvalidloss 1.74895  last_update 0\n",
      "train: iter 45  trainloss 1.10997  validloss 1.71796±0.00000  bestvalidloss 1.71796  last_update 0\n",
      "train: iter 46  trainloss 1.10157  validloss 1.73402±0.00000  bestvalidloss 1.71796  last_update 1\n",
      "train: iter 47  trainloss 1.07779  validloss 1.75358±0.00000  bestvalidloss 1.71796  last_update 2\n",
      "train: iter 48  trainloss 1.07889  validloss 1.69878±0.00000  bestvalidloss 1.69878  last_update 0\n",
      "train: iter 49  trainloss 1.06594  validloss 1.73451±0.00000  bestvalidloss 1.69878  last_update 1\n",
      "train: iter 50  trainloss 1.05062  validloss 1.74948±0.00000  bestvalidloss 1.69878  last_update 2\n",
      "train: iter 51  trainloss 1.04921  validloss 1.74336±0.00000  bestvalidloss 1.69878  last_update 3\n",
      "train: iter 52  trainloss 1.03913  validloss 1.69851±0.00000  bestvalidloss 1.69851  last_update 0\n",
      "train: iter 53  trainloss 1.02824  validloss 1.71145±0.00000  bestvalidloss 1.69851  last_update 1\n",
      "train: iter 54  trainloss 1.02953  validloss 1.71585±0.00000  bestvalidloss 1.69851  last_update 2\n",
      "train: iter 55  trainloss 0.99375  validloss 1.68684±0.00000  bestvalidloss 1.68684  last_update 0\n",
      "train: iter 56  trainloss 0.92554  validloss 1.67175±0.00000  bestvalidloss 1.67175  last_update 0\n",
      "train: iter 57  trainloss 0.90949  validloss 1.64320±0.00000  bestvalidloss 1.64320  last_update 0\n",
      "train: iter 58  trainloss 0.88999  validloss 1.54378±0.00000  bestvalidloss 1.54378  last_update 0\n",
      "train: iter 59  trainloss 0.86978  validloss 1.56040±0.00000  bestvalidloss 1.54378  last_update 1\n",
      "train: iter 60  trainloss 0.85283  validloss 1.50728±0.00000  bestvalidloss 1.50728  last_update 0\n",
      "train: iter 61  trainloss 0.85453  validloss 1.51663±0.00000  bestvalidloss 1.50728  last_update 1\n",
      "train: iter 62  trainloss 0.82772  validloss 1.45890±0.00000  bestvalidloss 1.45890  last_update 0\n",
      "train: iter 63  trainloss 0.81822  validloss 1.55185±0.00000  bestvalidloss 1.45890  last_update 1\n",
      "train: iter 64  trainloss 0.83121  validloss 1.45670±0.00000  bestvalidloss 1.45670  last_update 0\n",
      "train: iter 65  trainloss 0.80564  validloss 1.43843±0.00000  bestvalidloss 1.43843  last_update 0\n",
      "train: iter 66  trainloss 0.80887  validloss 1.44437±0.00000  bestvalidloss 1.43843  last_update 1\n",
      "train: iter 67  trainloss 0.80706  validloss 1.44135±0.00000  bestvalidloss 1.43843  last_update 2\n",
      "train: iter 68  trainloss 0.81001  validloss 1.44510±0.00000  bestvalidloss 1.43843  last_update 3\n",
      "train: iter 69  trainloss 0.78882  validloss 1.41472±0.00000  bestvalidloss 1.41472  last_update 0\n",
      "train: iter 70  trainloss 0.79341  validloss 1.40642±0.00000  bestvalidloss 1.40642  last_update 0\n",
      "train: iter 71  trainloss 0.77353  validloss 1.44495±0.00000  bestvalidloss 1.40642  last_update 1\n",
      "train: iter 72  trainloss 0.78557  validloss 1.34579±0.00000  bestvalidloss 1.34579  last_update 0\n",
      "train: iter 73  trainloss 0.77590  validloss 1.39553±0.00000  bestvalidloss 1.34579  last_update 1\n",
      "train: iter 74  trainloss 0.76042  validloss 1.43204±0.00000  bestvalidloss 1.34579  last_update 2\n",
      "train: iter 75  trainloss 0.76705  validloss 1.33326±0.00000  bestvalidloss 1.33326  last_update 0\n",
      "train: iter 76  trainloss 0.77194  validloss 1.39515±0.00000  bestvalidloss 1.33326  last_update 1\n",
      "train: iter 77  trainloss 0.75041  validloss 1.34515±0.00000  bestvalidloss 1.33326  last_update 2\n",
      "train: iter 78  trainloss 0.75535  validloss 1.37835±0.00000  bestvalidloss 1.33326  last_update 3\n",
      "train: iter 79  trainloss 0.74210  validloss 1.38537±0.00000  bestvalidloss 1.33326  last_update 4\n",
      "train: iter 80  trainloss 0.75317  validloss 1.29577±0.00000  bestvalidloss 1.29577  last_update 0\n",
      "train: iter 81  trainloss 0.74432  validloss 1.32054±0.00000  bestvalidloss 1.29577  last_update 1\n",
      "train: iter 82  trainloss 0.73520  validloss 1.36618±0.00000  bestvalidloss 1.29577  last_update 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 83  trainloss 0.73969  validloss 1.36192±0.00000  bestvalidloss 1.29577  last_update 3\n",
      "train: iter 84  trainloss 0.75720  validloss 1.26039±0.00000  bestvalidloss 1.26039  last_update 0\n",
      "train: iter 85  trainloss 0.73268  validloss 1.30362±0.00000  bestvalidloss 1.26039  last_update 1\n",
      "train: iter 86  trainloss 0.73785  validloss 1.33378±0.00000  bestvalidloss 1.26039  last_update 2\n",
      "train: iter 87  trainloss 0.73520  validloss 1.33302±0.00000  bestvalidloss 1.26039  last_update 3\n",
      "train: iter 88  trainloss 0.73945  validloss 1.31529±0.00000  bestvalidloss 1.26039  last_update 4\n",
      "train: iter 89  trainloss 0.72209  validloss 1.26006±0.00000  bestvalidloss 1.26006  last_update 0\n",
      "train: iter 90  trainloss 0.77104  validloss 1.33917±0.00000  bestvalidloss 1.26006  last_update 1\n",
      "train: iter 91  trainloss 0.73382  validloss 1.24629±0.00000  bestvalidloss 1.24629  last_update 0\n",
      "train: iter 92  trainloss 0.71939  validloss 1.30589±0.00000  bestvalidloss 1.24629  last_update 1\n",
      "train: iter 93  trainloss 0.72519  validloss 1.32768±0.00000  bestvalidloss 1.24629  last_update 2\n",
      "train: iter 94  trainloss 0.74409  validloss 1.35040±0.00000  bestvalidloss 1.24629  last_update 3\n",
      "train: iter 95  trainloss 0.71756  validloss 1.32089±0.00000  bestvalidloss 1.24629  last_update 4\n",
      "train: iter 96  trainloss 0.72585  validloss 1.34246±0.00000  bestvalidloss 1.24629  last_update 5\n",
      "train: iter 97  trainloss 0.72673  validloss 1.27152±0.00000  bestvalidloss 1.24629  last_update 6\n",
      "train: iter 98  trainloss 0.71416  validloss 1.28618±0.00000  bestvalidloss 1.24629  last_update 7\n",
      "train: iter 99  trainloss 0.73360  validloss 1.30603±0.00000  bestvalidloss 1.24629  last_update 8\n",
      "train: iter 100  trainloss 0.71901  validloss 1.27856±0.00000  bestvalidloss 1.24629  last_update 9\n",
      "train: iter 101  trainloss 0.73450  validloss 1.26083±0.00000  bestvalidloss 1.24629  last_update 10\n",
      "train: iter 102  trainloss 0.73178  validloss 1.25798±0.00000  bestvalidloss 1.24629  last_update 11\n",
      "train: iter 103  trainloss 0.72065  validloss 1.28485±0.00000  bestvalidloss 1.24629  last_update 12\n",
      "train: iter 104  trainloss 0.71922  validloss 1.32090±0.00000  bestvalidloss 1.24629  last_update 13\n",
      "train: iter 105  trainloss 0.73416  validloss 1.29948±0.00000  bestvalidloss 1.24629  last_update 14\n",
      "train: iter 106  trainloss 0.74447  validloss 1.31969±0.00000  bestvalidloss 1.24629  last_update 15\n",
      "train: iter 107  trainloss 0.73135  validloss 1.26460±0.00000  bestvalidloss 1.24629  last_update 16\n",
      "train: iter 108  trainloss 0.71065  validloss 1.29777±0.00000  bestvalidloss 1.24629  last_update 17\n",
      "train: iter 109  trainloss 0.72850  validloss 1.26743±0.00000  bestvalidloss 1.24629  last_update 18\n",
      "train: iter 110  trainloss 0.72112  validloss 1.23108±0.00000  bestvalidloss 1.23108  last_update 0\n",
      "train: iter 111  trainloss 0.71866  validloss 1.24636±0.00000  bestvalidloss 1.23108  last_update 1\n",
      "train: iter 112  trainloss 0.72730  validloss 1.28484±0.00000  bestvalidloss 1.23108  last_update 2\n",
      "train: iter 113  trainloss 0.72331  validloss 1.27863±0.00000  bestvalidloss 1.23108  last_update 3\n",
      "train: iter 114  trainloss 0.71712  validloss 1.30292±0.00000  bestvalidloss 1.23108  last_update 4\n",
      "train: iter 115  trainloss 0.74171  validloss 1.26259±0.00000  bestvalidloss 1.23108  last_update 5\n",
      "train: iter 116  trainloss 0.73597  validloss 1.27435±0.00000  bestvalidloss 1.23108  last_update 6\n",
      "train: iter 117  trainloss 0.70705  validloss 1.29943±0.00000  bestvalidloss 1.23108  last_update 7\n",
      "train: iter 118  trainloss 0.73041  validloss 1.37452±0.00000  bestvalidloss 1.23108  last_update 8\n",
      "train: iter 119  trainloss 0.71955  validloss 1.26310±0.00000  bestvalidloss 1.23108  last_update 9\n",
      "train: iter 120  trainloss 0.71775  validloss 1.20866±0.00000  bestvalidloss 1.20866  last_update 0\n",
      "train: iter 121  trainloss 0.72647  validloss 1.26831±0.00000  bestvalidloss 1.20866  last_update 1\n",
      "train: iter 122  trainloss 0.72677  validloss 1.27372±0.00000  bestvalidloss 1.20866  last_update 2\n",
      "train: iter 123  trainloss 0.72260  validloss 1.26585±0.00000  bestvalidloss 1.20866  last_update 3\n",
      "train: iter 124  trainloss 0.72012  validloss 1.31811±0.00000  bestvalidloss 1.20866  last_update 4\n",
      "train: iter 125  trainloss 0.71875  validloss 1.32203±0.00000  bestvalidloss 1.20866  last_update 5\n",
      "train: iter 126  trainloss 0.72803  validloss 1.32521±0.00000  bestvalidloss 1.20866  last_update 6\n",
      "train: iter 127  trainloss 0.71701  validloss 1.30758±0.00000  bestvalidloss 1.20866  last_update 7\n",
      "train: iter 128  trainloss 0.72658  validloss 1.24729±0.00000  bestvalidloss 1.20866  last_update 8\n",
      "train: iter 129  trainloss 0.73314  validloss 1.25795±0.00000  bestvalidloss 1.20866  last_update 9\n",
      "train: iter 130  trainloss 0.72329  validloss 1.27288±0.00000  bestvalidloss 1.20866  last_update 10\n",
      "train: iter 131  trainloss 0.73645  validloss 1.28117±0.00000  bestvalidloss 1.20866  last_update 11\n",
      "train: iter 132  trainloss 0.74462  validloss 1.32145±0.00000  bestvalidloss 1.20866  last_update 12\n",
      "train: iter 133  trainloss 0.71654  validloss 1.32303±0.00000  bestvalidloss 1.20866  last_update 13\n",
      "train: iter 134  trainloss 0.71795  validloss 1.31158±0.00000  bestvalidloss 1.20866  last_update 14\n",
      "train: iter 135  trainloss 0.71369  validloss 1.32354±0.00000  bestvalidloss 1.20866  last_update 15\n",
      "train: iter 136  trainloss 0.72875  validloss 1.26426±0.00000  bestvalidloss 1.20866  last_update 16\n",
      "train: iter 137  trainloss 0.72413  validloss 1.38167±0.00000  bestvalidloss 1.20866  last_update 17\n",
      "train: iter 138  trainloss 0.71215  validloss 1.25624±0.00000  bestvalidloss 1.20866  last_update 18\n",
      "train: iter 139  trainloss 0.71600  validloss 1.22568±0.00000  bestvalidloss 1.20866  last_update 19\n",
      "train: iter 140  trainloss 0.72310  validloss 1.27156±0.00000  bestvalidloss 1.20866  last_update 20\n",
      "train: iter 141  trainloss 0.71900  validloss 1.26249±0.00000  bestvalidloss 1.20866  last_update 21\n",
      "train: iter 142  trainloss 0.72296  validloss 1.33273±0.00000  bestvalidloss 1.20866  last_update 22\n",
      "train: iter 143  trainloss 0.70764  validloss 1.29466±0.00000  bestvalidloss 1.20866  last_update 23\n",
      "train: iter 144  trainloss 0.72203  validloss 1.24355±0.00000  bestvalidloss 1.20866  last_update 24\n",
      "train: iter 145  trainloss 0.71415  validloss 1.24126±0.00000  bestvalidloss 1.20866  last_update 25\n",
      "train: iter 146  trainloss 0.71563  validloss 1.31839±0.00000  bestvalidloss 1.20866  last_update 26\n",
      "train: iter 147  trainloss 0.72515  validloss 1.34240±0.00000  bestvalidloss 1.20866  last_update 27\n",
      "train: iter 148  trainloss 0.71836  validloss 1.28537±0.00000  bestvalidloss 1.20866  last_update 28\n",
      "train: iter 149  trainloss 0.72261  validloss 1.24867±0.00000  bestvalidloss 1.20866  last_update 29\n",
      "train: iter 150  trainloss 0.72165  validloss 1.35439±0.00000  bestvalidloss 1.20866  last_update 30\n",
      "train: iter 151  trainloss 0.72720  validloss 1.24904±0.00000  bestvalidloss 1.20866  last_update 31\n",
      "train: iter 152  trainloss 0.71677  validloss 1.34152±0.00000  bestvalidloss 1.20866  last_update 32\n",
      "train: iter 153  trainloss 0.72097  validloss 1.26605±0.00000  bestvalidloss 1.20866  last_update 33\n",
      "train: iter 154  trainloss 0.71026  validloss 1.34455±0.00000  bestvalidloss 1.20866  last_update 34\n",
      "train: iter 155  trainloss 0.71770  validloss 1.25115±0.00000  bestvalidloss 1.20866  last_update 35\n",
      "train: iter 156  trainloss 0.71387  validloss 1.27860±0.00000  bestvalidloss 1.20866  last_update 36\n",
      "train: iter 157  trainloss 0.72042  validloss 1.29491±0.00000  bestvalidloss 1.20866  last_update 37\n",
      "train: iter 158  trainloss 0.71117  validloss 1.25727±0.00000  bestvalidloss 1.20866  last_update 38\n",
      "train: iter 159  trainloss 0.71866  validloss 1.30663±0.00000  bestvalidloss 1.20866  last_update 39\n",
      "train: iter 160  trainloss 0.72139  validloss 1.21485±0.00000  bestvalidloss 1.20866  last_update 40\n",
      "train: iter 161  trainloss 0.72414  validloss 1.22240±0.00000  bestvalidloss 1.20866  last_update 41\n",
      "train: iter 162  trainloss 0.72744  validloss 1.28039±0.00000  bestvalidloss 1.20866  last_update 42\n",
      "train: iter 163  trainloss 0.72069  validloss 1.25039±0.00000  bestvalidloss 1.20866  last_update 43\n",
      "train: iter 164  trainloss 0.71816  validloss 1.32818±0.00000  bestvalidloss 1.20866  last_update 44\n",
      "train: iter 165  trainloss 0.71604  validloss 1.29572±0.00000  bestvalidloss 1.20866  last_update 45\n",
      "train: iter 166  trainloss 0.71612  validloss 1.26807±0.00000  bestvalidloss 1.20866  last_update 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 167  trainloss 0.71459  validloss 1.27520±0.00000  bestvalidloss 1.20866  last_update 47\n",
      "train: iter 168  trainloss 0.72325  validloss 1.30591±0.00000  bestvalidloss 1.20866  last_update 48\n",
      "train: iter 169  trainloss 0.71296  validloss 1.31200±0.00000  bestvalidloss 1.20866  last_update 49\n",
      "train: iter 170  trainloss 0.71765  validloss 1.29950±0.00000  bestvalidloss 1.20866  last_update 50\n",
      "train: iter 171  trainloss 0.72592  validloss 1.27606±0.00000  bestvalidloss 1.20866  last_update 51\n",
      "train: iter 172  trainloss 0.71011  validloss 1.24297±0.00000  bestvalidloss 1.20866  last_update 52\n",
      "train: iter 173  trainloss 0.71792  validloss 1.26156±0.00000  bestvalidloss 1.20866  last_update 53\n",
      "train: iter 174  trainloss 0.70084  validloss 1.27145±0.00000  bestvalidloss 1.20866  last_update 54\n",
      "train: iter 175  trainloss 0.73246  validloss 1.23811±0.00000  bestvalidloss 1.20866  last_update 55\n",
      "train: iter 176  trainloss 0.71930  validloss 1.28607±0.00000  bestvalidloss 1.20866  last_update 56\n",
      "train: iter 177  trainloss 0.72395  validloss 1.31093±0.00000  bestvalidloss 1.20866  last_update 57\n",
      "train: iter 178  trainloss 0.73104  validloss 1.30223±0.00000  bestvalidloss 1.20866  last_update 58\n",
      "train: iter 179  trainloss 0.70688  validloss 1.24171±0.00000  bestvalidloss 1.20866  last_update 59\n",
      "train: iter 180  trainloss 0.71684  validloss 1.26739±0.00000  bestvalidloss 1.20866  last_update 60\n",
      "train: iter 181  trainloss 0.72343  validloss 1.30285±0.00000  bestvalidloss 1.20866  last_update 61\n",
      "train: iter 182  trainloss 0.71395  validloss 1.28598±0.00000  bestvalidloss 1.20866  last_update 62\n",
      "train: iter 183  trainloss 0.72386  validloss 1.27076±0.00000  bestvalidloss 1.20866  last_update 63\n",
      "train: iter 184  trainloss 0.72881  validloss 1.25367±0.00000  bestvalidloss 1.20866  last_update 64\n",
      "train: iter 185  trainloss 0.71975  validloss 1.33358±0.00000  bestvalidloss 1.20866  last_update 65\n",
      "train: iter 186  trainloss 0.72543  validloss 1.24104±0.00000  bestvalidloss 1.20866  last_update 66\n",
      "train: iter 187  trainloss 0.70763  validloss 1.29176±0.00000  bestvalidloss 1.20866  last_update 67\n",
      "train: iter 188  trainloss 0.70480  validloss 1.29519±0.00000  bestvalidloss 1.20866  last_update 68\n",
      "train: iter 189  trainloss 0.72207  validloss 1.26216±0.00000  bestvalidloss 1.20866  last_update 69\n",
      "train: iter 190  trainloss 0.72628  validloss 1.37476±0.00000  bestvalidloss 1.20866  last_update 70\n",
      "train: iter 191  trainloss 0.71736  validloss 1.38708±0.00000  bestvalidloss 1.20866  last_update 71\n",
      "train: iter 192  trainloss 0.70841  validloss 1.29316±0.00000  bestvalidloss 1.20866  last_update 72\n",
      "train: iter 193  trainloss 0.71525  validloss 1.29916±0.00000  bestvalidloss 1.20866  last_update 73\n",
      "train: iter 194  trainloss 0.71768  validloss 1.28021±0.00000  bestvalidloss 1.20866  last_update 74\n",
      "train: iter 195  trainloss 0.71244  validloss 1.26070±0.00000  bestvalidloss 1.20866  last_update 75\n",
      "train: iter 196  trainloss 0.71284  validloss 1.25778±0.00000  bestvalidloss 1.20866  last_update 76\n",
      "train: iter 197  trainloss 0.70826  validloss 1.27561±0.00000  bestvalidloss 1.20866  last_update 77\n",
      "train: iter 198  trainloss 0.72120  validloss 1.28092±0.00000  bestvalidloss 1.20866  last_update 78\n",
      "train: iter 199  trainloss 0.71447  validloss 1.25042±0.00000  bestvalidloss 1.20866  last_update 79\n",
      "train: iter 200  trainloss 0.70188  validloss 1.27442±0.00000  bestvalidloss 1.20866  last_update 80\n",
      "train: iter 201  trainloss 0.70716  validloss 1.22936±0.00000  bestvalidloss 1.20866  last_update 81\n",
      "train: iter 202  trainloss 0.71079  validloss 1.25952±0.00000  bestvalidloss 1.20866  last_update 82\n",
      "train: iter 203  trainloss 0.70182  validloss 1.28532±0.00000  bestvalidloss 1.20866  last_update 83\n",
      "train: iter 204  trainloss 0.72357  validloss 1.23724±0.00000  bestvalidloss 1.20866  last_update 84\n",
      "train: iter 205  trainloss 0.71581  validloss 1.29775±0.00000  bestvalidloss 1.20866  last_update 85\n",
      "train: iter 206  trainloss 0.70680  validloss 1.27752±0.00000  bestvalidloss 1.20866  last_update 86\n",
      "train: iter 207  trainloss 0.70633  validloss 1.27085±0.00000  bestvalidloss 1.20866  last_update 87\n",
      "train: iter 208  trainloss 0.71258  validloss 1.34062±0.00000  bestvalidloss 1.20866  last_update 88\n",
      "train: iter 209  trainloss 0.71528  validloss 1.22792±0.00000  bestvalidloss 1.20866  last_update 89\n",
      "train: iter 210  trainloss 0.71265  validloss 1.30074±0.00000  bestvalidloss 1.20866  last_update 90\n",
      "train: iter 211  trainloss 0.71282  validloss 1.31555±0.00000  bestvalidloss 1.20866  last_update 91\n",
      "train: iter 212  trainloss 0.71226  validloss 1.25262±0.00000  bestvalidloss 1.20866  last_update 92\n",
      "train: iter 213  trainloss 0.70063  validloss 1.23497±0.00000  bestvalidloss 1.20866  last_update 93\n",
      "train: iter 214  trainloss 0.72834  validloss 1.24727±0.00000  bestvalidloss 1.20866  last_update 94\n",
      "train: iter 215  trainloss 0.71004  validloss 1.28678±0.00000  bestvalidloss 1.20866  last_update 95\n",
      "train: iter 216  trainloss 0.70578  validloss 1.34590±0.00000  bestvalidloss 1.20866  last_update 96\n",
      "train: iter 217  trainloss 0.70942  validloss 1.24854±0.00000  bestvalidloss 1.20866  last_update 97\n",
      "train: iter 218  trainloss 0.71534  validloss 1.32584±0.00000  bestvalidloss 1.20866  last_update 98\n",
      "train: iter 219  trainloss 0.70552  validloss 1.24661±0.00000  bestvalidloss 1.20866  last_update 99\n",
      "train: iter 220  trainloss 0.72738  validloss 1.23914±0.00000  bestvalidloss 1.20866  last_update 100\n",
      "train: fin\n"
     ]
    }
   ],
   "source": [
    "train_curve, valid_curve = vi.train_penalty(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ae20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78af8ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base save ckpt ckpt_basevi_cartpole_unweighted\n",
      "base load self.initial_belief.data.sum() tensor(-12.0195)\n",
      "base load dec.state_dict()['net_phat.0.weight'].sum() tensor(1.7195)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vi.save(ckpt_key=\"unweighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672208e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c8f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07698666314332152\n",
      "tensor([1.3300])\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility check\n",
    "print(np.random.randn())\n",
    "print(torch.randn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ef7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732366f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24744d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce713d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727f804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c69747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85609792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e850eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d322b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd8a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b9abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde31bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bbee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee0bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29ec70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
