{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f49dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "import datetime\n",
    "import torch\n",
    "import gym\n",
    "import custom_gym\n",
    "import random\n",
    "from config import cfg_seed, cfg_env, cfg_z_dim, cfg_default_lr, cfg_default_early\n",
    "seed = cfg_seed\n",
    "env_str=cfg_env\n",
    "default_lr=cfg_default_lr\n",
    "default_early=cfg_default_early\n",
    "\n",
    "if cfg_env == \"pendulum\":\n",
    "    env_name = \"CustomPendulum-v0\"\n",
    "if cfg_env == \"cartpole\":\n",
    "    env_name = \"CustomCartPole-v0\"\n",
    "\n",
    "figfilenamehead = \"fig_policy_evaluation_\"+cfg_env+\"_\"\n",
    "\n",
    "num_iter_max=200000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf77b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "s_dim = env.reset().flatten().shape[0]\n",
    "a_dim = env.action_space.sample().flatten().shape[0]\n",
    "z_dim = cfg_z_dim\n",
    "print(s_dim, a_dim, z_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7c5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sac import SAC\n",
    "# agent = SAC(env.observation_space.shape[0], env.action_space)\n",
    "# agent.load_checkpoint(ckpt_path=\"checkpoints/sac_checkpoint_custom_\"+env_str+\"_mdp_\", evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9c09c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proxima-hishinuma/anaconda3/envs/iwvi/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# データ生成\n",
    "from get_offline_data import getOfflineData\n",
    "getOfflineData(env_name=env_name, episode_num=100, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d57437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offline_data[1].sum() tensor(7130.6104)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "offline_data = pickle.load(open(\"offline_data_\"+env_str+\".pkl\",\"rb\"))\n",
    "\n",
    "debug_info = pickle.load(open(\"offline_data_debug_info_\"+env_str+\".pkl\",\"rb\"))\n",
    "debug_info = np.array(debug_info)\n",
    "print(\"offline_data[1].sum()\", offline_data[1].sum())\n",
    "print(debug_info.shape)\n",
    "# c_array = debug_info[:,1]\n",
    "\n",
    "# episode_index = 9\n",
    "# plt.plot(offline_data[episode_index][:,0],offline_data[episode_index][:,1])\n",
    "# print(\"env_param: \",debug_info[episode_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767b8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec: h_dim 48\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(vi_base)\n",
    "import vi_base\n",
    "\n",
    "args_init_dict = {\n",
    "    \"offline_data\": offline_data,\n",
    "    \"s_dim\": s_dim,\n",
    "    \"a_dim\": a_dim,\n",
    "    \"z_dim\": z_dim,\n",
    "    #              \"policy\":agent.select_action,\n",
    "    \"mdp_policy\":None,\n",
    "    \"bamdp_policy\":None,\n",
    "    \"debug_info\":debug_info,\n",
    "    \"env\" : env,\n",
    "    \"ckpt_suffix\" : env_str,\n",
    "    }\n",
    "\n",
    "vi = vi_base.baseVI(args_init_dict)\n",
    "# print(np.random.randn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c06f958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vae: enc_dec\n",
      "train: iter 0  trainloss 672.45059  validloss 1050.09350±0.00000  bestvalidloss 1050.09350  last_update 0\n",
      "train: iter 1  trainloss 147.35522  validloss 474.31043±0.00000  bestvalidloss 474.31043  last_update 0\n",
      "train: iter 2  trainloss -188.89792  validloss 61.32985±0.00000  bestvalidloss 61.32985  last_update 0\n",
      "train: iter 3  trainloss -447.25883  validloss -159.40684±0.00000  bestvalidloss -159.40684  last_update 0\n",
      "train: iter 4  trainloss -618.24016  validloss -320.55755±0.00000  bestvalidloss -320.55755  last_update 0\n",
      "train: iter 5  trainloss -777.81647  validloss -501.28083±0.00000  bestvalidloss -501.28083  last_update 0\n",
      "train: iter 6  trainloss -875.18149  validloss -542.31085±0.00000  bestvalidloss -542.31085  last_update 0\n",
      "train: iter 7  trainloss -951.15372  validloss -765.06280±0.00000  bestvalidloss -765.06280  last_update 0\n",
      "train: iter 8  trainloss -1030.11993  validloss -834.99042±0.00000  bestvalidloss -834.99042  last_update 0\n",
      "train: iter 9  trainloss -1130.23986  validloss -923.75566±0.00000  bestvalidloss -923.75566  last_update 0\n",
      "train: iter 10  trainloss -1083.63738  validloss -916.48378±0.00000  bestvalidloss -923.75566  last_update 1\n",
      "train: iter 11  trainloss -1206.23854  validloss -1057.29411±0.00000  bestvalidloss -1057.29411  last_update 0\n",
      "train: iter 12  trainloss -1316.49894  validloss -1061.98816±0.00000  bestvalidloss -1061.98816  last_update 0\n",
      "train: iter 13  trainloss -1272.88121  validloss -1076.43271±0.00000  bestvalidloss -1076.43271  last_update 0\n",
      "train: iter 14  trainloss -1334.44199  validloss -1249.93354±0.00000  bestvalidloss -1249.93354  last_update 0\n",
      "train: iter 15  trainloss -1410.96623  validloss -1303.70483±0.00000  bestvalidloss -1303.70483  last_update 0\n",
      "train: iter 16  trainloss -1484.92115  validloss -1032.41416±0.00000  bestvalidloss -1303.70483  last_update 1\n",
      "train: iter 17  trainloss -1520.19618  validloss -1389.65803±0.00000  bestvalidloss -1389.65803  last_update 0\n",
      "train: iter 18  trainloss -1343.60757  validloss -1269.62757±0.00000  bestvalidloss -1389.65803  last_update 1\n",
      "train: iter 19  trainloss -1538.02943  validloss -1396.61869±0.00000  bestvalidloss -1396.61869  last_update 0\n",
      "train: iter 20  trainloss -1542.84841  validloss -1451.16710±0.00000  bestvalidloss -1451.16710  last_update 0\n",
      "train: iter 21  trainloss -1575.96317  validloss -1490.86387±0.00000  bestvalidloss -1490.86387  last_update 0\n",
      "train: iter 22  trainloss -1614.97467  validloss -1441.66431±0.00000  bestvalidloss -1490.86387  last_update 1\n",
      "train: iter 23  trainloss -1543.95630  validloss -1491.29417±0.00000  bestvalidloss -1491.29417  last_update 0\n",
      "train: iter 24  trainloss -1438.85858  validloss -1270.81426±0.00000  bestvalidloss -1491.29417  last_update 1\n",
      "train: iter 25  trainloss -1579.85740  validloss -1489.91932±0.00000  bestvalidloss -1491.29417  last_update 2\n",
      "train: iter 26  trainloss -1621.32970  validloss -1518.42755±0.00000  bestvalidloss -1518.42755  last_update 0\n",
      "train: iter 27  trainloss -1627.82977  validloss -1415.86346±0.00000  bestvalidloss -1518.42755  last_update 1\n",
      "train: iter 28  trainloss -1626.81771  validloss -1440.37597±0.00000  bestvalidloss -1518.42755  last_update 2\n",
      "train: iter 29  trainloss -1653.99647  validloss -1488.12796±0.00000  bestvalidloss -1518.42755  last_update 3\n",
      "train: iter 30  trainloss -1645.25877  validloss -1580.41026±0.00000  bestvalidloss -1580.41026  last_update 0\n",
      "train: iter 31  trainloss -1662.43947  validloss -1471.79057±0.00000  bestvalidloss -1580.41026  last_update 1\n",
      "train: iter 32  trainloss -1668.52363  validloss -1587.66552±0.00000  bestvalidloss -1587.66552  last_update 0\n",
      "train: iter 33  trainloss -1667.59279  validloss -1607.33260±0.00000  bestvalidloss -1607.33260  last_update 0\n",
      "train: iter 34  trainloss -1676.51077  validloss -1544.78529±0.00000  bestvalidloss -1607.33260  last_update 1\n",
      "train: iter 35  trainloss -1632.03428  validloss -1453.71010±0.00000  bestvalidloss -1607.33260  last_update 2\n",
      "train: iter 36  trainloss -1534.61873  validloss -1525.61489±0.00000  bestvalidloss -1607.33260  last_update 3\n",
      "train: iter 37  trainloss -1559.85939  validloss -1257.72295±0.00000  bestvalidloss -1607.33260  last_update 4\n",
      "train: iter 38  trainloss -1707.88769  validloss -1518.62582±0.00000  bestvalidloss -1607.33260  last_update 5\n",
      "train: iter 39  trainloss -1716.00046  validloss -1657.95737±0.00000  bestvalidloss -1657.95737  last_update 0\n",
      "train: iter 40  trainloss -1713.96054  validloss -1660.24598±0.00000  bestvalidloss -1660.24598  last_update 0\n",
      "train: iter 41  trainloss -1718.03159  validloss -1627.43524±0.00000  bestvalidloss -1660.24598  last_update 1\n",
      "train: iter 42  trainloss -1700.50431  validloss -1538.96476±0.00000  bestvalidloss -1660.24598  last_update 2\n",
      "train: iter 43  trainloss -1733.11477  validloss -1643.84161±0.00000  bestvalidloss -1660.24598  last_update 3\n",
      "train: iter 44  trainloss -1753.43652  validloss -1678.94045±0.00000  bestvalidloss -1678.94045  last_update 0\n",
      "train: iter 45  trainloss -1685.69980  validloss -1663.85298±0.00000  bestvalidloss -1678.94045  last_update 1\n",
      "train: iter 46  trainloss -1751.49462  validloss -1611.80649±0.00000  bestvalidloss -1678.94045  last_update 2\n",
      "train: iter 47  trainloss -1742.03109  validloss -1706.30481±0.00000  bestvalidloss -1706.30481  last_update 0\n",
      "train: iter 48  trainloss -1753.21992  validloss -1663.14791±0.00000  bestvalidloss -1706.30481  last_update 1\n",
      "train: iter 49  trainloss -1773.04288  validloss -1681.10621±0.00000  bestvalidloss -1706.30481  last_update 2\n",
      "train: iter 50  trainloss -1743.17120  validloss -1703.40220±0.00000  bestvalidloss -1706.30481  last_update 3\n",
      "train: iter 51  trainloss -1712.44016  validloss -1656.99014±0.00000  bestvalidloss -1706.30481  last_update 4\n",
      "train: iter 52  trainloss -1787.39370  validloss -1699.45605±0.00000  bestvalidloss -1706.30481  last_update 5\n",
      "train: iter 53  trainloss -1768.64792  validloss -1696.51414±0.00000  bestvalidloss -1706.30481  last_update 6\n",
      "train: iter 54  trainloss -1760.54587  validloss -1612.87510±0.00000  bestvalidloss -1706.30481  last_update 7\n",
      "train: iter 55  trainloss -1777.18494  validloss -1707.17374±0.00000  bestvalidloss -1707.17374  last_update 0\n",
      "train: iter 56  trainloss -1743.03546  validloss -1514.42885±0.00000  bestvalidloss -1707.17374  last_update 1\n",
      "train: iter 57  trainloss -1719.33147  validloss -1624.79901±0.00000  bestvalidloss -1707.17374  last_update 2\n",
      "train: iter 58  trainloss -1725.45979  validloss -1614.42389±0.00000  bestvalidloss -1707.17374  last_update 3\n",
      "train: iter 59  trainloss -1569.25497  validloss -1377.11156±0.00000  bestvalidloss -1707.17374  last_update 4\n",
      "train: iter 60  trainloss -1809.12661  validloss -1635.13587±0.00000  bestvalidloss -1707.17374  last_update 5\n",
      "train: iter 61  trainloss -1795.97567  validloss -1720.76243±0.00000  bestvalidloss -1720.76243  last_update 0\n",
      "train: iter 62  trainloss -1818.97587  validloss -1756.39105±0.00000  bestvalidloss -1756.39105  last_update 0\n",
      "train: iter 63  trainloss -1790.37689  validloss -1713.92390±0.00000  bestvalidloss -1756.39105  last_update 1\n",
      "train: iter 64  trainloss -1709.41965  validloss -1704.08148±0.00000  bestvalidloss -1756.39105  last_update 2\n",
      "train: iter 65  trainloss -1823.30157  validloss -1662.76172±0.00000  bestvalidloss -1756.39105  last_update 3\n",
      "train: iter 66  trainloss -1811.92026  validloss -1748.18528±0.00000  bestvalidloss -1756.39105  last_update 4\n",
      "train: iter 67  trainloss -1831.44127  validloss -1671.22282±0.00000  bestvalidloss -1756.39105  last_update 5\n",
      "train: iter 68  trainloss -1812.87255  validloss -1761.15894±0.00000  bestvalidloss -1761.15894  last_update 0\n",
      "train: iter 69  trainloss -1791.25678  validloss -1774.63551±0.00000  bestvalidloss -1774.63551  last_update 0\n",
      "train: iter 70  trainloss -1558.31826  validloss -1656.82740±0.00000  bestvalidloss -1774.63551  last_update 1\n",
      "train: iter 71  trainloss -1777.02740  validloss -1237.06870±0.00000  bestvalidloss -1774.63551  last_update 2\n",
      "train: iter 72  trainloss -1842.23637  validloss -1778.36173±0.00000  bestvalidloss -1778.36173  last_update 0\n",
      "train: iter 73  trainloss -1795.55781  validloss -1751.78728±0.00000  bestvalidloss -1778.36173  last_update 1\n",
      "train: iter 74  trainloss -1857.75975  validloss -1795.06041±0.00000  bestvalidloss -1795.06041  last_update 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 75  trainloss -1796.38094  validloss -1758.39094±0.00000  bestvalidloss -1795.06041  last_update 1\n",
      "train: iter 76  trainloss -1861.02898  validloss -1815.31055±0.00000  bestvalidloss -1815.31055  last_update 0\n",
      "train: iter 77  trainloss -1799.18858  validloss -1670.61552±0.00000  bestvalidloss -1815.31055  last_update 1\n",
      "train: iter 78  trainloss -1860.64181  validloss -1811.90768±0.00000  bestvalidloss -1815.31055  last_update 2\n",
      "train: iter 79  trainloss -1877.42610  validloss -1809.83046±0.00000  bestvalidloss -1815.31055  last_update 3\n",
      "train: iter 80  trainloss -1851.09617  validloss -1775.36255±0.00000  bestvalidloss -1815.31055  last_update 4\n",
      "train: iter 81  trainloss -1817.29247  validloss -1752.87663±0.00000  bestvalidloss -1815.31055  last_update 5\n",
      "train: iter 82  trainloss -1828.90306  validloss -1591.97860±0.00000  bestvalidloss -1815.31055  last_update 6\n",
      "train: iter 83  trainloss -1879.75497  validloss -1815.78613±0.00000  bestvalidloss -1815.78613  last_update 0\n",
      "train: iter 84  trainloss -1875.40497  validloss -1813.55733±0.00000  bestvalidloss -1815.78613  last_update 1\n",
      "train: iter 85  trainloss -1874.76515  validloss -1715.27698±0.00000  bestvalidloss -1815.78613  last_update 2\n",
      "train: iter 86  trainloss -1682.67021  validloss -1719.57831±0.00000  bestvalidloss -1815.78613  last_update 3\n",
      "train: iter 87  trainloss -1758.30753  validloss -1524.44212±0.00000  bestvalidloss -1815.78613  last_update 4\n",
      "train: iter 88  trainloss -1811.20928  validloss -1798.02314±0.00000  bestvalidloss -1815.78613  last_update 5\n",
      "train: iter 89  trainloss -1874.04924  validloss -1783.24804±0.00000  bestvalidloss -1815.78613  last_update 6\n",
      "train: iter 90  trainloss -1886.41517  validloss -1852.87211±0.00000  bestvalidloss -1852.87211  last_update 0\n",
      "train: iter 91  trainloss -1906.70611  validloss -1841.48289±0.00000  bestvalidloss -1852.87211  last_update 1\n",
      "train: iter 92  trainloss -1906.09715  validloss -1820.72382±0.00000  bestvalidloss -1852.87211  last_update 2\n",
      "train: iter 93  trainloss -1872.89254  validloss -1845.81994±0.00000  bestvalidloss -1852.87211  last_update 3\n",
      "train: iter 94  trainloss -1876.19646  validloss -1841.65318±0.00000  bestvalidloss -1852.87211  last_update 4\n",
      "train: iter 95  trainloss -1904.89686  validloss -1789.06558±0.00000  bestvalidloss -1852.87211  last_update 5\n",
      "train: iter 96  trainloss -1881.75171  validloss -1854.59911±0.00000  bestvalidloss -1854.59911  last_update 0\n",
      "train: iter 97  trainloss -1888.60627  validloss -1863.77796±0.00000  bestvalidloss -1863.77796  last_update 0\n",
      "train: iter 98  trainloss -1929.89798  validloss -1742.96309±0.00000  bestvalidloss -1863.77796  last_update 1\n",
      "train: iter 99  trainloss -1910.03568  validloss -1881.77477±0.00000  bestvalidloss -1881.77477  last_update 0\n",
      "train: iter 100  trainloss -1738.30679  validloss -1739.37299±0.00000  bestvalidloss -1881.77477  last_update 1\n",
      "train: iter 101  trainloss -1819.48293  validloss -1772.49400±0.00000  bestvalidloss -1881.77477  last_update 2\n",
      "train: iter 102  trainloss -1943.99057  validloss -1774.05215±0.00000  bestvalidloss -1881.77477  last_update 3\n",
      "train: iter 103  trainloss -1990.38675  validloss -1900.73134±0.00000  bestvalidloss -1900.73134  last_update 0\n",
      "train: iter 104  trainloss -2009.06563  validloss -1932.74115±0.00000  bestvalidloss -1932.74115  last_update 0\n",
      "train: iter 105  trainloss -2032.40516  validloss -1942.32939±0.00000  bestvalidloss -1942.32939  last_update 0\n",
      "train: iter 106  trainloss -1931.02659  validloss -1965.51782±0.00000  bestvalidloss -1965.51782  last_update 0\n",
      "train: iter 107  trainloss -1966.68908  validloss -1943.20457±0.00000  bestvalidloss -1965.51782  last_update 1\n",
      "train: iter 108  trainloss -2000.29009  validloss -1871.98308±0.00000  bestvalidloss -1965.51782  last_update 2\n",
      "train: iter 109  trainloss -1971.31590  validloss -1722.97839±0.00000  bestvalidloss -1965.51782  last_update 3\n",
      "train: iter 110  trainloss -2043.38476  validloss -1978.68605±0.00000  bestvalidloss -1978.68605  last_update 0\n",
      "train: iter 111  trainloss -2018.68628  validloss -1946.35574±0.00000  bestvalidloss -1978.68605  last_update 1\n",
      "train: iter 112  trainloss -2022.45691  validloss -1903.64384±0.00000  bestvalidloss -1978.68605  last_update 2\n",
      "train: iter 113  trainloss -2052.72991  validloss -1976.63229±0.00000  bestvalidloss -1978.68605  last_update 3\n",
      "train: iter 114  trainloss -2011.09989  validloss -1950.47782±0.00000  bestvalidloss -1978.68605  last_update 4\n",
      "train: iter 115  trainloss -2045.98075  validloss -1961.43610±0.00000  bestvalidloss -1978.68605  last_update 5\n",
      "train: iter 116  trainloss -2041.92971  validloss -1999.24424±0.00000  bestvalidloss -1999.24424  last_update 0\n",
      "train: iter 117  trainloss -1993.13703  validloss -1959.86068±0.00000  bestvalidloss -1999.24424  last_update 1\n",
      "train: iter 118  trainloss -1941.11669  validloss -1995.34139±0.00000  bestvalidloss -1999.24424  last_update 2\n",
      "train: iter 119  trainloss -2065.60429  validloss -1981.05978±0.00000  bestvalidloss -1999.24424  last_update 3\n",
      "train: iter 120  trainloss -2046.22400  validloss -1998.40849±0.00000  bestvalidloss -1999.24424  last_update 4\n",
      "train: iter 121  trainloss -2056.71283  validloss -1863.97744±0.00000  bestvalidloss -1999.24424  last_update 5\n",
      "train: iter 122  trainloss -1963.52675  validloss -1957.25382±0.00000  bestvalidloss -1999.24424  last_update 6\n",
      "train: iter 123  trainloss -2060.51300  validloss -2000.89904±0.00000  bestvalidloss -2000.89904  last_update 0\n",
      "train: iter 124  trainloss -2080.69311  validloss -2002.34423±0.00000  bestvalidloss -2002.34423  last_update 0\n",
      "train: iter 125  trainloss -2079.73876  validloss -2018.25284±0.00000  bestvalidloss -2018.25284  last_update 0\n",
      "train: iter 126  trainloss -2060.32418  validloss -1966.45213±0.00000  bestvalidloss -2018.25284  last_update 1\n",
      "train: iter 127  trainloss -2101.50398  validloss -2004.11074±0.00000  bestvalidloss -2018.25284  last_update 2\n",
      "train: iter 128  trainloss -2058.13233  validloss -2025.68276±0.00000  bestvalidloss -2025.68276  last_update 0\n",
      "train: iter 129  trainloss -2081.37897  validloss -1921.89257±0.00000  bestvalidloss -2025.68276  last_update 1\n",
      "train: iter 130  trainloss -2041.35241  validloss -1994.59141±0.00000  bestvalidloss -2025.68276  last_update 2\n",
      "train: iter 131  trainloss -2097.79012  validloss -2031.07440±0.00000  bestvalidloss -2031.07440  last_update 0\n",
      "train: iter 132  trainloss -2039.56935  validloss -2028.75656±0.00000  bestvalidloss -2031.07440  last_update 1\n",
      "train: iter 133  trainloss -2060.49661  validloss -1891.88972±0.00000  bestvalidloss -2031.07440  last_update 2\n",
      "train: iter 134  trainloss -2060.27856  validloss -2020.11067±0.00000  bestvalidloss -2031.07440  last_update 3\n",
      "train: iter 135  trainloss -2104.87081  validloss -2044.81346±0.00000  bestvalidloss -2044.81346  last_update 0\n",
      "train: iter 136  trainloss -2063.18026  validloss -2062.70497±0.00000  bestvalidloss -2062.70497  last_update 0\n",
      "train: iter 137  trainloss -2048.20155  validloss -1981.92597±0.00000  bestvalidloss -2062.70497  last_update 1\n",
      "train: iter 138  trainloss -2101.38193  validloss -2039.59623±0.00000  bestvalidloss -2062.70497  last_update 2\n",
      "train: iter 139  trainloss -1984.55251  validloss -2046.72644±0.00000  bestvalidloss -2062.70497  last_update 3\n",
      "train: iter 140  trainloss -1903.03969  validloss -1922.35159±0.00000  bestvalidloss -2062.70497  last_update 4\n",
      "train: iter 141  trainloss -1963.61392  validloss -2007.72673±0.00000  bestvalidloss -2062.70497  last_update 5\n",
      "train: iter 142  trainloss -2047.55049  validloss -2008.29553±0.00000  bestvalidloss -2062.70497  last_update 6\n",
      "train: iter 143  trainloss -2090.98833  validloss -2007.41536±0.00000  bestvalidloss -2062.70497  last_update 7\n",
      "train: iter 144  trainloss -2103.37275  validloss -2054.06335±0.00000  bestvalidloss -2062.70497  last_update 8\n",
      "train: iter 145  trainloss -2108.53774  validloss -2054.48604±0.00000  bestvalidloss -2062.70497  last_update 9\n",
      "train: iter 146  trainloss -2105.08232  validloss -2032.64655±0.00000  bestvalidloss -2062.70497  last_update 10\n",
      "train: iter 147  trainloss -2119.32834  validloss -2074.52473±0.00000  bestvalidloss -2074.52473  last_update 0\n",
      "train: iter 148  trainloss -2107.64271  validloss -2068.98873±0.00000  bestvalidloss -2074.52473  last_update 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 149  trainloss -2122.93578  validloss -2072.99366±0.00000  bestvalidloss -2074.52473  last_update 2\n",
      "train: iter 150  trainloss -2083.94631  validloss -2036.76771±0.00000  bestvalidloss -2074.52473  last_update 3\n",
      "train: iter 151  trainloss -1955.72265  validloss -1996.43982±0.00000  bestvalidloss -2074.52473  last_update 4\n",
      "train: iter 152  trainloss -2085.00566  validloss -1990.76064±0.00000  bestvalidloss -2074.52473  last_update 5\n",
      "train: iter 153  trainloss -2120.98721  validloss -2059.95657±0.00000  bestvalidloss -2074.52473  last_update 6\n",
      "train: iter 154  trainloss -2089.38387  validloss -2065.09089±0.00000  bestvalidloss -2074.52473  last_update 7\n",
      "train: iter 155  trainloss -1877.97910  validloss -1882.68853±0.00000  bestvalidloss -2074.52473  last_update 8\n",
      "train: iter 156  trainloss -2076.95949  validloss -2005.58424±0.00000  bestvalidloss -2074.52473  last_update 9\n",
      "train: iter 157  trainloss -2072.00869  validloss -1978.39995±0.00000  bestvalidloss -2074.52473  last_update 10\n",
      "train: iter 158  trainloss -2123.13775  validloss -2061.71045±0.00000  bestvalidloss -2074.52473  last_update 11\n",
      "train: iter 159  trainloss -2128.53293  validloss -2072.91362±0.00000  bestvalidloss -2074.52473  last_update 12\n",
      "train: iter 160  trainloss -2128.58967  validloss -2086.50718±0.00000  bestvalidloss -2086.50718  last_update 0\n",
      "train: iter 161  trainloss -2124.55486  validloss -2073.59273±0.00000  bestvalidloss -2086.50718  last_update 1\n",
      "train: iter 162  trainloss -2091.22026  validloss -2095.86995±0.00000  bestvalidloss -2095.86995  last_update 0\n",
      "train: iter 163  trainloss -2097.51700  validloss -2060.98380±0.00000  bestvalidloss -2095.86995  last_update 1\n",
      "train: iter 164  trainloss -2096.68522  validloss -2037.61928±0.00000  bestvalidloss -2095.86995  last_update 2\n",
      "train: iter 165  trainloss -2117.03090  validloss -2067.23354±0.00000  bestvalidloss -2095.86995  last_update 3\n",
      "train: iter 166  trainloss -2107.04354  validloss -2046.89655±0.00000  bestvalidloss -2095.86995  last_update 4\n",
      "train: iter 167  trainloss -1981.14782  validloss -1946.48513±0.00000  bestvalidloss -2095.86995  last_update 5\n",
      "train: iter 168  trainloss -2071.15546  validloss -1854.52561±0.00000  bestvalidloss -2095.86995  last_update 6\n",
      "train: iter 169  trainloss -2071.53561  validloss -2003.46006±0.00000  bestvalidloss -2095.86995  last_update 7\n",
      "train: iter 170  trainloss -2127.63885  validloss -2098.86813±0.00000  bestvalidloss -2098.86813  last_update 0\n",
      "train: iter 171  trainloss -2099.15258  validloss -1988.67621±0.00000  bestvalidloss -2098.86813  last_update 1\n",
      "train: iter 172  trainloss -2083.01126  validloss -2073.16016±0.00000  bestvalidloss -2098.86813  last_update 2\n",
      "train: iter 173  trainloss -2116.98099  validloss -2061.65654±0.00000  bestvalidloss -2098.86813  last_update 3\n",
      "train: iter 174  trainloss -2117.12118  validloss -1937.07985±0.00000  bestvalidloss -2098.86813  last_update 4\n",
      "train: iter 175  trainloss -2129.90520  validloss -2073.69954±0.00000  bestvalidloss -2098.86813  last_update 5\n",
      "train: iter 176  trainloss -2106.78072  validloss -2094.97603±0.00000  bestvalidloss -2098.86813  last_update 6\n",
      "train: iter 177  trainloss -2101.10275  validloss -2058.97755±0.00000  bestvalidloss -2098.86813  last_update 7\n",
      "train: iter 178  trainloss -1799.39559  validloss -1917.47057±0.00000  bestvalidloss -2098.86813  last_update 8\n",
      "train: iter 179  trainloss -2107.85453  validloss -2030.83055±0.00000  bestvalidloss -2098.86813  last_update 9\n",
      "train: iter 180  trainloss -2095.55460  validloss -2075.47686±0.00000  bestvalidloss -2098.86813  last_update 10\n",
      "train: iter 181  trainloss -2135.24628  validloss -2048.43859±0.00000  bestvalidloss -2098.86813  last_update 11\n",
      "train: iter 182  trainloss -2135.85275  validloss -2087.20015±0.00000  bestvalidloss -2098.86813  last_update 12\n",
      "train: iter 183  trainloss -2144.66996  validloss -2041.90867±0.00000  bestvalidloss -2098.86813  last_update 13\n",
      "train: iter 184  trainloss -2120.37496  validloss -2106.36766±0.00000  bestvalidloss -2106.36766  last_update 0\n",
      "train: iter 185  trainloss -2137.01366  validloss -2093.79630±0.00000  bestvalidloss -2106.36766  last_update 1\n",
      "train: iter 186  trainloss -2029.15489  validloss -2106.20197±0.00000  bestvalidloss -2106.36766  last_update 2\n",
      "train: iter 187  trainloss -2095.40848  validloss -1990.73805±0.00000  bestvalidloss -2106.36766  last_update 3\n",
      "train: iter 188  trainloss -2133.62544  validloss -2112.34467±0.00000  bestvalidloss -2112.34467  last_update 0\n",
      "train: iter 189  trainloss -2159.21028  validloss -2106.03377±0.00000  bestvalidloss -2112.34467  last_update 1\n",
      "train: iter 190  trainloss -2144.46274  validloss -2109.05125±0.00000  bestvalidloss -2112.34467  last_update 2\n",
      "train: iter 191  trainloss -2138.44748  validloss -2099.52514±0.00000  bestvalidloss -2112.34467  last_update 3\n",
      "train: iter 192  trainloss -2142.68102  validloss -2112.50285±0.00000  bestvalidloss -2112.50285  last_update 0\n",
      "train: iter 193  trainloss -2157.76402  validloss -2122.49127±0.00000  bestvalidloss -2122.49127  last_update 0\n",
      "train: iter 194  trainloss -2100.71643  validloss -2108.22698±0.00000  bestvalidloss -2122.49127  last_update 1\n",
      "train: iter 195  trainloss -2089.98271  validloss -2086.29117±0.00000  bestvalidloss -2122.49127  last_update 2\n",
      "train: iter 196  trainloss -2119.13336  validloss -2092.00021±0.00000  bestvalidloss -2122.49127  last_update 3\n",
      "train: iter 197  trainloss -2094.78926  validloss -2087.18487±0.00000  bestvalidloss -2122.49127  last_update 4\n",
      "train: iter 198  trainloss -2009.52685  validloss -2023.01309±0.00000  bestvalidloss -2122.49127  last_update 5\n",
      "train: iter 199  trainloss -2047.91982  validloss -1785.79273±0.00000  bestvalidloss -2122.49127  last_update 6\n",
      "train: iter 200  trainloss -2079.05610  validloss -2099.46684±0.00000  bestvalidloss -2122.49127  last_update 7\n",
      "train: iter 201  trainloss -2110.14922  validloss -2087.71866±0.00000  bestvalidloss -2122.49127  last_update 8\n",
      "train: iter 202  trainloss -2154.13346  validloss -2096.56464±0.00000  bestvalidloss -2122.49127  last_update 9\n",
      "train: iter 203  trainloss -2115.79612  validloss -2102.09469±0.00000  bestvalidloss -2122.49127  last_update 10\n",
      "train: iter 204  trainloss -2154.59686  validloss -2124.57308±0.00000  bestvalidloss -2124.57308  last_update 0\n",
      "train: iter 205  trainloss -2132.06852  validloss -2098.04283±0.00000  bestvalidloss -2124.57308  last_update 1\n",
      "train: iter 206  trainloss -2151.91053  validloss -2090.67741±0.00000  bestvalidloss -2124.57308  last_update 2\n",
      "train: iter 207  trainloss -2160.34716  validloss -2119.71633±0.00000  bestvalidloss -2124.57308  last_update 3\n",
      "train: iter 208  trainloss -2171.42718  validloss -2088.13643±0.00000  bestvalidloss -2124.57308  last_update 4\n",
      "train: iter 209  trainloss -2149.51466  validloss -2094.51296±0.00000  bestvalidloss -2124.57308  last_update 5\n",
      "train: iter 210  trainloss -2150.54239  validloss -2093.72020±0.00000  bestvalidloss -2124.57308  last_update 6\n",
      "train: iter 211  trainloss -2126.04214  validloss -2099.37726±0.00000  bestvalidloss -2124.57308  last_update 7\n",
      "train: iter 212  trainloss -2160.93392  validloss -2086.43327±0.00000  bestvalidloss -2124.57308  last_update 8\n",
      "train: iter 213  trainloss -2121.21778  validloss -2103.04868±0.00000  bestvalidloss -2124.57308  last_update 9\n",
      "train: iter 214  trainloss -2077.71951  validloss -2054.64835±0.00000  bestvalidloss -2124.57308  last_update 10\n",
      "train: iter 215  trainloss -2157.86356  validloss -2023.40178±0.00000  bestvalidloss -2124.57308  last_update 11\n",
      "train: iter 216  trainloss -2168.82136  validloss -2084.46981±0.00000  bestvalidloss -2124.57308  last_update 12\n",
      "train: iter 217  trainloss -2165.52219  validloss -2148.77823±0.00000  bestvalidloss -2148.77823  last_update 0\n",
      "train: iter 218  trainloss -2150.64030  validloss -2134.23093±0.00000  bestvalidloss -2148.77823  last_update 1\n",
      "train: iter 219  trainloss -2160.81887  validloss -2129.27309±0.00000  bestvalidloss -2148.77823  last_update 2\n",
      "train: iter 220  trainloss -2161.97744  validloss -2108.44221±0.00000  bestvalidloss -2148.77823  last_update 3\n",
      "train: iter 221  trainloss -2147.78655  validloss -2103.83141±0.00000  bestvalidloss -2148.77823  last_update 4\n",
      "train: iter 222  trainloss -2126.21211  validloss -2077.42947±0.00000  bestvalidloss -2148.77823  last_update 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 223  trainloss -2119.70095  validloss -1999.65002±0.00000  bestvalidloss -2148.77823  last_update 6\n",
      "train: iter 224  trainloss -2156.19170  validloss -2089.61617±0.00000  bestvalidloss -2148.77823  last_update 7\n",
      "train: iter 225  trainloss -1808.34113  validloss -2065.10311±0.00000  bestvalidloss -2148.77823  last_update 8\n",
      "train: iter 226  trainloss -2113.34507  validloss -1956.22315±0.00000  bestvalidloss -2148.77823  last_update 9\n",
      "train: iter 227  trainloss -2167.63025  validloss -2091.93611±0.00000  bestvalidloss -2148.77823  last_update 10\n",
      "train: iter 228  trainloss -2173.75998  validloss -2119.99302±0.00000  bestvalidloss -2148.77823  last_update 11\n",
      "train: iter 229  trainloss -2188.40907  validloss -2143.11258±0.00000  bestvalidloss -2148.77823  last_update 12\n",
      "train: iter 230  trainloss -2135.00846  validloss -2143.68273±0.00000  bestvalidloss -2148.77823  last_update 13\n",
      "train: iter 231  trainloss -2157.53727  validloss -2115.36561±0.00000  bestvalidloss -2148.77823  last_update 14\n",
      "train: iter 232  trainloss -2155.76074  validloss -2131.21189±0.00000  bestvalidloss -2148.77823  last_update 15\n",
      "train: iter 233  trainloss -2169.90791  validloss -2027.90456±0.00000  bestvalidloss -2148.77823  last_update 16\n",
      "train: iter 234  trainloss -2168.10676  validloss -2121.54239±0.00000  bestvalidloss -2148.77823  last_update 17\n",
      "train: iter 235  trainloss -2158.95403  validloss -2090.33438±0.00000  bestvalidloss -2148.77823  last_update 18\n",
      "train: iter 236  trainloss -2182.58149  validloss -2165.69847±0.00000  bestvalidloss -2165.69847  last_update 0\n",
      "train: iter 237  trainloss -2157.85886  validloss -2131.04459±0.00000  bestvalidloss -2165.69847  last_update 1\n",
      "train: iter 238  trainloss -2147.40662  validloss -2100.01567±0.00000  bestvalidloss -2165.69847  last_update 2\n",
      "train: iter 239  trainloss -2163.49210  validloss -2087.24781±0.00000  bestvalidloss -2165.69847  last_update 3\n",
      "train: iter 240  trainloss -2158.81225  validloss -2137.88513±0.00000  bestvalidloss -2165.69847  last_update 4\n",
      "train: iter 241  trainloss -2167.69126  validloss -2160.08664±0.00000  bestvalidloss -2165.69847  last_update 5\n",
      "train: iter 242  trainloss -2174.87631  validloss -2164.89745±0.00000  bestvalidloss -2165.69847  last_update 6\n",
      "train: iter 243  trainloss -1848.06585  validloss -2085.48759±0.00000  bestvalidloss -2165.69847  last_update 7\n",
      "train: iter 244  trainloss -2005.23739  validloss -1831.14238±0.00000  bestvalidloss -2165.69847  last_update 8\n",
      "train: iter 245  trainloss -2169.59484  validloss -2117.77942±0.00000  bestvalidloss -2165.69847  last_update 9\n",
      "train: iter 246  trainloss -2187.56024  validloss -2101.40306±0.00000  bestvalidloss -2165.69847  last_update 10\n",
      "train: iter 247  trainloss -2191.63786  validloss -2147.93020±0.00000  bestvalidloss -2165.69847  last_update 11\n",
      "train: iter 248  trainloss -2174.20226  validloss -2107.20170±0.00000  bestvalidloss -2165.69847  last_update 12\n",
      "train: iter 249  trainloss -2168.59582  validloss -2123.52770±0.00000  bestvalidloss -2165.69847  last_update 13\n",
      "train: iter 250  trainloss -2131.58286  validloss -2125.01379±0.00000  bestvalidloss -2165.69847  last_update 14\n",
      "train: iter 251  trainloss -2181.47000  validloss -2065.58550±0.00000  bestvalidloss -2165.69847  last_update 15\n",
      "train: iter 252  trainloss -2140.58566  validloss -2140.95928±0.00000  bestvalidloss -2165.69847  last_update 16\n",
      "train: iter 253  trainloss -2185.93951  validloss -2149.08688±0.00000  bestvalidloss -2165.69847  last_update 17\n",
      "train: iter 254  trainloss -2188.38885  validloss -2161.55871±0.00000  bestvalidloss -2165.69847  last_update 18\n",
      "train: iter 255  trainloss -2138.46949  validloss -2142.09250±0.00000  bestvalidloss -2165.69847  last_update 19\n",
      "train: iter 256  trainloss -2182.24967  validloss -2165.23571±0.00000  bestvalidloss -2165.69847  last_update 20\n",
      "train: iter 257  trainloss -2129.70745  validloss -2065.37281±0.00000  bestvalidloss -2165.69847  last_update 21\n",
      "train: iter 258  trainloss -2177.65895  validloss -2132.70204±0.00000  bestvalidloss -2165.69847  last_update 22\n",
      "train: iter 259  trainloss -2138.52500  validloss -2107.55472±0.00000  bestvalidloss -2165.69847  last_update 23\n",
      "train: iter 260  trainloss -2120.20632  validloss -2087.69156±0.00000  bestvalidloss -2165.69847  last_update 24\n",
      "train: iter 261  trainloss -2122.82326  validloss -2071.53212±0.00000  bestvalidloss -2165.69847  last_update 25\n",
      "train: iter 262  trainloss -2158.44187  validloss -2095.50534±0.00000  bestvalidloss -2165.69847  last_update 26\n",
      "train: iter 263  trainloss -2189.88380  validloss -2129.86230±0.00000  bestvalidloss -2165.69847  last_update 27\n",
      "train: iter 264  trainloss -2180.30722  validloss -2170.61210±0.00000  bestvalidloss -2170.61210  last_update 0\n",
      "train: iter 265  trainloss -2137.05394  validloss -2148.40558±0.00000  bestvalidloss -2170.61210  last_update 1\n",
      "train: iter 266  trainloss -2135.01435  validloss -2132.15203±0.00000  bestvalidloss -2170.61210  last_update 2\n",
      "train: iter 267  trainloss -2145.44768  validloss -2132.46268±0.00000  bestvalidloss -2170.61210  last_update 3\n",
      "train: iter 268  trainloss -2182.46267  validloss -2146.29656±0.00000  bestvalidloss -2170.61210  last_update 4\n",
      "train: iter 269  trainloss -2098.03033  validloss -2126.97056±0.00000  bestvalidloss -2170.61210  last_update 5\n",
      "train: iter 270  trainloss -1991.60364  validloss -1918.90355±0.00000  bestvalidloss -2170.61210  last_update 6\n",
      "train: iter 271  trainloss -2169.03460  validloss -2111.08971±0.00000  bestvalidloss -2170.61210  last_update 7\n",
      "train: iter 272  trainloss -2208.42546  validloss -2183.75659±0.00000  bestvalidloss -2183.75659  last_update 0\n",
      "train: iter 273  trainloss -2180.03238  validloss -2177.89230±0.00000  bestvalidloss -2183.75659  last_update 1\n",
      "train: iter 274  trainloss -2197.37517  validloss -2156.85015±0.00000  bestvalidloss -2183.75659  last_update 2\n",
      "train: iter 275  trainloss -2188.17754  validloss -2169.75051±0.00000  bestvalidloss -2183.75659  last_update 3\n",
      "train: iter 276  trainloss -2166.14646  validloss -2156.66078±0.00000  bestvalidloss -2183.75659  last_update 4\n",
      "train: iter 277  trainloss -2196.45239  validloss -2160.10369±0.00000  bestvalidloss -2183.75659  last_update 5\n",
      "train: iter 278  trainloss -2193.68815  validloss -2165.38372±0.00000  bestvalidloss -2183.75659  last_update 6\n",
      "train: iter 279  trainloss -2209.38511  validloss -2177.13308±0.00000  bestvalidloss -2183.75659  last_update 7\n",
      "train: iter 280  trainloss -2153.65543  validloss -2165.84617±0.00000  bestvalidloss -2183.75659  last_update 8\n",
      "train: iter 281  trainloss -2182.61260  validloss -2075.93176±0.00000  bestvalidloss -2183.75659  last_update 9\n",
      "train: iter 282  trainloss -2203.28843  validloss -2181.37925±0.00000  bestvalidloss -2183.75659  last_update 10\n",
      "train: iter 283  trainloss -2186.58216  validloss -2149.69476±0.00000  bestvalidloss -2183.75659  last_update 11\n",
      "train: iter 284  trainloss -2165.30021  validloss -2146.67850±0.00000  bestvalidloss -2183.75659  last_update 12\n",
      "train: iter 285  trainloss -2158.74372  validloss -2094.61328±0.00000  bestvalidloss -2183.75659  last_update 13\n",
      "train: iter 286  trainloss -2200.75750  validloss -2177.42945±0.00000  bestvalidloss -2183.75659  last_update 14\n",
      "train: iter 287  trainloss -2198.69309  validloss -2145.31350±0.00000  bestvalidloss -2183.75659  last_update 15\n",
      "train: iter 288  trainloss -2202.94516  validloss -2184.94150±0.00000  bestvalidloss -2184.94150  last_update 0\n",
      "train: iter 289  trainloss -2168.98080  validloss -2110.94848±0.00000  bestvalidloss -2184.94150  last_update 1\n",
      "train: iter 290  trainloss -2186.59758  validloss -2153.46833±0.00000  bestvalidloss -2184.94150  last_update 2\n",
      "train: iter 291  trainloss -2174.45867  validloss -2164.05262±0.00000  bestvalidloss -2184.94150  last_update 3\n",
      "train: iter 292  trainloss -2204.61481  validloss -2151.69960±0.00000  bestvalidloss -2184.94150  last_update 4\n",
      "train: iter 293  trainloss -2194.89934  validloss -2162.85900±0.00000  bestvalidloss -2184.94150  last_update 5\n",
      "train: iter 294  trainloss -2181.86363  validloss -2172.18817±0.00000  bestvalidloss -2184.94150  last_update 6\n",
      "train: iter 295  trainloss -2184.39830  validloss -2158.69451±0.00000  bestvalidloss -2184.94150  last_update 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 296  trainloss -2188.65530  validloss -2134.03969±0.00000  bestvalidloss -2184.94150  last_update 8\n",
      "train: iter 297  trainloss -2111.05498  validloss -2142.22634±0.00000  bestvalidloss -2184.94150  last_update 9\n",
      "train: iter 298  trainloss -2159.59117  validloss -2097.09366±0.00000  bestvalidloss -2184.94150  last_update 10\n",
      "train: iter 299  trainloss -2170.26892  validloss -2074.67956±0.00000  bestvalidloss -2184.94150  last_update 11\n",
      "train: iter 300  trainloss -2133.91374  validloss -2129.91341±0.00000  bestvalidloss -2184.94150  last_update 12\n",
      "train: iter 301  trainloss -2120.71937  validloss -2126.45934±0.00000  bestvalidloss -2184.94150  last_update 13\n",
      "train: iter 302  trainloss -2151.42913  validloss -1931.56708±0.00000  bestvalidloss -2184.94150  last_update 14\n",
      "train: iter 303  trainloss -2201.44228  validloss -2156.06434±0.00000  bestvalidloss -2184.94150  last_update 15\n",
      "train: iter 304  trainloss -2211.93545  validloss -2184.60827±0.00000  bestvalidloss -2184.94150  last_update 16\n",
      "train: iter 305  trainloss -1994.59622  validloss -2156.43662±0.00000  bestvalidloss -2184.94150  last_update 17\n",
      "train: iter 306  trainloss -2181.20862  validloss -2139.98113±0.00000  bestvalidloss -2184.94150  last_update 18\n",
      "train: iter 307  trainloss -2184.69662  validloss -2159.03780±0.00000  bestvalidloss -2184.94150  last_update 19\n",
      "train: iter 308  trainloss -2217.24893  validloss -2172.20349±0.00000  bestvalidloss -2184.94150  last_update 20\n",
      "train: iter 309  trainloss -2213.55661  validloss -2157.74036±0.00000  bestvalidloss -2184.94150  last_update 21\n",
      "train: iter 310  trainloss -2203.36313  validloss -2162.27942±0.00000  bestvalidloss -2184.94150  last_update 22\n",
      "train: iter 311  trainloss -2213.17994  validloss -2187.25344±0.00000  bestvalidloss -2187.25344  last_update 0\n",
      "train: iter 312  trainloss -2187.49660  validloss -2165.48542±0.00000  bestvalidloss -2187.25344  last_update 1\n",
      "train: iter 313  trainloss -2203.51069  validloss -2117.71925±0.00000  bestvalidloss -2187.25344  last_update 2\n",
      "train: iter 314  trainloss -2212.13782  validloss -2193.54014±0.00000  bestvalidloss -2193.54014  last_update 0\n",
      "train: iter 315  trainloss -2183.19972  validloss -2173.49712±0.00000  bestvalidloss -2193.54014  last_update 1\n",
      "train: iter 316  trainloss -2105.85852  validloss -2078.02876±0.00000  bestvalidloss -2193.54014  last_update 2\n",
      "train: iter 317  trainloss -2176.13035  validloss -2079.30163±0.00000  bestvalidloss -2193.54014  last_update 3\n",
      "train: iter 318  trainloss -2218.94946  validloss -2181.55623±0.00000  bestvalidloss -2193.54014  last_update 4\n",
      "train: iter 319  trainloss -2186.24840  validloss -2142.13075±0.00000  bestvalidloss -2193.54014  last_update 5\n",
      "train: iter 320  trainloss -2209.29845  validloss -2172.87364±0.00000  bestvalidloss -2193.54014  last_update 6\n",
      "train: iter 321  trainloss -2206.00794  validloss -2186.07316±0.00000  bestvalidloss -2193.54014  last_update 7\n",
      "train: iter 322  trainloss -2140.16981  validloss -2110.93345±0.00000  bestvalidloss -2193.54014  last_update 8\n",
      "train: iter 323  trainloss -2217.51452  validloss -2198.58873±0.00000  bestvalidloss -2198.58873  last_update 0\n",
      "train: iter 324  trainloss -2160.38326  validloss -2166.02887±0.00000  bestvalidloss -2198.58873  last_update 1\n",
      "train: iter 325  trainloss -2183.24572  validloss -2065.76988±0.00000  bestvalidloss -2198.58873  last_update 2\n",
      "train: iter 326  trainloss -2149.31526  validloss -2155.82722±0.00000  bestvalidloss -2198.58873  last_update 3\n",
      "train: iter 327  trainloss -2197.20075  validloss -2101.51251±0.00000  bestvalidloss -2198.58873  last_update 4\n",
      "train: iter 328  trainloss -2224.00530  validloss -2197.80419±0.00000  bestvalidloss -2198.58873  last_update 5\n",
      "train: iter 329  trainloss -2210.62245  validloss -2190.51415±0.00000  bestvalidloss -2198.58873  last_update 6\n",
      "train: iter 330  trainloss -2184.62881  validloss -2169.89556±0.00000  bestvalidloss -2198.58873  last_update 7\n",
      "train: iter 331  trainloss -2209.59235  validloss -2179.79201±0.00000  bestvalidloss -2198.58873  last_update 8\n",
      "train: iter 332  trainloss -2208.08011  validloss -2165.95906±0.00000  bestvalidloss -2198.58873  last_update 9\n",
      "train: iter 333  trainloss -2204.41255  validloss -2156.05457±0.00000  bestvalidloss -2198.58873  last_update 10\n",
      "train: iter 334  trainloss -2204.43964  validloss -2189.99291±0.00000  bestvalidloss -2198.58873  last_update 11\n",
      "train: iter 335  trainloss -2216.84578  validloss -2166.42224±0.00000  bestvalidloss -2198.58873  last_update 12\n",
      "train: iter 336  trainloss -2205.75994  validloss -2165.96305±0.00000  bestvalidloss -2198.58873  last_update 13\n",
      "train: iter 337  trainloss -2211.71139  validloss -2182.66911±0.00000  bestvalidloss -2198.58873  last_update 14\n",
      "train: iter 338  trainloss -2138.06462  validloss -2146.37746±0.00000  bestvalidloss -2198.58873  last_update 15\n",
      "train: iter 339  trainloss -2204.24562  validloss -2169.30715±0.00000  bestvalidloss -2198.58873  last_update 16\n",
      "train: iter 340  trainloss -2186.42667  validloss -2175.77225±0.00000  bestvalidloss -2198.58873  last_update 17\n",
      "train: iter 341  trainloss -2215.07323  validloss -2173.15557±0.00000  bestvalidloss -2198.58873  last_update 18\n",
      "train: iter 342  trainloss -2225.09719  validloss -2182.49023±0.00000  bestvalidloss -2198.58873  last_update 19\n",
      "train: iter 343  trainloss -2212.33138  validloss -2211.64000±0.00000  bestvalidloss -2211.64000  last_update 0\n",
      "train: iter 344  trainloss -2218.20677  validloss -2214.40347±0.00000  bestvalidloss -2214.40347  last_update 0\n",
      "train: iter 345  trainloss -2211.37189  validloss -2193.14202±0.00000  bestvalidloss -2214.40347  last_update 1\n",
      "train: iter 346  trainloss -2158.37115  validloss -2200.13528±0.00000  bestvalidloss -2214.40347  last_update 2\n",
      "train: iter 347  trainloss -2090.34594  validloss -2168.33344±0.00000  bestvalidloss -2214.40347  last_update 3\n",
      "train: iter 348  trainloss -2115.02513  validloss -2167.06505±0.00000  bestvalidloss -2214.40347  last_update 4\n",
      "train: iter 349  trainloss -2200.00015  validloss -2148.11683±0.00000  bestvalidloss -2214.40347  last_update 5\n",
      "train: iter 350  trainloss -2214.67345  validloss -2186.86367±0.00000  bestvalidloss -2214.40347  last_update 6\n",
      "train: iter 351  trainloss -2226.64327  validloss -2166.46173±0.00000  bestvalidloss -2214.40347  last_update 7\n",
      "train: iter 352  trainloss -2229.09683  validloss -2140.06871±0.00000  bestvalidloss -2214.40347  last_update 8\n",
      "train: iter 353  trainloss -2208.46412  validloss -2204.00618±0.00000  bestvalidloss -2214.40347  last_update 9\n",
      "train: iter 354  trainloss -2222.17128  validloss -2148.20649±0.00000  bestvalidloss -2214.40347  last_update 10\n",
      "train: iter 355  trainloss -2192.50813  validloss -2213.23721±0.00000  bestvalidloss -2214.40347  last_update 11\n",
      "train: iter 356  trainloss -2199.95261  validloss -2126.65883±0.00000  bestvalidloss -2214.40347  last_update 12\n",
      "train: iter 357  trainloss -2174.81151  validloss -2176.00441±0.00000  bestvalidloss -2214.40347  last_update 13\n",
      "train: iter 358  trainloss -2200.71115  validloss -2091.78665±0.00000  bestvalidloss -2214.40347  last_update 14\n",
      "train: iter 359  trainloss -2237.08039  validloss -2213.89801±0.00000  bestvalidloss -2214.40347  last_update 15\n",
      "train: iter 360  trainloss -2235.14023  validloss -2214.87576±0.00000  bestvalidloss -2214.87576  last_update 0\n",
      "train: iter 361  trainloss -2227.41598  validloss -2215.69409±0.00000  bestvalidloss -2215.69409  last_update 0\n",
      "train: iter 362  trainloss -2200.26217  validloss -2195.60776±0.00000  bestvalidloss -2215.69409  last_update 1\n",
      "train: iter 363  trainloss -2226.96393  validloss -2212.18405±0.00000  bestvalidloss -2215.69409  last_update 2\n",
      "train: iter 364  trainloss -2130.06321  validloss -2188.23040±0.00000  bestvalidloss -2215.69409  last_update 3\n",
      "train: iter 365  trainloss -2145.18296  validloss -2061.43280±0.00000  bestvalidloss -2215.69409  last_update 4\n",
      "train: iter 366  trainloss -2219.03325  validloss -2139.26765±0.00000  bestvalidloss -2215.69409  last_update 5\n",
      "train: iter 367  trainloss -2202.38463  validloss -2200.02135±0.00000  bestvalidloss -2215.69409  last_update 6\n",
      "train: iter 368  trainloss -2227.86494  validloss -2207.62136±0.00000  bestvalidloss -2215.69409  last_update 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 369  trainloss -2203.24139  validloss -2192.12090±0.00000  bestvalidloss -2215.69409  last_update 8\n",
      "train: iter 370  trainloss -2207.72414  validloss -2174.19763±0.00000  bestvalidloss -2215.69409  last_update 9\n",
      "train: iter 371  trainloss -2216.26061  validloss -2206.43206±0.00000  bestvalidloss -2215.69409  last_update 10\n",
      "train: iter 372  trainloss -2153.57332  validloss -2142.27079±0.00000  bestvalidloss -2215.69409  last_update 11\n",
      "train: iter 373  trainloss -2213.12742  validloss -2189.71168±0.00000  bestvalidloss -2215.69409  last_update 12\n",
      "train: iter 374  trainloss -2238.01724  validloss -2220.74340±0.00000  bestvalidloss -2220.74340  last_update 0\n",
      "train: iter 375  trainloss -2238.20798  validloss -2205.21506±0.00000  bestvalidloss -2220.74340  last_update 1\n",
      "train: iter 376  trainloss -2236.55676  validloss -2204.40042±0.00000  bestvalidloss -2220.74340  last_update 2\n",
      "train: iter 377  trainloss -2232.45394  validloss -2231.62198±0.00000  bestvalidloss -2231.62198  last_update 0\n",
      "train: iter 378  trainloss -2187.33435  validloss -2184.17367±0.00000  bestvalidloss -2231.62198  last_update 1\n",
      "train: iter 379  trainloss -2223.50788  validloss -2187.17612±0.00000  bestvalidloss -2231.62198  last_update 2\n",
      "train: iter 380  trainloss -2228.90355  validloss -2214.23541±0.00000  bestvalidloss -2231.62198  last_update 3\n",
      "train: iter 381  trainloss -2242.77900  validloss -2207.58956±0.00000  bestvalidloss -2231.62198  last_update 4\n",
      "train: iter 382  trainloss -2244.86696  validloss -2215.74224±0.00000  bestvalidloss -2231.62198  last_update 5\n",
      "train: iter 383  trainloss -2190.15126  validloss -2195.05316±0.00000  bestvalidloss -2231.62198  last_update 6\n",
      "train: iter 384  trainloss -2209.12287  validloss -2194.46183±0.00000  bestvalidloss -2231.62198  last_update 7\n",
      "train: iter 385  trainloss -2222.27061  validloss -2211.05200±0.00000  bestvalidloss -2231.62198  last_update 8\n",
      "train: iter 386  trainloss -2197.45299  validloss -2213.47938±0.00000  bestvalidloss -2231.62198  last_update 9\n",
      "train: iter 387  trainloss -2221.19514  validloss -2156.62527±0.00000  bestvalidloss -2231.62198  last_update 10\n",
      "train: iter 388  trainloss -2219.28759  validloss -2191.08203±0.00000  bestvalidloss -2231.62198  last_update 11\n",
      "train: iter 389  trainloss -2197.09757  validloss -2129.74687±0.00000  bestvalidloss -2231.62198  last_update 12\n",
      "train: iter 390  trainloss -2177.74373  validloss -2120.21841±0.00000  bestvalidloss -2231.62198  last_update 13\n",
      "train: iter 391  trainloss -2230.21077  validloss -2211.07247±0.00000  bestvalidloss -2231.62198  last_update 14\n",
      "train: iter 392  trainloss -2173.83117  validloss -2207.24799±0.00000  bestvalidloss -2231.62198  last_update 15\n",
      "train: iter 393  trainloss -2227.70759  validloss -2189.23300±0.00000  bestvalidloss -2231.62198  last_update 16\n",
      "train: iter 394  trainloss -2225.40018  validloss -2179.96755±0.00000  bestvalidloss -2231.62198  last_update 17\n",
      "train: iter 395  trainloss -2205.55522  validloss -2211.89831±0.00000  bestvalidloss -2231.62198  last_update 18\n",
      "train: iter 396  trainloss -2224.96502  validloss -2206.09760±0.00000  bestvalidloss -2231.62198  last_update 19\n",
      "train: iter 397  trainloss -2214.77236  validloss -2209.75942±0.00000  bestvalidloss -2231.62198  last_update 20\n",
      "train: iter 398  trainloss -2210.04311  validloss -2158.49047±0.00000  bestvalidloss -2231.62198  last_update 21\n",
      "train: iter 399  trainloss -2222.81895  validloss -2192.00513±0.00000  bestvalidloss -2231.62198  last_update 22\n",
      "train: iter 400  trainloss -2243.37278  validloss -2193.99874±0.00000  bestvalidloss -2231.62198  last_update 23\n",
      "train: iter 401  trainloss -2238.26841  validloss -2229.16766±0.00000  bestvalidloss -2231.62198  last_update 24\n",
      "train: iter 402  trainloss -2213.40628  validloss -2207.10405±0.00000  bestvalidloss -2231.62198  last_update 25\n",
      "train: iter 403  trainloss -2171.64440  validloss -2148.38828±0.00000  bestvalidloss -2231.62198  last_update 26\n",
      "train: iter 404  trainloss -2185.07170  validloss -2150.86332±0.00000  bestvalidloss -2231.62198  last_update 27\n",
      "train: iter 405  trainloss -2136.23641  validloss -1999.66069±0.00000  bestvalidloss -2231.62198  last_update 28\n",
      "train: iter 406  trainloss -2165.98319  validloss -2156.64279±0.00000  bestvalidloss -2231.62198  last_update 29\n",
      "train: iter 407  trainloss -2224.95282  validloss -2185.92775±0.00000  bestvalidloss -2231.62198  last_update 30\n",
      "train: iter 408  trainloss -2250.39776  validloss -2229.72371±0.00000  bestvalidloss -2231.62198  last_update 31\n",
      "train: iter 409  trainloss -2247.73685  validloss -2222.75507±0.00000  bestvalidloss -2231.62198  last_update 32\n",
      "train: iter 410  trainloss -2242.71872  validloss -2152.80426±0.00000  bestvalidloss -2231.62198  last_update 33\n",
      "train: iter 411  trainloss -2251.14918  validloss -2224.65974±0.00000  bestvalidloss -2231.62198  last_update 34\n",
      "train: iter 412  trainloss -2103.31085  validloss -2186.29059±0.00000  bestvalidloss -2231.62198  last_update 35\n",
      "train: iter 413  trainloss -2184.67527  validloss -2147.42747±0.00000  bestvalidloss -2231.62198  last_update 36\n",
      "train: iter 414  trainloss -2246.58802  validloss -2220.16625±0.00000  bestvalidloss -2231.62198  last_update 37\n",
      "train: iter 415  trainloss -2247.40802  validloss -2207.44135±0.00000  bestvalidloss -2231.62198  last_update 38\n",
      "train: iter 416  trainloss -2212.35276  validloss -2179.21609±0.00000  bestvalidloss -2231.62198  last_update 39\n",
      "train: iter 417  trainloss -2218.31902  validloss -2107.95642±0.00000  bestvalidloss -2231.62198  last_update 40\n",
      "train: iter 418  trainloss -2194.72254  validloss -2112.73655±0.00000  bestvalidloss -2231.62198  last_update 41\n",
      "train: iter 419  trainloss -2230.67267  validloss -2215.41081±0.00000  bestvalidloss -2231.62198  last_update 42\n",
      "train: iter 420  trainloss -2222.62652  validloss -2208.23762±0.00000  bestvalidloss -2231.62198  last_update 43\n",
      "train: iter 421  trainloss -2231.53976  validloss -2163.17228±0.00000  bestvalidloss -2231.62198  last_update 44\n",
      "train: iter 422  trainloss -2241.58767  validloss -2179.09032±0.00000  bestvalidloss -2231.62198  last_update 45\n",
      "train: iter 423  trainloss -2233.19706  validloss -2207.05963±0.00000  bestvalidloss -2231.62198  last_update 46\n",
      "train: iter 424  trainloss -2214.38620  validloss -2169.81771±0.00000  bestvalidloss -2231.62198  last_update 47\n",
      "train: iter 425  trainloss -2187.21961  validloss -2073.65767±0.00000  bestvalidloss -2231.62198  last_update 48\n",
      "train: iter 426  trainloss -2189.35334  validloss -2190.50800±0.00000  bestvalidloss -2231.62198  last_update 49\n",
      "train: iter 427  trainloss -2248.14236  validloss -2223.47536±0.00000  bestvalidloss -2231.62198  last_update 50\n",
      "train: iter 428  trainloss -2199.51836  validloss -2216.33537±0.00000  bestvalidloss -2231.62198  last_update 51\n",
      "train: iter 429  trainloss -1998.17470  validloss -2117.60146±0.00000  bestvalidloss -2231.62198  last_update 52\n",
      "train: iter 430  trainloss -2223.68829  validloss -2087.50979±0.00000  bestvalidloss -2231.62198  last_update 53\n",
      "train: iter 431  trainloss -2220.62357  validloss -2103.81136±0.00000  bestvalidloss -2231.62198  last_update 54\n",
      "train: iter 432  trainloss -2259.23405  validloss -2223.99416±0.00000  bestvalidloss -2231.62198  last_update 55\n",
      "train: iter 433  trainloss -2249.90245  validloss -2217.89261±0.00000  bestvalidloss -2231.62198  last_update 56\n",
      "train: iter 434  trainloss -2255.49270  validloss -2213.96802±0.00000  bestvalidloss -2231.62198  last_update 57\n",
      "train: iter 435  trainloss -2248.60226  validloss -2228.23720±0.00000  bestvalidloss -2231.62198  last_update 58\n",
      "train: iter 436  trainloss -2243.54014  validloss -2170.35050±0.00000  bestvalidloss -2231.62198  last_update 59\n",
      "train: iter 437  trainloss -2257.24827  validloss -2180.03297±0.00000  bestvalidloss -2231.62198  last_update 60\n",
      "train: iter 438  trainloss -2266.64491  validloss -2218.26235±0.00000  bestvalidloss -2231.62198  last_update 61\n",
      "train: iter 439  trainloss -2246.82153  validloss -2206.77472±0.00000  bestvalidloss -2231.62198  last_update 62\n",
      "train: iter 440  trainloss -2248.42031  validloss -2208.98870±0.00000  bestvalidloss -2231.62198  last_update 63\n",
      "train: iter 441  trainloss -2223.69546  validloss -2197.11705±0.00000  bestvalidloss -2231.62198  last_update 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 442  trainloss -2248.41974  validloss -2201.56756±0.00000  bestvalidloss -2231.62198  last_update 65\n",
      "train: iter 443  trainloss -2249.79475  validloss -2176.85511±0.00000  bestvalidloss -2231.62198  last_update 66\n",
      "train: iter 444  trainloss -2230.31098  validloss -2202.77836±0.00000  bestvalidloss -2231.62198  last_update 67\n",
      "train: iter 445  trainloss -2238.91728  validloss -2149.39258±0.00000  bestvalidloss -2231.62198  last_update 68\n",
      "train: iter 446  trainloss -2251.02716  validloss -2191.98974±0.00000  bestvalidloss -2231.62198  last_update 69\n",
      "train: iter 447  trainloss -2226.66287  validloss -2226.43555±0.00000  bestvalidloss -2231.62198  last_update 70\n",
      "train: iter 448  trainloss -2203.92752  validloss -2125.07932±0.00000  bestvalidloss -2231.62198  last_update 71\n",
      "train: iter 449  trainloss -2226.56259  validloss -2205.97825±0.00000  bestvalidloss -2231.62198  last_update 72\n",
      "train: iter 450  trainloss -2235.56807  validloss -2191.71371±0.00000  bestvalidloss -2231.62198  last_update 73\n",
      "train: iter 451  trainloss -2245.59644  validloss -2137.01722±0.00000  bestvalidloss -2231.62198  last_update 74\n",
      "train: iter 452  trainloss -2263.28104  validloss -2220.47122±0.00000  bestvalidloss -2231.62198  last_update 75\n",
      "train: iter 453  trainloss -2234.27567  validloss -2229.03358±0.00000  bestvalidloss -2231.62198  last_update 76\n",
      "train: iter 454  trainloss -2187.47164  validloss -2138.05943±0.00000  bestvalidloss -2231.62198  last_update 77\n",
      "train: iter 455  trainloss -2215.61219  validloss -2145.32319±0.00000  bestvalidloss -2231.62198  last_update 78\n",
      "train: iter 456  trainloss -2195.20458  validloss -2064.45928±0.00000  bestvalidloss -2231.62198  last_update 79\n",
      "train: iter 457  trainloss -2263.28044  validloss -2183.37264±0.00000  bestvalidloss -2231.62198  last_update 80\n",
      "train: iter 458  trainloss -2243.96912  validloss -2220.52490±0.00000  bestvalidloss -2231.62198  last_update 81\n",
      "train: iter 459  trainloss -2244.77524  validloss -2176.57388±0.00000  bestvalidloss -2231.62198  last_update 82\n",
      "train: iter 460  trainloss -2261.13330  validloss -2220.12857±0.00000  bestvalidloss -2231.62198  last_update 83\n",
      "train: iter 461  trainloss -2260.71749  validloss -2193.80699±0.00000  bestvalidloss -2231.62198  last_update 84\n",
      "train: iter 462  trainloss -2147.00375  validloss -2172.23910±0.00000  bestvalidloss -2231.62198  last_update 85\n",
      "train: iter 463  trainloss -2264.09432  validloss -2189.01873±0.00000  bestvalidloss -2231.62198  last_update 86\n",
      "train: iter 464  trainloss -2255.22138  validloss -2225.83397±0.00000  bestvalidloss -2231.62198  last_update 87\n",
      "train: iter 465  trainloss -2259.65948  validloss -2188.27373±0.00000  bestvalidloss -2231.62198  last_update 88\n",
      "train: iter 466  trainloss -2273.74722  validloss -2116.07523±0.00000  bestvalidloss -2231.62198  last_update 89\n",
      "train: iter 467  trainloss -2266.88034  validloss -2227.26760±0.00000  bestvalidloss -2231.62198  last_update 90\n",
      "train: iter 468  trainloss -2250.67787  validloss -2191.32249±0.00000  bestvalidloss -2231.62198  last_update 91\n",
      "train: iter 469  trainloss -2262.83830  validloss -2235.04572±0.00000  bestvalidloss -2235.04572  last_update 0\n",
      "train: iter 470  trainloss -2266.53569  validloss -2213.65819±0.00000  bestvalidloss -2235.04572  last_update 1\n",
      "train: iter 471  trainloss -2255.12192  validloss -2229.33352±0.00000  bestvalidloss -2235.04572  last_update 2\n",
      "train: iter 472  trainloss -2253.73328  validloss -2199.86619±0.00000  bestvalidloss -2235.04572  last_update 3\n",
      "train: iter 473  trainloss -2168.53814  validloss -2200.32317±0.00000  bestvalidloss -2235.04572  last_update 4\n",
      "train: iter 474  trainloss -2158.48183  validloss -2131.27263±0.00000  bestvalidloss -2235.04572  last_update 5\n",
      "train: iter 475  trainloss -2259.39608  validloss -2017.46509±0.00000  bestvalidloss -2235.04572  last_update 6\n",
      "train: iter 476  trainloss -2274.11116  validloss -2214.04449±0.00000  bestvalidloss -2235.04572  last_update 7\n",
      "train: iter 477  trainloss -2276.31196  validloss -2218.53926±0.00000  bestvalidloss -2235.04572  last_update 8\n",
      "train: iter 478  trainloss -2289.23909  validloss -2192.22033±0.00000  bestvalidloss -2235.04572  last_update 9\n",
      "train: iter 479  trainloss -2269.85233  validloss -2221.87488±0.00000  bestvalidloss -2235.04572  last_update 10\n",
      "train: iter 480  trainloss -2278.58452  validloss -2210.27223±0.00000  bestvalidloss -2235.04572  last_update 11\n",
      "train: iter 481  trainloss -2286.82836  validloss -2208.84712±0.00000  bestvalidloss -2235.04572  last_update 12\n",
      "train: iter 482  trainloss -2294.65939  validloss -2216.03136±0.00000  bestvalidloss -2235.04572  last_update 13\n",
      "train: iter 483  trainloss -2280.88009  validloss -2230.92633±0.00000  bestvalidloss -2235.04572  last_update 14\n",
      "train: iter 484  trainloss -2256.87234  validloss -2196.41178±0.00000  bestvalidloss -2235.04572  last_update 15\n",
      "train: iter 485  trainloss -2256.47189  validloss -2190.45294±0.00000  bestvalidloss -2235.04572  last_update 16\n",
      "train: iter 486  trainloss -2240.94964  validloss -2232.51343±0.00000  bestvalidloss -2235.04572  last_update 17\n",
      "train: iter 487  trainloss -2230.38329  validloss -2073.76114±0.00000  bestvalidloss -2235.04572  last_update 18\n",
      "train: iter 488  trainloss -2272.54279  validloss -2245.74724±0.00000  bestvalidloss -2245.74724  last_update 0\n",
      "train: iter 489  trainloss -2262.57601  validloss -2246.84588±0.00000  bestvalidloss -2246.84588  last_update 0\n",
      "train: iter 490  trainloss -2260.70524  validloss -2221.01544±0.00000  bestvalidloss -2246.84588  last_update 1\n",
      "train: iter 491  trainloss -2269.54679  validloss -2257.26041±0.00000  bestvalidloss -2257.26041  last_update 0\n",
      "train: iter 492  trainloss -2300.62810  validloss -2220.21237±0.00000  bestvalidloss -2257.26041  last_update 1\n",
      "train: iter 493  trainloss -2292.53664  validloss -2230.67883±0.00000  bestvalidloss -2257.26041  last_update 2\n",
      "train: iter 494  trainloss -2270.80760  validloss -2226.21086±0.00000  bestvalidloss -2257.26041  last_update 3\n",
      "train: iter 495  trainloss -2280.24615  validloss -2236.83700±0.00000  bestvalidloss -2257.26041  last_update 4\n",
      "train: iter 496  trainloss -2288.28554  validloss -2210.99308±0.00000  bestvalidloss -2257.26041  last_update 5\n",
      "train: iter 497  trainloss -2302.55893  validloss -2257.54059±0.00000  bestvalidloss -2257.54059  last_update 0\n",
      "train: iter 498  trainloss -2300.66967  validloss -2235.79859±0.00000  bestvalidloss -2257.54059  last_update 1\n",
      "train: iter 499  trainloss -2291.75641  validloss -2251.78185±0.00000  bestvalidloss -2257.54059  last_update 2\n",
      "train: iter 500  trainloss -2291.63277  validloss -2233.51593±0.00000  bestvalidloss -2257.54059  last_update 3\n",
      "train: iter 501  trainloss -2263.60556  validloss -2230.65905±0.00000  bestvalidloss -2257.54059  last_update 4\n",
      "train: iter 502  trainloss -2278.72931  validloss -2115.67652±0.00000  bestvalidloss -2257.54059  last_update 5\n",
      "train: iter 503  trainloss -2267.33175  validloss -2228.97838±0.00000  bestvalidloss -2257.54059  last_update 6\n",
      "train: iter 504  trainloss -2300.57498  validloss -2238.10656±0.00000  bestvalidloss -2257.54059  last_update 7\n",
      "train: iter 505  trainloss -2289.23653  validloss -2230.83304±0.00000  bestvalidloss -2257.54059  last_update 8\n",
      "train: iter 506  trainloss -2291.84276  validloss -2254.90678±0.00000  bestvalidloss -2257.54059  last_update 9\n",
      "train: iter 507  trainloss -2290.49639  validloss -2226.53026±0.00000  bestvalidloss -2257.54059  last_update 10\n",
      "train: iter 508  trainloss -2267.85208  validloss -2245.14282±0.00000  bestvalidloss -2257.54059  last_update 11\n",
      "train: iter 509  trainloss -2194.76325  validloss -2197.32257±0.00000  bestvalidloss -2257.54059  last_update 12\n",
      "train: iter 510  trainloss -2199.11149  validloss -2038.25568±0.00000  bestvalidloss -2257.54059  last_update 13\n",
      "train: iter 511  trainloss -2251.21353  validloss -2126.23380±0.00000  bestvalidloss -2257.54059  last_update 14\n",
      "train: iter 512  trainloss -2281.98386  validloss -2149.67153±0.00000  bestvalidloss -2257.54059  last_update 15\n",
      "train: iter 513  trainloss -2282.65903  validloss -2224.35033±0.00000  bestvalidloss -2257.54059  last_update 16\n",
      "train: iter 514  trainloss -2287.84965  validloss -2221.28412±0.00000  bestvalidloss -2257.54059  last_update 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 515  trainloss -2281.23638  validloss -2240.24272±0.00000  bestvalidloss -2257.54059  last_update 18\n",
      "train: iter 516  trainloss -2297.67970  validloss -2240.25034±0.00000  bestvalidloss -2257.54059  last_update 19\n",
      "train: iter 517  trainloss -2291.38134  validloss -2251.54343±0.00000  bestvalidloss -2257.54059  last_update 20\n",
      "train: iter 518  trainloss -2252.49298  validloss -2215.01525±0.00000  bestvalidloss -2257.54059  last_update 21\n",
      "train: iter 519  trainloss -2235.43731  validloss -2165.52432±0.00000  bestvalidloss -2257.54059  last_update 22\n",
      "train: iter 520  trainloss -2275.30138  validloss -2146.39859±0.00000  bestvalidloss -2257.54059  last_update 23\n",
      "train: iter 521  trainloss -2302.95886  validloss -2211.08368±0.00000  bestvalidloss -2257.54059  last_update 24\n",
      "train: iter 522  trainloss -2311.91867  validloss -2245.55809±0.00000  bestvalidloss -2257.54059  last_update 25\n",
      "train: iter 523  trainloss -2297.27505  validloss -2253.86584±0.00000  bestvalidloss -2257.54059  last_update 26\n",
      "train: iter 524  trainloss -2269.57061  validloss -2218.66729±0.00000  bestvalidloss -2257.54059  last_update 27\n",
      "train: iter 525  trainloss -2260.29326  validloss -2196.70781±0.00000  bestvalidloss -2257.54059  last_update 28\n",
      "train: iter 526  trainloss -2276.08019  validloss -2197.83064±0.00000  bestvalidloss -2257.54059  last_update 29\n",
      "train: iter 527  trainloss -2266.04257  validloss -2208.05041±0.00000  bestvalidloss -2257.54059  last_update 30\n",
      "train: iter 528  trainloss -2285.11397  validloss -2249.09457±0.00000  bestvalidloss -2257.54059  last_update 31\n",
      "train: iter 529  trainloss -2266.71394  validloss -2234.10786±0.00000  bestvalidloss -2257.54059  last_update 32\n",
      "train: iter 530  trainloss -2290.17926  validloss -2203.32621±0.00000  bestvalidloss -2257.54059  last_update 33\n",
      "train: iter 531  trainloss -2273.88935  validloss -2245.55744±0.00000  bestvalidloss -2257.54059  last_update 34\n",
      "train: iter 532  trainloss -2290.31859  validloss -2233.21692±0.00000  bestvalidloss -2257.54059  last_update 35\n",
      "train: iter 533  trainloss -2297.47868  validloss -2235.65663±0.00000  bestvalidloss -2257.54059  last_update 36\n",
      "train: iter 534  trainloss -2287.26580  validloss -2256.57534±0.00000  bestvalidloss -2257.54059  last_update 37\n",
      "train: iter 535  trainloss -2262.01950  validloss -2188.37663±0.00000  bestvalidloss -2257.54059  last_update 38\n",
      "train: iter 536  trainloss -2283.76715  validloss -2221.58013±0.00000  bestvalidloss -2257.54059  last_update 39\n",
      "train: iter 537  trainloss -2257.76400  validloss -2214.70229±0.00000  bestvalidloss -2257.54059  last_update 40\n",
      "train: iter 538  trainloss -2299.47171  validloss -2257.79836±0.00000  bestvalidloss -2257.79836  last_update 0\n",
      "train: iter 539  trainloss -2300.74063  validloss -2255.31747±0.00000  bestvalidloss -2257.79836  last_update 1\n",
      "train: iter 540  trainloss -2303.52871  validloss -2252.49371±0.00000  bestvalidloss -2257.79836  last_update 2\n",
      "train: iter 541  trainloss -2289.82036  validloss -2238.13271±0.00000  bestvalidloss -2257.79836  last_update 3\n",
      "train: iter 542  trainloss -2285.96831  validloss -2254.56582±0.00000  bestvalidloss -2257.79836  last_update 4\n",
      "train: iter 543  trainloss -2280.73938  validloss -2244.82950±0.00000  bestvalidloss -2257.79836  last_update 5\n",
      "train: iter 544  trainloss -2222.24061  validloss -2165.48314±0.00000  bestvalidloss -2257.79836  last_update 6\n",
      "train: iter 545  trainloss -2203.92022  validloss -2169.99437±0.00000  bestvalidloss -2257.79836  last_update 7\n",
      "train: iter 546  trainloss -2244.97391  validloss -2203.06424±0.00000  bestvalidloss -2257.79836  last_update 8\n",
      "train: iter 547  trainloss -2279.21232  validloss -2196.66362±0.00000  bestvalidloss -2257.79836  last_update 9\n",
      "train: iter 548  trainloss -2301.51452  validloss -2241.87503±0.00000  bestvalidloss -2257.79836  last_update 10\n",
      "train: iter 549  trainloss -2298.25343  validloss -2243.91394±0.00000  bestvalidloss -2257.79836  last_update 11\n",
      "train: iter 550  trainloss -2302.13645  validloss -2238.74561±0.00000  bestvalidloss -2257.79836  last_update 12\n",
      "train: iter 551  trainloss -2245.46395  validloss -2223.69969±0.00000  bestvalidloss -2257.79836  last_update 13\n",
      "train: iter 552  trainloss -2272.61033  validloss -2243.87479±0.00000  bestvalidloss -2257.79836  last_update 14\n",
      "train: iter 553  trainloss -2298.21173  validloss -2231.25413±0.00000  bestvalidloss -2257.79836  last_update 15\n",
      "train: iter 554  trainloss -2297.28559  validloss -2222.20392±0.00000  bestvalidloss -2257.79836  last_update 16\n",
      "train: iter 555  trainloss -2284.74417  validloss -2207.19775±0.00000  bestvalidloss -2257.79836  last_update 17\n",
      "train: iter 556  trainloss -2295.22143  validloss -2240.80869±0.00000  bestvalidloss -2257.79836  last_update 18\n",
      "train: iter 557  trainloss -2300.54595  validloss -2224.03011±0.00000  bestvalidloss -2257.79836  last_update 19\n",
      "train: iter 558  trainloss -2287.47927  validloss -2244.91556±0.00000  bestvalidloss -2257.79836  last_update 20\n",
      "train: iter 559  trainloss -2278.59858  validloss -2256.12434±0.00000  bestvalidloss -2257.79836  last_update 21\n",
      "train: iter 560  trainloss -2282.84481  validloss -2200.36282±0.00000  bestvalidloss -2257.79836  last_update 22\n",
      "train: iter 561  trainloss -2273.70316  validloss -2173.25529±0.00000  bestvalidloss -2257.79836  last_update 23\n",
      "train: iter 562  trainloss -2297.71979  validloss -2224.60475±0.00000  bestvalidloss -2257.79836  last_update 24\n",
      "train: iter 563  trainloss -2294.48853  validloss -2236.51027±0.00000  bestvalidloss -2257.79836  last_update 25\n",
      "train: iter 564  trainloss -2300.13784  validloss -2215.94195±0.00000  bestvalidloss -2257.79836  last_update 26\n",
      "train: iter 565  trainloss -2268.87077  validloss -2203.64211±0.00000  bestvalidloss -2257.79836  last_update 27\n",
      "train: iter 566  trainloss -2291.80476  validloss -2182.64223±0.00000  bestvalidloss -2257.79836  last_update 28\n",
      "train: iter 567  trainloss -2299.99605  validloss -2246.02882±0.00000  bestvalidloss -2257.79836  last_update 29\n",
      "train: iter 568  trainloss -2271.63054  validloss -2229.33457±0.00000  bestvalidloss -2257.79836  last_update 30\n",
      "train: iter 569  trainloss -2290.11465  validloss -2237.29258±0.00000  bestvalidloss -2257.79836  last_update 31\n",
      "train: iter 570  trainloss -2292.11120  validloss -2218.14053±0.00000  bestvalidloss -2257.79836  last_update 32\n",
      "train: iter 571  trainloss -2302.30477  validloss -2237.14106±0.00000  bestvalidloss -2257.79836  last_update 33\n",
      "train: iter 572  trainloss -2296.73736  validloss -2225.79842±0.00000  bestvalidloss -2257.79836  last_update 34\n",
      "train: iter 573  trainloss -2249.58600  validloss -2246.14630±0.00000  bestvalidloss -2257.79836  last_update 35\n",
      "train: iter 574  trainloss -2242.08928  validloss -2198.79550±0.00000  bestvalidloss -2257.79836  last_update 36\n",
      "train: iter 575  trainloss -2265.90429  validloss -2101.88467±0.00000  bestvalidloss -2257.79836  last_update 37\n",
      "train: iter 576  trainloss -2299.93342  validloss -2197.76601±0.00000  bestvalidloss -2257.79836  last_update 38\n",
      "train: iter 577  trainloss -2271.99692  validloss -2220.28971±0.00000  bestvalidloss -2257.79836  last_update 39\n",
      "train: iter 578  trainloss -2299.73183  validloss -2246.73409±0.00000  bestvalidloss -2257.79836  last_update 40\n",
      "train: iter 579  trainloss -2287.35794  validloss -2221.26539±0.00000  bestvalidloss -2257.79836  last_update 41\n",
      "train: iter 580  trainloss -2305.88086  validloss -2242.30141±0.00000  bestvalidloss -2257.79836  last_update 42\n",
      "train: iter 581  trainloss -2299.86144  validloss -2246.64081±0.00000  bestvalidloss -2257.79836  last_update 43\n",
      "train: iter 582  trainloss -2294.21489  validloss -2254.34955±0.00000  bestvalidloss -2257.79836  last_update 44\n",
      "train: iter 583  trainloss -2284.93884  validloss -2229.53761±0.00000  bestvalidloss -2257.79836  last_update 45\n",
      "train: iter 584  trainloss -2284.89135  validloss -2252.54237±0.00000  bestvalidloss -2257.79836  last_update 46\n",
      "train: iter 585  trainloss -2292.38981  validloss -2216.02745±0.00000  bestvalidloss -2257.79836  last_update 47\n",
      "train: iter 586  trainloss -2159.13950  validloss -2205.57366±0.00000  bestvalidloss -2257.79836  last_update 48\n",
      "train: iter 587  trainloss -2291.11096  validloss -2135.94957±0.00000  bestvalidloss -2257.79836  last_update 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 588  trainloss -2301.47515  validloss -2230.29402±0.00000  bestvalidloss -2257.79836  last_update 50\n",
      "train: iter 589  trainloss -2307.12632  validloss -2259.67822±0.00000  bestvalidloss -2259.67822  last_update 0\n",
      "train: iter 590  trainloss -2304.75414  validloss -2236.82750±0.00000  bestvalidloss -2259.67822  last_update 1\n",
      "train: iter 591  trainloss -2315.99890  validloss -2237.41646±0.00000  bestvalidloss -2259.67822  last_update 2\n",
      "train: iter 592  trainloss -2309.92298  validloss -2252.55158±0.00000  bestvalidloss -2259.67822  last_update 3\n",
      "train: iter 593  trainloss -2302.49732  validloss -2224.11710±0.00000  bestvalidloss -2259.67822  last_update 4\n",
      "train: iter 594  trainloss -2272.87044  validloss -2209.99670±0.00000  bestvalidloss -2259.67822  last_update 5\n",
      "train: iter 595  trainloss -2258.37525  validloss -2170.94542±0.00000  bestvalidloss -2259.67822  last_update 6\n",
      "train: iter 596  trainloss -2285.84112  validloss -2231.13262±0.00000  bestvalidloss -2259.67822  last_update 7\n",
      "train: iter 597  trainloss -2301.60650  validloss -2216.92205±0.00000  bestvalidloss -2259.67822  last_update 8\n",
      "train: iter 598  trainloss -2269.29727  validloss -2195.55449±0.00000  bestvalidloss -2259.67822  last_update 9\n",
      "train: iter 599  trainloss -2300.14655  validloss -2234.16770±0.00000  bestvalidloss -2259.67822  last_update 10\n",
      "train: iter 600  trainloss -2288.69660  validloss -2246.85510±0.00000  bestvalidloss -2259.67822  last_update 11\n",
      "train: iter 601  trainloss -2265.49756  validloss -2203.92770±0.00000  bestvalidloss -2259.67822  last_update 12\n",
      "train: iter 602  trainloss -2282.02018  validloss -2249.46281±0.00000  bestvalidloss -2259.67822  last_update 13\n",
      "train: iter 603  trainloss -2297.75311  validloss -2236.96403±0.00000  bestvalidloss -2259.67822  last_update 14\n",
      "train: iter 604  trainloss -2308.39800  validloss -2255.29153±0.00000  bestvalidloss -2259.67822  last_update 15\n",
      "train: iter 605  trainloss -2289.36266  validloss -2229.50640±0.00000  bestvalidloss -2259.67822  last_update 16\n",
      "train: iter 606  trainloss -2292.83996  validloss -2205.63730±0.00000  bestvalidloss -2259.67822  last_update 17\n",
      "train: iter 607  trainloss -2282.47750  validloss -2125.53556±0.00000  bestvalidloss -2259.67822  last_update 18\n",
      "train: iter 608  trainloss -2300.98277  validloss -2247.57681±0.00000  bestvalidloss -2259.67822  last_update 19\n",
      "train: iter 609  trainloss -2309.81277  validloss -2255.71619±0.00000  bestvalidloss -2259.67822  last_update 20\n",
      "train: iter 610  trainloss -2288.79746  validloss -2249.77688±0.00000  bestvalidloss -2259.67822  last_update 21\n",
      "train: iter 611  trainloss -2296.86359  validloss -2217.18970±0.00000  bestvalidloss -2259.67822  last_update 22\n",
      "train: iter 612  trainloss -2307.37363  validloss -2248.35923±0.00000  bestvalidloss -2259.67822  last_update 23\n",
      "train: iter 613  trainloss -2297.46518  validloss -2260.96693±0.00000  bestvalidloss -2260.96693  last_update 0\n",
      "train: iter 614  trainloss -2263.25163  validloss -2211.24000±0.00000  bestvalidloss -2260.96693  last_update 1\n",
      "train: iter 615  trainloss -2270.34513  validloss -2228.80588±0.00000  bestvalidloss -2260.96693  last_update 2\n",
      "train: iter 616  trainloss -2174.98591  validloss -2212.20539±0.00000  bestvalidloss -2260.96693  last_update 3\n",
      "train: iter 617  trainloss -2275.49722  validloss -1968.79625±0.00000  bestvalidloss -2260.96693  last_update 4\n",
      "train: iter 618  trainloss -2308.13066  validloss -2233.57910±0.00000  bestvalidloss -2260.96693  last_update 5\n",
      "train: iter 619  trainloss -2313.05281  validloss -2238.68971±0.00000  bestvalidloss -2260.96693  last_update 6\n",
      "train: iter 620  trainloss -2308.48845  validloss -2231.82427±0.00000  bestvalidloss -2260.96693  last_update 7\n",
      "train: iter 621  trainloss -2311.48451  validloss -2250.84736±0.00000  bestvalidloss -2260.96693  last_update 8\n",
      "train: iter 622  trainloss -2306.81774  validloss -2239.95173±0.00000  bestvalidloss -2260.96693  last_update 9\n",
      "train: iter 623  trainloss -2294.86415  validloss -2228.32798±0.00000  bestvalidloss -2260.96693  last_update 10\n",
      "train: iter 624  trainloss -2293.88973  validloss -2263.29818±0.00000  bestvalidloss -2263.29818  last_update 0\n",
      "train: iter 625  trainloss -2298.71402  validloss -2244.73765±0.00000  bestvalidloss -2263.29818  last_update 1\n",
      "train: iter 626  trainloss -2301.71067  validloss -2209.93864±0.00000  bestvalidloss -2263.29818  last_update 2\n",
      "train: iter 627  trainloss -2300.50443  validloss -2243.51324±0.00000  bestvalidloss -2263.29818  last_update 3\n",
      "train: iter 628  trainloss -2302.97787  validloss -2245.06965±0.00000  bestvalidloss -2263.29818  last_update 4\n",
      "train: iter 629  trainloss -2302.38106  validloss -2247.52216±0.00000  bestvalidloss -2263.29818  last_update 5\n",
      "train: iter 630  trainloss -2301.09491  validloss -2250.98751±0.00000  bestvalidloss -2263.29818  last_update 6\n",
      "train: iter 631  trainloss -2278.76646  validloss -2245.03474±0.00000  bestvalidloss -2263.29818  last_update 7\n",
      "train: iter 632  trainloss -2306.20524  validloss -2238.81459±0.00000  bestvalidloss -2263.29818  last_update 8\n",
      "train: iter 633  trainloss -2277.62287  validloss -2237.02009±0.00000  bestvalidloss -2263.29818  last_update 9\n",
      "train: iter 634  trainloss -2307.46208  validloss -2230.71634±0.00000  bestvalidloss -2263.29818  last_update 10\n",
      "train: iter 635  trainloss -2256.54749  validloss -2243.42375±0.00000  bestvalidloss -2263.29818  last_update 11\n",
      "train: iter 636  trainloss -2292.06633  validloss -2200.32378±0.00000  bestvalidloss -2263.29818  last_update 12\n",
      "train: iter 637  trainloss -2276.01352  validloss -2176.95083±0.00000  bestvalidloss -2263.29818  last_update 13\n",
      "train: iter 638  trainloss -2308.83696  validloss -2229.13425±0.00000  bestvalidloss -2263.29818  last_update 14\n",
      "train: iter 639  trainloss -2279.75142  validloss -2258.19082±0.00000  bestvalidloss -2263.29818  last_update 15\n",
      "train: iter 640  trainloss -2206.16840  validloss -2169.74408±0.00000  bestvalidloss -2263.29818  last_update 16\n",
      "train: iter 641  trainloss -2315.25296  validloss -2248.05773±0.00000  bestvalidloss -2263.29818  last_update 17\n",
      "train: iter 642  trainloss -2313.67180  validloss -2258.57103±0.00000  bestvalidloss -2263.29818  last_update 18\n",
      "train: iter 643  trainloss -2307.19904  validloss -2235.71539±0.00000  bestvalidloss -2263.29818  last_update 19\n",
      "train: iter 644  trainloss -2307.87823  validloss -2239.05839±0.00000  bestvalidloss -2263.29818  last_update 20\n",
      "train: iter 645  trainloss -2288.69254  validloss -2218.88400±0.00000  bestvalidloss -2263.29818  last_update 21\n",
      "train: iter 646  trainloss -2274.81668  validloss -2260.55082±0.00000  bestvalidloss -2263.29818  last_update 22\n",
      "train: iter 647  trainloss -2294.92251  validloss -2212.04623±0.00000  bestvalidloss -2263.29818  last_update 23\n",
      "train: iter 648  trainloss -2312.23245  validloss -2257.50619±0.00000  bestvalidloss -2263.29818  last_update 24\n",
      "train: iter 649  trainloss -2299.47114  validloss -2256.25267±0.00000  bestvalidloss -2263.29818  last_update 25\n",
      "train: iter 650  trainloss -2279.48068  validloss -2235.63147±0.00000  bestvalidloss -2263.29818  last_update 26\n",
      "train: iter 651  trainloss -2279.33450  validloss -2219.31445±0.00000  bestvalidloss -2263.29818  last_update 27\n",
      "train: iter 652  trainloss -2305.79916  validloss -2246.30944±0.00000  bestvalidloss -2263.29818  last_update 28\n",
      "train: iter 653  trainloss -2310.17041  validloss -2257.29146±0.00000  bestvalidloss -2263.29818  last_update 29\n",
      "train: iter 654  trainloss -2310.40500  validloss -2189.74757±0.00000  bestvalidloss -2263.29818  last_update 30\n",
      "train: iter 655  trainloss -2309.41295  validloss -2261.85255±0.00000  bestvalidloss -2263.29818  last_update 31\n",
      "train: iter 656  trainloss -2297.88469  validloss -2255.48548±0.00000  bestvalidloss -2263.29818  last_update 32\n",
      "train: iter 657  trainloss -2299.04337  validloss -2236.00715±0.00000  bestvalidloss -2263.29818  last_update 33\n",
      "train: iter 658  trainloss -2297.53135  validloss -2229.15276±0.00000  bestvalidloss -2263.29818  last_update 34\n",
      "train: iter 659  trainloss -2293.09084  validloss -2183.13140±0.00000  bestvalidloss -2263.29818  last_update 35\n",
      "train: iter 660  trainloss -2229.20370  validloss -2182.24044±0.00000  bestvalidloss -2263.29818  last_update 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 661  trainloss -2292.41463  validloss -2233.15089±0.00000  bestvalidloss -2263.29818  last_update 37\n",
      "train: iter 662  trainloss -2292.77668  validloss -2216.33522±0.00000  bestvalidloss -2263.29818  last_update 38\n",
      "train: iter 663  trainloss -2297.51449  validloss -2222.36174±0.00000  bestvalidloss -2263.29818  last_update 39\n",
      "train: iter 664  trainloss -2308.30054  validloss -2253.43223±0.00000  bestvalidloss -2263.29818  last_update 40\n",
      "train: iter 665  trainloss -2303.11415  validloss -2242.67482±0.00000  bestvalidloss -2263.29818  last_update 41\n",
      "train: iter 666  trainloss -2307.89516  validloss -2255.64945±0.00000  bestvalidloss -2263.29818  last_update 42\n",
      "train: iter 667  trainloss -2296.04692  validloss -2258.02076±0.00000  bestvalidloss -2263.29818  last_update 43\n",
      "train: iter 668  trainloss -2300.99898  validloss -2253.82955±0.00000  bestvalidloss -2263.29818  last_update 44\n",
      "train: iter 669  trainloss -2305.43027  validloss -2235.95346±0.00000  bestvalidloss -2263.29818  last_update 45\n",
      "train: iter 670  trainloss -2311.06846  validloss -2249.68958±0.00000  bestvalidloss -2263.29818  last_update 46\n",
      "train: iter 671  trainloss -2304.70964  validloss -2230.36115±0.00000  bestvalidloss -2263.29818  last_update 47\n",
      "train: iter 672  trainloss -2300.70011  validloss -2240.47020±0.00000  bestvalidloss -2263.29818  last_update 48\n",
      "train: iter 673  trainloss -2287.25511  validloss -2256.13414±0.00000  bestvalidloss -2263.29818  last_update 49\n",
      "train: iter 674  trainloss -2234.62667  validloss -2235.78938±0.00000  bestvalidloss -2263.29818  last_update 50\n",
      "train: iter 675  trainloss -2252.54575  validloss -2252.43342±0.00000  bestvalidloss -2263.29818  last_update 51\n",
      "train: iter 676  trainloss -2307.04990  validloss -2239.15641±0.00000  bestvalidloss -2263.29818  last_update 52\n",
      "train: iter 677  trainloss -2316.74491  validloss -2261.02718±0.00000  bestvalidloss -2263.29818  last_update 53\n",
      "train: iter 678  trainloss -2315.99962  validloss -2259.69527±0.00000  bestvalidloss -2263.29818  last_update 54\n",
      "train: iter 679  trainloss -2314.55843  validloss -2238.57537±0.00000  bestvalidloss -2263.29818  last_update 55\n",
      "train: iter 680  trainloss -2310.85898  validloss -2265.05477±0.00000  bestvalidloss -2265.05477  last_update 0\n",
      "train: iter 681  trainloss -2308.80893  validloss -2247.62758±0.00000  bestvalidloss -2265.05477  last_update 1\n",
      "train: iter 682  trainloss -2314.53077  validloss -2259.87055±0.00000  bestvalidloss -2265.05477  last_update 2\n",
      "train: iter 683  trainloss -2285.34603  validloss -2243.62050±0.00000  bestvalidloss -2265.05477  last_update 3\n",
      "train: iter 684  trainloss -2297.02618  validloss -2231.09986±0.00000  bestvalidloss -2265.05477  last_update 4\n",
      "train: iter 685  trainloss -2232.01880  validloss -2167.54924±0.00000  bestvalidloss -2265.05477  last_update 5\n",
      "train: iter 686  trainloss -2311.69736  validloss -2253.14603±0.00000  bestvalidloss -2265.05477  last_update 6\n",
      "train: iter 687  trainloss -2316.54597  validloss -2259.01719±0.00000  bestvalidloss -2265.05477  last_update 7\n",
      "train: iter 688  trainloss -2307.21859  validloss -2257.71025±0.00000  bestvalidloss -2265.05477  last_update 8\n",
      "train: iter 689  trainloss -2265.94877  validloss -2178.14648±0.00000  bestvalidloss -2265.05477  last_update 9\n",
      "train: iter 690  trainloss -2306.51520  validloss -2137.18443±0.00000  bestvalidloss -2265.05477  last_update 10\n",
      "train: iter 691  trainloss -2308.59719  validloss -2240.62887±0.00000  bestvalidloss -2265.05477  last_update 11\n",
      "train: iter 692  trainloss -2319.24841  validloss -2266.35806±0.00000  bestvalidloss -2266.35806  last_update 0\n",
      "train: iter 693  trainloss -2303.94863  validloss -2247.46172±0.00000  bestvalidloss -2266.35806  last_update 1\n",
      "train: iter 694  trainloss -2302.74225  validloss -2262.29522±0.00000  bestvalidloss -2266.35806  last_update 2\n",
      "train: iter 695  trainloss -2317.39079  validloss -2246.74214±0.00000  bestvalidloss -2266.35806  last_update 3\n",
      "train: iter 696  trainloss -2322.30605  validloss -2255.08658±0.00000  bestvalidloss -2266.35806  last_update 4\n",
      "train: iter 697  trainloss -2292.32871  validloss -2266.58868±0.00000  bestvalidloss -2266.58868  last_update 0\n",
      "train: iter 698  trainloss -2298.86537  validloss -2198.12813±0.00000  bestvalidloss -2266.58868  last_update 1\n",
      "train: iter 699  trainloss -2315.63763  validloss -2267.77018±0.00000  bestvalidloss -2267.77018  last_update 0\n",
      "train: iter 700  trainloss -2320.78197  validloss -2273.46586±0.00000  bestvalidloss -2273.46586  last_update 0\n",
      "train: iter 701  trainloss -2309.86916  validloss -2252.42443±0.00000  bestvalidloss -2273.46586  last_update 1\n",
      "train: iter 702  trainloss -2306.97987  validloss -2268.32666±0.00000  bestvalidloss -2273.46586  last_update 2\n",
      "train: iter 703  trainloss -2264.43741  validloss -2178.47176±0.00000  bestvalidloss -2273.46586  last_update 3\n",
      "train: iter 704  trainloss -2291.88322  validloss -2213.25170±0.00000  bestvalidloss -2273.46586  last_update 4\n",
      "train: iter 705  trainloss -2263.27453  validloss -2259.78097±0.00000  bestvalidloss -2273.46586  last_update 5\n",
      "train: iter 706  trainloss -2310.17650  validloss -2259.06210±0.00000  bestvalidloss -2273.46586  last_update 6\n",
      "train: iter 707  trainloss -2286.70027  validloss -2264.62437±0.00000  bestvalidloss -2273.46586  last_update 7\n",
      "train: iter 708  trainloss -2300.71157  validloss -2226.19709±0.00000  bestvalidloss -2273.46586  last_update 8\n",
      "train: iter 709  trainloss -2310.22290  validloss -2244.84933±0.00000  bestvalidloss -2273.46586  last_update 9\n",
      "train: iter 710  trainloss -2312.44669  validloss -2260.50929±0.00000  bestvalidloss -2273.46586  last_update 10\n",
      "train: iter 711  trainloss -2304.33081  validloss -2205.00292±0.00000  bestvalidloss -2273.46586  last_update 11\n",
      "train: iter 712  trainloss -2298.59636  validloss -2215.70701±0.00000  bestvalidloss -2273.46586  last_update 12\n",
      "train: iter 713  trainloss -2299.73604  validloss -2236.66524±0.00000  bestvalidloss -2273.46586  last_update 13\n",
      "train: iter 714  trainloss -2261.85375  validloss -2256.21270±0.00000  bestvalidloss -2273.46586  last_update 14\n",
      "train: iter 715  trainloss -2294.19957  validloss -2150.82655±0.00000  bestvalidloss -2273.46586  last_update 15\n",
      "train: iter 716  trainloss -2304.41452  validloss -2242.28992±0.00000  bestvalidloss -2273.46586  last_update 16\n",
      "train: iter 717  trainloss -2316.14121  validloss -2263.65256±0.00000  bestvalidloss -2273.46586  last_update 17\n",
      "train: iter 718  trainloss -2304.23829  validloss -2256.81234±0.00000  bestvalidloss -2273.46586  last_update 18\n",
      "train: iter 719  trainloss -2300.31618  validloss -2231.25809±0.00000  bestvalidloss -2273.46586  last_update 19\n",
      "train: iter 720  trainloss -2309.59524  validloss -2260.67340±0.00000  bestvalidloss -2273.46586  last_update 20\n",
      "train: iter 721  trainloss -2320.07023  validloss -2275.56471±0.00000  bestvalidloss -2275.56471  last_update 0\n",
      "train: iter 722  trainloss -2305.59094  validloss -2267.57493±0.00000  bestvalidloss -2275.56471  last_update 1\n",
      "train: iter 723  trainloss -2289.48872  validloss -2258.38833±0.00000  bestvalidloss -2275.56471  last_update 2\n",
      "train: iter 724  trainloss -2301.49229  validloss -2208.38013±0.00000  bestvalidloss -2275.56471  last_update 3\n",
      "train: iter 725  trainloss -2313.50732  validloss -2260.77502±0.00000  bestvalidloss -2275.56471  last_update 4\n",
      "train: iter 726  trainloss -2309.25154  validloss -2200.97976±0.00000  bestvalidloss -2275.56471  last_update 5\n",
      "train: iter 727  trainloss -2310.18856  validloss -2257.66562±0.00000  bestvalidloss -2275.56471  last_update 6\n",
      "train: iter 728  trainloss -2310.22648  validloss -2261.46396±0.00000  bestvalidloss -2275.56471  last_update 7\n",
      "train: iter 729  trainloss -2289.84942  validloss -2240.38010±0.00000  bestvalidloss -2275.56471  last_update 8\n",
      "train: iter 730  trainloss -2185.88334  validloss -2216.55491±0.00000  bestvalidloss -2275.56471  last_update 9\n",
      "train: iter 731  trainloss -2185.99657  validloss -2039.41769±0.00000  bestvalidloss -2275.56471  last_update 10\n",
      "train: iter 732  trainloss -2299.94354  validloss -2243.26576±0.00000  bestvalidloss -2275.56471  last_update 11\n",
      "train: iter 733  trainloss -2315.87527  validloss -2226.18537±0.00000  bestvalidloss -2275.56471  last_update 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 734  trainloss -2309.84663  validloss -2246.50851±0.00000  bestvalidloss -2275.56471  last_update 13\n",
      "train: iter 735  trainloss -2314.47554  validloss -2258.62535±0.00000  bestvalidloss -2275.56471  last_update 14\n",
      "train: iter 736  trainloss -2309.53346  validloss -2253.80198±0.00000  bestvalidloss -2275.56471  last_update 15\n",
      "train: iter 737  trainloss -2302.80245  validloss -2252.04634±0.00000  bestvalidloss -2275.56471  last_update 16\n",
      "train: iter 738  trainloss -2314.91455  validloss -2262.62790±0.00000  bestvalidloss -2275.56471  last_update 17\n",
      "train: iter 739  trainloss -2316.84715  validloss -2235.52340±0.00000  bestvalidloss -2275.56471  last_update 18\n",
      "train: iter 740  trainloss -2322.57838  validloss -2258.58354±0.00000  bestvalidloss -2275.56471  last_update 19\n",
      "train: iter 741  trainloss -2321.93754  validloss -2228.51502±0.00000  bestvalidloss -2275.56471  last_update 20\n",
      "train: iter 742  trainloss -2316.14052  validloss -2240.82454±0.00000  bestvalidloss -2275.56471  last_update 21\n",
      "train: iter 743  trainloss -2314.62751  validloss -2235.17889±0.00000  bestvalidloss -2275.56471  last_update 22\n",
      "train: iter 744  trainloss -2324.28064  validloss -2276.11749±0.00000  bestvalidloss -2276.11749  last_update 0\n",
      "train: iter 745  trainloss -2323.18857  validloss -2260.24264±0.00000  bestvalidloss -2276.11749  last_update 1\n",
      "train: iter 746  trainloss -2307.03297  validloss -2261.08828±0.00000  bestvalidloss -2276.11749  last_update 2\n",
      "train: iter 747  trainloss -2307.78701  validloss -2243.67792±0.00000  bestvalidloss -2276.11749  last_update 3\n",
      "train: iter 748  trainloss -2283.05553  validloss -2253.75057±0.00000  bestvalidloss -2276.11749  last_update 4\n",
      "train: iter 749  trainloss -2310.61803  validloss -2236.08810±0.00000  bestvalidloss -2276.11749  last_update 5\n",
      "train: iter 750  trainloss -2300.54887  validloss -2256.14837±0.00000  bestvalidloss -2276.11749  last_update 6\n",
      "train: iter 751  trainloss -2248.05625  validloss -2243.18088±0.00000  bestvalidloss -2276.11749  last_update 7\n",
      "train: iter 752  trainloss -2255.22636  validloss -2051.47684±0.00000  bestvalidloss -2276.11749  last_update 8\n",
      "train: iter 753  trainloss -2317.82508  validloss -2258.09728±0.00000  bestvalidloss -2276.11749  last_update 9\n",
      "train: iter 754  trainloss -2320.83866  validloss -2263.89673±0.00000  bestvalidloss -2276.11749  last_update 10\n",
      "train: iter 755  trainloss -2321.27036  validloss -2272.97518±0.00000  bestvalidloss -2276.11749  last_update 11\n",
      "train: iter 756  trainloss -2318.24002  validloss -2264.80491±0.00000  bestvalidloss -2276.11749  last_update 12\n",
      "train: iter 757  trainloss -2314.27306  validloss -2269.89501±0.00000  bestvalidloss -2276.11749  last_update 13\n",
      "train: iter 758  trainloss -2317.72915  validloss -2265.64457±0.00000  bestvalidloss -2276.11749  last_update 14\n",
      "train: iter 759  trainloss -2308.29948  validloss -2258.42328±0.00000  bestvalidloss -2276.11749  last_update 15\n",
      "train: iter 760  trainloss -2320.79991  validloss -2252.86207±0.00000  bestvalidloss -2276.11749  last_update 16\n",
      "train: iter 761  trainloss -2315.64857  validloss -2277.66370±0.00000  bestvalidloss -2277.66370  last_update 0\n",
      "train: iter 762  trainloss -2288.96617  validloss -2241.16483±0.00000  bestvalidloss -2277.66370  last_update 1\n",
      "train: iter 763  trainloss -2315.32640  validloss -2272.75679±0.00000  bestvalidloss -2277.66370  last_update 2\n",
      "train: iter 764  trainloss -2309.71691  validloss -2264.29524±0.00000  bestvalidloss -2277.66370  last_update 3\n",
      "train: iter 765  trainloss -2276.06409  validloss -2240.26831±0.00000  bestvalidloss -2277.66370  last_update 4\n",
      "train: iter 766  trainloss -2288.15912  validloss -2052.63521±0.00000  bestvalidloss -2277.66370  last_update 5\n",
      "train: iter 767  trainloss -2282.78163  validloss -2252.52482±0.00000  bestvalidloss -2277.66370  last_update 6\n",
      "train: iter 768  trainloss -2271.71311  validloss -2214.99312±0.00000  bestvalidloss -2277.66370  last_update 7\n",
      "train: iter 769  trainloss -2315.24041  validloss -2242.12482±0.00000  bestvalidloss -2277.66370  last_update 8\n",
      "train: iter 770  trainloss -2322.27358  validloss -2256.55159±0.00000  bestvalidloss -2277.66370  last_update 9\n",
      "train: iter 771  trainloss -2304.74559  validloss -2265.32097±0.00000  bestvalidloss -2277.66370  last_update 10\n",
      "train: iter 772  trainloss -2316.90119  validloss -2233.53028±0.00000  bestvalidloss -2277.66370  last_update 11\n",
      "train: iter 773  trainloss -2300.35386  validloss -2258.11548±0.00000  bestvalidloss -2277.66370  last_update 12\n",
      "train: iter 774  trainloss -2300.56125  validloss -2244.10574±0.00000  bestvalidloss -2277.66370  last_update 13\n",
      "train: iter 775  trainloss -2310.81859  validloss -2241.40284±0.00000  bestvalidloss -2277.66370  last_update 14\n",
      "train: iter 776  trainloss -2308.42592  validloss -2222.90205±0.00000  bestvalidloss -2277.66370  last_update 15\n",
      "train: iter 777  trainloss -2320.23323  validloss -2270.02264±0.00000  bestvalidloss -2277.66370  last_update 16\n",
      "train: iter 778  trainloss -2317.41680  validloss -2256.04466±0.00000  bestvalidloss -2277.66370  last_update 17\n",
      "train: iter 779  trainloss -2302.52760  validloss -2250.27042±0.00000  bestvalidloss -2277.66370  last_update 18\n",
      "train: iter 780  trainloss -2311.41362  validloss -2226.46503±0.00000  bestvalidloss -2277.66370  last_update 19\n",
      "train: iter 781  trainloss -2244.37620  validloss -2252.29420±0.00000  bestvalidloss -2277.66370  last_update 20\n",
      "train: iter 782  trainloss -2303.05952  validloss -2185.48944±0.00000  bestvalidloss -2277.66370  last_update 21\n",
      "train: iter 783  trainloss -2319.88039  validloss -2248.52015±0.00000  bestvalidloss -2277.66370  last_update 22\n",
      "train: iter 784  trainloss -2312.29664  validloss -2225.48510±0.00000  bestvalidloss -2277.66370  last_update 23\n",
      "train: iter 785  trainloss -2294.24461  validloss -2253.10491±0.00000  bestvalidloss -2277.66370  last_update 24\n",
      "train: iter 786  trainloss -2313.76907  validloss -2250.32189±0.00000  bestvalidloss -2277.66370  last_update 25\n",
      "train: iter 787  trainloss -2303.89888  validloss -2273.17092±0.00000  bestvalidloss -2277.66370  last_update 26\n",
      "train: iter 788  trainloss -2319.41216  validloss -2255.02183±0.00000  bestvalidloss -2277.66370  last_update 27\n",
      "train: iter 789  trainloss -2307.67891  validloss -2212.07365±0.00000  bestvalidloss -2277.66370  last_update 28\n",
      "train: iter 790  trainloss -2266.40520  validloss -2237.72950±0.00000  bestvalidloss -2277.66370  last_update 29\n",
      "train: iter 791  trainloss -2282.30940  validloss -2161.12628±0.00000  bestvalidloss -2277.66370  last_update 30\n",
      "train: iter 792  trainloss -2309.84523  validloss -2241.42989±0.00000  bestvalidloss -2277.66370  last_update 31\n",
      "train: iter 793  trainloss -2322.22131  validloss -2272.88947±0.00000  bestvalidloss -2277.66370  last_update 32\n",
      "train: iter 794  trainloss -2328.95176  validloss -2266.09195±0.00000  bestvalidloss -2277.66370  last_update 33\n",
      "train: iter 795  trainloss -2319.98735  validloss -2246.53198±0.00000  bestvalidloss -2277.66370  last_update 34\n",
      "train: iter 796  trainloss -2318.17317  validloss -2262.69591±0.00000  bestvalidloss -2277.66370  last_update 35\n",
      "train: iter 797  trainloss -2316.30986  validloss -2279.88848±0.00000  bestvalidloss -2279.88848  last_update 0\n",
      "train: iter 798  trainloss -2315.44846  validloss -2256.66946±0.00000  bestvalidloss -2279.88848  last_update 1\n",
      "train: iter 799  trainloss -2323.14383  validloss -2241.02616±0.00000  bestvalidloss -2279.88848  last_update 2\n",
      "train: iter 800  trainloss -2314.18944  validloss -2251.59404±0.00000  bestvalidloss -2279.88848  last_update 3\n",
      "train: iter 801  trainloss -2314.26498  validloss -2251.79016±0.00000  bestvalidloss -2279.88848  last_update 4\n",
      "train: iter 802  trainloss -2319.64271  validloss -2261.55687±0.00000  bestvalidloss -2279.88848  last_update 5\n",
      "train: iter 803  trainloss -2294.77798  validloss -2229.03680±0.00000  bestvalidloss -2279.88848  last_update 6\n",
      "train: iter 804  trainloss -2294.32920  validloss -2273.95588±0.00000  bestvalidloss -2279.88848  last_update 7\n",
      "train: iter 805  trainloss -2111.00441  validloss -2212.19980±0.00000  bestvalidloss -2279.88848  last_update 8\n",
      "train: iter 806  trainloss -2248.27378  validloss -2066.82632±0.00000  bestvalidloss -2279.88848  last_update 9\n",
      "train: iter 807  trainloss -2321.37040  validloss -2252.62584±0.00000  bestvalidloss -2279.88848  last_update 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 808  trainloss -2312.97566  validloss -2266.11080±0.00000  bestvalidloss -2279.88848  last_update 11\n",
      "train: iter 809  trainloss -2320.49764  validloss -2261.45590±0.00000  bestvalidloss -2279.88848  last_update 12\n",
      "train: iter 810  trainloss -2316.70315  validloss -2258.08779±0.00000  bestvalidloss -2279.88848  last_update 13\n",
      "train: iter 811  trainloss -2312.50618  validloss -2193.89586±0.00000  bestvalidloss -2279.88848  last_update 14\n",
      "train: iter 812  trainloss -2326.78571  validloss -2269.86644±0.00000  bestvalidloss -2279.88848  last_update 15\n",
      "train: iter 813  trainloss -2290.01069  validloss -2222.80396±0.00000  bestvalidloss -2279.88848  last_update 16\n",
      "train: iter 814  trainloss -2310.39505  validloss -2210.70969±0.00000  bestvalidloss -2279.88848  last_update 17\n",
      "train: iter 815  trainloss -2326.44400  validloss -2263.52368±0.00000  bestvalidloss -2279.88848  last_update 18\n",
      "train: iter 816  trainloss -2306.96939  validloss -2256.04324±0.00000  bestvalidloss -2279.88848  last_update 19\n",
      "train: iter 817  trainloss -2291.74506  validloss -2207.91716±0.00000  bestvalidloss -2279.88848  last_update 20\n",
      "train: iter 818  trainloss -2198.83487  validloss -2257.17613±0.00000  bestvalidloss -2279.88848  last_update 21\n",
      "train: iter 819  trainloss -2313.15727  validloss -2171.25812±0.00000  bestvalidloss -2279.88848  last_update 22\n",
      "train: iter 820  trainloss -2326.33324  validloss -2276.92504±0.00000  bestvalidloss -2279.88848  last_update 23\n",
      "train: iter 821  trainloss -2327.61710  validloss -2267.81074±0.00000  bestvalidloss -2279.88848  last_update 24\n",
      "train: iter 822  trainloss -2324.36617  validloss -2272.13428±0.00000  bestvalidloss -2279.88848  last_update 25\n",
      "train: iter 823  trainloss -2310.94063  validloss -2264.49913±0.00000  bestvalidloss -2279.88848  last_update 26\n",
      "train: iter 824  trainloss -2309.49946  validloss -2261.69032±0.00000  bestvalidloss -2279.88848  last_update 27\n",
      "train: iter 825  trainloss -2327.20188  validloss -2265.14518±0.00000  bestvalidloss -2279.88848  last_update 28\n",
      "train: iter 826  trainloss -2298.48783  validloss -2275.78022±0.00000  bestvalidloss -2279.88848  last_update 29\n",
      "train: iter 827  trainloss -2305.35059  validloss -2138.16215±0.00000  bestvalidloss -2279.88848  last_update 30\n",
      "train: iter 828  trainloss -2318.66052  validloss -2200.77989±0.00000  bestvalidloss -2279.88848  last_update 31\n",
      "train: iter 829  trainloss -2309.07067  validloss -2229.86633±0.00000  bestvalidloss -2279.88848  last_update 32\n",
      "train: iter 830  trainloss -2320.00139  validloss -2257.53245±0.00000  bestvalidloss -2279.88848  last_update 33\n",
      "train: iter 831  trainloss -2311.04458  validloss -2248.75221±0.00000  bestvalidloss -2279.88848  last_update 34\n",
      "train: iter 832  trainloss -2320.74843  validloss -2266.60928±0.00000  bestvalidloss -2279.88848  last_update 35\n",
      "train: iter 833  trainloss -2308.90947  validloss -2241.56011±0.00000  bestvalidloss -2279.88848  last_update 36\n",
      "train: iter 834  trainloss -2309.00634  validloss -2255.88163±0.00000  bestvalidloss -2279.88848  last_update 37\n",
      "train: iter 835  trainloss -2262.41742  validloss -2223.93249±0.00000  bestvalidloss -2279.88848  last_update 38\n",
      "train: iter 836  trainloss -2320.60118  validloss -2218.85598±0.00000  bestvalidloss -2279.88848  last_update 39\n",
      "train: iter 837  trainloss -2326.63906  validloss -2253.18495±0.00000  bestvalidloss -2279.88848  last_update 40\n",
      "train: iter 838  trainloss -2326.67195  validloss -2265.45585±0.00000  bestvalidloss -2279.88848  last_update 41\n",
      "train: iter 839  trainloss -2309.96366  validloss -2273.29853±0.00000  bestvalidloss -2279.88848  last_update 42\n",
      "train: iter 840  trainloss -2316.14272  validloss -2229.28193±0.00000  bestvalidloss -2279.88848  last_update 43\n",
      "train: iter 841  trainloss -2320.42849  validloss -2250.19376±0.00000  bestvalidloss -2279.88848  last_update 44\n",
      "train: iter 842  trainloss -2306.21852  validloss -2211.91769±0.00000  bestvalidloss -2279.88848  last_update 45\n",
      "train: iter 843  trainloss -2308.14552  validloss -2199.58811±0.00000  bestvalidloss -2279.88848  last_update 46\n",
      "train: iter 844  trainloss -2284.68746  validloss -2240.84008±0.00000  bestvalidloss -2279.88848  last_update 47\n",
      "train: iter 845  trainloss -2314.74555  validloss -2246.24844±0.00000  bestvalidloss -2279.88848  last_update 48\n",
      "train: iter 846  trainloss -2322.25184  validloss -2276.64427±0.00000  bestvalidloss -2279.88848  last_update 49\n",
      "train: iter 847  trainloss -2312.22439  validloss -2241.20453±0.00000  bestvalidloss -2279.88848  last_update 50\n",
      "train: iter 848  trainloss -2323.48658  validloss -2254.53254±0.00000  bestvalidloss -2279.88848  last_update 51\n",
      "train: iter 849  trainloss -2313.61007  validloss -2260.83093±0.00000  bestvalidloss -2279.88848  last_update 52\n",
      "train: iter 850  trainloss -2296.82304  validloss -2249.34773±0.00000  bestvalidloss -2279.88848  last_update 53\n",
      "train: iter 851  trainloss -2312.97052  validloss -2253.83720±0.00000  bestvalidloss -2279.88848  last_update 54\n",
      "train: iter 852  trainloss -2317.94959  validloss -2236.85048±0.00000  bestvalidloss -2279.88848  last_update 55\n",
      "train: iter 853  trainloss -2315.15885  validloss -2262.75419±0.00000  bestvalidloss -2279.88848  last_update 56\n",
      "train: iter 854  trainloss -2289.97708  validloss -2254.59346±0.00000  bestvalidloss -2279.88848  last_update 57\n",
      "train: iter 855  trainloss -2255.89600  validloss -2177.21314±0.00000  bestvalidloss -2279.88848  last_update 58\n",
      "train: iter 856  trainloss -2298.12715  validloss -2176.05239±0.00000  bestvalidloss -2279.88848  last_update 59\n",
      "train: iter 857  trainloss -2324.20406  validloss -2248.26249±0.00000  bestvalidloss -2279.88848  last_update 60\n",
      "train: iter 858  trainloss -2306.12993  validloss -2262.82424±0.00000  bestvalidloss -2279.88848  last_update 61\n",
      "train: iter 859  trainloss -2315.85258  validloss -2231.38890±0.00000  bestvalidloss -2279.88848  last_update 62\n",
      "train: iter 860  trainloss -2311.37894  validloss -2251.38799±0.00000  bestvalidloss -2279.88848  last_update 63\n",
      "train: iter 861  trainloss -2316.60449  validloss -2271.46980±0.00000  bestvalidloss -2279.88848  last_update 64\n",
      "train: iter 862  trainloss -2328.93969  validloss -2257.68927±0.00000  bestvalidloss -2279.88848  last_update 65\n",
      "train: iter 863  trainloss -2314.58879  validloss -2252.43162±0.00000  bestvalidloss -2279.88848  last_update 66\n",
      "train: iter 864  trainloss -2314.20137  validloss -2275.70119±0.00000  bestvalidloss -2279.88848  last_update 67\n",
      "train: iter 865  trainloss -2314.65495  validloss -2269.70991±0.00000  bestvalidloss -2279.88848  last_update 68\n",
      "train: iter 866  trainloss -2307.95703  validloss -2231.09608±0.00000  bestvalidloss -2279.88848  last_update 69\n",
      "train: iter 867  trainloss -2282.67426  validloss -2127.66846±0.00000  bestvalidloss -2279.88848  last_update 70\n",
      "train: iter 868  trainloss -2311.23004  validloss -2265.25161±0.00000  bestvalidloss -2279.88848  last_update 71\n",
      "train: iter 869  trainloss -2312.03353  validloss -2242.75432±0.00000  bestvalidloss -2279.88848  last_update 72\n",
      "train: iter 870  trainloss -2309.13100  validloss -2200.90787±0.00000  bestvalidloss -2279.88848  last_update 73\n",
      "train: iter 871  trainloss -2302.90904  validloss -2244.85847±0.00000  bestvalidloss -2279.88848  last_update 74\n",
      "train: iter 872  trainloss -2316.48289  validloss -2237.67591±0.00000  bestvalidloss -2279.88848  last_update 75\n",
      "train: iter 873  trainloss -2317.85519  validloss -2260.98126±0.00000  bestvalidloss -2279.88848  last_update 76\n",
      "train: iter 874  trainloss -2312.29834  validloss -2262.82219±0.00000  bestvalidloss -2279.88848  last_update 77\n",
      "train: iter 875  trainloss -2325.31702  validloss -2259.80419±0.00000  bestvalidloss -2279.88848  last_update 78\n",
      "train: iter 876  trainloss -2327.68867  validloss -2264.93021±0.00000  bestvalidloss -2279.88848  last_update 79\n",
      "train: iter 877  trainloss -2302.88869  validloss -2259.82847±0.00000  bestvalidloss -2279.88848  last_update 80\n",
      "train: iter 878  trainloss -2242.44239  validloss -2136.69177±0.00000  bestvalidloss -2279.88848  last_update 81\n",
      "train: iter 879  trainloss -2294.11443  validloss -2218.92094±0.00000  bestvalidloss -2279.88848  last_update 82\n",
      "train: iter 880  trainloss -2280.63499  validloss -2208.68568±0.00000  bestvalidloss -2279.88848  last_update 83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 881  trainloss -2322.06402  validloss -2253.77129±0.00000  bestvalidloss -2279.88848  last_update 84\n",
      "train: iter 882  trainloss -2319.03902  validloss -2245.17388±0.00000  bestvalidloss -2279.88848  last_update 85\n",
      "train: iter 883  trainloss -2319.76195  validloss -2248.92714±0.00000  bestvalidloss -2279.88848  last_update 86\n",
      "train: iter 884  trainloss -2315.76591  validloss -2258.75654±0.00000  bestvalidloss -2279.88848  last_update 87\n",
      "train: iter 885  trainloss -2320.61175  validloss -2253.02159±0.00000  bestvalidloss -2279.88848  last_update 88\n",
      "train: iter 886  trainloss -2308.27278  validloss -2210.74432±0.00000  bestvalidloss -2279.88848  last_update 89\n",
      "train: iter 887  trainloss -2325.28435  validloss -2255.07899±0.00000  bestvalidloss -2279.88848  last_update 90\n",
      "train: iter 888  trainloss -2318.65527  validloss -2250.04722±0.00000  bestvalidloss -2279.88848  last_update 91\n",
      "train: iter 889  trainloss -2325.84661  validloss -2248.13403±0.00000  bestvalidloss -2279.88848  last_update 92\n",
      "train: iter 890  trainloss -2316.04138  validloss -2229.37606±0.00000  bestvalidloss -2279.88848  last_update 93\n",
      "train: iter 891  trainloss -2304.76153  validloss -2237.75542±0.00000  bestvalidloss -2279.88848  last_update 94\n",
      "train: iter 892  trainloss -2297.27522  validloss -2222.56427±0.00000  bestvalidloss -2279.88848  last_update 95\n",
      "train: iter 893  trainloss -2328.14634  validloss -2269.76418±0.00000  bestvalidloss -2279.88848  last_update 96\n",
      "train: iter 894  trainloss -2324.84274  validloss -2260.97623±0.00000  bestvalidloss -2279.88848  last_update 97\n",
      "train: iter 895  trainloss -2316.30720  validloss -2249.80731±0.00000  bestvalidloss -2279.88848  last_update 98\n",
      "train: iter 896  trainloss -2313.61481  validloss -2247.18319±0.00000  bestvalidloss -2279.88848  last_update 99\n",
      "train: iter 897  trainloss -2286.79450  validloss -2269.59694±0.00000  bestvalidloss -2279.88848  last_update 100\n",
      "train: fin\n",
      "penalty_target_min tensor(-12.3240) penalty_target_max tensor(-2.0070)\n"
     ]
    }
   ],
   "source": [
    "train_curve1, valid_curve1 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=1)\n",
    "vi.update_mulogvar_offlinedata()\n",
    "\n",
    "# train_curve2, valid_curve2 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=2)\n",
    "# vi.update_mulogvar_offlinedata()\n",
    "\n",
    "# train_curve3, valid_curve3 = vi.train_unweighted_vae(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early, flag=3)\n",
    "# vi.update_mulogvar_offlinedata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c548788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_curve1)\n",
    "plt.plot(valid_curve1)\n",
    "# plt.ylim([-2000, 1000])\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(train_curve2)\n",
    "# plt.plot(valid_curve2)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_curve3)\n",
    "# plt.plot(valid_curve3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2656b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd778499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 0  trainloss 5.20550  validloss 5.43600±0.00000  bestvalidloss 5.43600  last_update 0\n",
      "train: iter 1  trainloss 4.80454  validloss 4.99786±0.00000  bestvalidloss 4.99786  last_update 0\n",
      "train: iter 2  trainloss 4.47717  validloss 4.61749±0.00000  bestvalidloss 4.61749  last_update 0\n",
      "train: iter 3  trainloss 4.21239  validloss 4.33713±0.00000  bestvalidloss 4.33713  last_update 0\n",
      "train: iter 4  trainloss 3.99326  validloss 4.08973±0.00000  bestvalidloss 4.08973  last_update 0\n",
      "train: iter 5  trainloss 3.78917  validloss 3.88720±0.00000  bestvalidloss 3.88720  last_update 0\n",
      "train: iter 6  trainloss 3.60528  validloss 3.69840±0.00000  bestvalidloss 3.69840  last_update 0\n",
      "train: iter 7  trainloss 3.44461  validloss 3.52545±0.00000  bestvalidloss 3.52545  last_update 0\n",
      "train: iter 8  trainloss 3.29045  validloss 3.35305±0.00000  bestvalidloss 3.35305  last_update 0\n",
      "train: iter 9  trainloss 3.13861  validloss 3.20937±0.00000  bestvalidloss 3.20937  last_update 0\n",
      "train: iter 10  trainloss 2.99172  validloss 3.05924±0.00000  bestvalidloss 3.05924  last_update 0\n",
      "train: iter 11  trainloss 2.85060  validloss 2.92774±0.00000  bestvalidloss 2.92774  last_update 0\n",
      "train: iter 12  trainloss 2.70864  validloss 2.77541±0.00000  bestvalidloss 2.77541  last_update 0\n",
      "train: iter 13  trainloss 2.57911  validloss 2.63810±0.00000  bestvalidloss 2.63810  last_update 0\n",
      "train: iter 14  trainloss 2.44991  validloss 2.50547±0.00000  bestvalidloss 2.50547  last_update 0\n",
      "train: iter 15  trainloss 2.32636  validloss 2.38183±0.00000  bestvalidloss 2.38183  last_update 0\n",
      "train: iter 16  trainloss 2.20932  validloss 2.26013±0.00000  bestvalidloss 2.26013  last_update 0\n",
      "train: iter 17  trainloss 2.10085  validloss 2.15319±0.00000  bestvalidloss 2.15319  last_update 0\n",
      "train: iter 18  trainloss 1.99780  validloss 2.03563±0.00000  bestvalidloss 2.03563  last_update 0\n",
      "train: iter 19  trainloss 1.90454  validloss 1.94720±0.00000  bestvalidloss 1.94720  last_update 0\n",
      "train: iter 20  trainloss 1.81609  validloss 1.84858±0.00000  bestvalidloss 1.84858  last_update 0\n",
      "train: iter 21  trainloss 1.73051  validloss 1.75947±0.00000  bestvalidloss 1.75947  last_update 0\n",
      "train: iter 22  trainloss 1.64710  validloss 1.67842±0.00000  bestvalidloss 1.67842  last_update 0\n",
      "train: iter 23  trainloss 1.56523  validloss 1.59736±0.00000  bestvalidloss 1.59736  last_update 0\n",
      "train: iter 24  trainloss 1.48704  validloss 1.51157±0.00000  bestvalidloss 1.51157  last_update 0\n",
      "train: iter 25  trainloss 1.40414  validloss 1.42634±0.00000  bestvalidloss 1.42634  last_update 0\n",
      "train: iter 26  trainloss 1.32441  validloss 1.34835±0.00000  bestvalidloss 1.34835  last_update 0\n",
      "train: iter 27  trainloss 1.23915  validloss 1.26773±0.00000  bestvalidloss 1.26773  last_update 0\n",
      "train: iter 28  trainloss 1.15680  validloss 1.18268±0.00000  bestvalidloss 1.18268  last_update 0\n",
      "train: iter 29  trainloss 1.07108  validloss 1.09534±0.00000  bestvalidloss 1.09534  last_update 0\n",
      "train: iter 30  trainloss 0.99027  validloss 0.99613±0.00000  bestvalidloss 0.99613  last_update 0\n",
      "train: iter 31  trainloss 0.90185  validloss 0.91892±0.00000  bestvalidloss 0.91892  last_update 0\n",
      "train: iter 32  trainloss 0.82069  validloss 0.82920±0.00000  bestvalidloss 0.82920  last_update 0\n",
      "train: iter 33  trainloss 0.73432  validloss 0.73693±0.00000  bestvalidloss 0.73693  last_update 0\n",
      "train: iter 34  trainloss 0.64935  validloss 0.65358±0.00000  bestvalidloss 0.65358  last_update 0\n",
      "train: iter 35  trainloss 0.55560  validloss 0.55684±0.00000  bestvalidloss 0.55684  last_update 0\n",
      "train: iter 36  trainloss 0.46901  validloss 0.47981±0.00000  bestvalidloss 0.47981  last_update 0\n",
      "train: iter 37  trainloss 0.38856  validloss 0.38967±0.00000  bestvalidloss 0.38967  last_update 0\n",
      "train: iter 38  trainloss 0.30577  validloss 0.29119±0.00000  bestvalidloss 0.29119  last_update 0\n",
      "train: iter 39  trainloss 0.22546  validloss 0.20240±0.00000  bestvalidloss 0.20240  last_update 0\n",
      "train: iter 40  trainloss 0.14597  validloss 0.12996±0.00000  bestvalidloss 0.12996  last_update 0\n",
      "train: iter 41  trainloss 0.07177  validloss 0.03529±0.00000  bestvalidloss 0.03529  last_update 0\n",
      "train: iter 42  trainloss 0.00505  validloss -0.04237±0.00000  bestvalidloss -0.04237  last_update 0\n",
      "train: iter 43  trainloss -0.07764  validloss -0.10380±0.00000  bestvalidloss -0.10380  last_update 0\n",
      "train: iter 44  trainloss -0.14054  validloss -0.19300±0.00000  bestvalidloss -0.19300  last_update 0\n",
      "train: iter 45  trainloss -0.18634  validloss -0.24808±0.00000  bestvalidloss -0.24808  last_update 0\n",
      "train: iter 46  trainloss -0.26486  validloss -0.31414±0.00000  bestvalidloss -0.31414  last_update 0\n",
      "train: iter 47  trainloss -0.32909  validloss -0.38094±0.00000  bestvalidloss -0.38094  last_update 0\n",
      "train: iter 48  trainloss -0.35676  validloss -0.44703±0.00000  bestvalidloss -0.44703  last_update 0\n",
      "train: iter 49  trainloss -0.43930  validloss -0.49590±0.00000  bestvalidloss -0.49590  last_update 0\n",
      "train: iter 50  trainloss -0.48793  validloss -0.56443±0.00000  bestvalidloss -0.56443  last_update 0\n",
      "train: iter 51  trainloss -0.54089  validloss -0.62103±0.00000  bestvalidloss -0.62103  last_update 0\n",
      "train: iter 52  trainloss -0.59214  validloss -0.66034±0.00000  bestvalidloss -0.66034  last_update 0\n",
      "train: iter 53  trainloss -0.64911  validloss -0.73603±0.00000  bestvalidloss -0.73603  last_update 0\n",
      "train: iter 54  trainloss -0.70125  validloss -0.80394±0.00000  bestvalidloss -0.80394  last_update 0\n",
      "train: iter 55  trainloss -0.71307  validloss -0.85284±0.00000  bestvalidloss -0.85284  last_update 0\n",
      "train: iter 56  trainloss -0.77459  validloss -0.89550±0.00000  bestvalidloss -0.89550  last_update 0\n",
      "train: iter 57  trainloss -0.81543  validloss -0.94917±0.00000  bestvalidloss -0.94917  last_update 0\n",
      "train: iter 58  trainloss -0.86892  validloss -1.01041±0.00000  bestvalidloss -1.01041  last_update 0\n",
      "train: iter 59  trainloss -0.91847  validloss -1.06221±0.00000  bestvalidloss -1.06221  last_update 0\n",
      "train: iter 60  trainloss -0.96036  validloss -1.10720±0.00000  bestvalidloss -1.10720  last_update 0\n",
      "train: iter 61  trainloss -0.96889  validloss -1.11115±0.00000  bestvalidloss -1.11115  last_update 0\n",
      "train: iter 62  trainloss -1.01880  validloss -1.20564±0.00000  bestvalidloss -1.20564  last_update 0\n",
      "train: iter 63  trainloss -1.08308  validloss -1.19507±0.00000  bestvalidloss -1.20564  last_update 1\n",
      "train: iter 64  trainloss -1.11344  validloss -1.29462±0.00000  bestvalidloss -1.29462  last_update 0\n",
      "train: iter 65  trainloss -1.14765  validloss -1.35027±0.00000  bestvalidloss -1.35027  last_update 0\n",
      "train: iter 66  trainloss -1.14885  validloss -1.35197±0.00000  bestvalidloss -1.35197  last_update 0\n",
      "train: iter 67  trainloss -1.17769  validloss -1.37856±0.00000  bestvalidloss -1.37856  last_update 0\n",
      "train: iter 68  trainloss -1.23204  validloss -1.44663±0.00000  bestvalidloss -1.44663  last_update 0\n",
      "train: iter 69  trainloss -1.28005  validloss -1.51039±0.00000  bestvalidloss -1.51039  last_update 0\n",
      "train: iter 70  trainloss -1.28526  validloss -1.51990±0.00000  bestvalidloss -1.51990  last_update 0\n",
      "train: iter 71  trainloss -1.32712  validloss -1.56611±0.00000  bestvalidloss -1.56611  last_update 0\n",
      "train: iter 72  trainloss -1.34550  validloss -1.59193±0.00000  bestvalidloss -1.59193  last_update 0\n",
      "train: iter 73  trainloss -1.37988  validloss -1.60834±0.00000  bestvalidloss -1.60834  last_update 0\n",
      "train: iter 74  trainloss -1.43950  validloss -1.65135±0.00000  bestvalidloss -1.65135  last_update 0\n",
      "train: iter 75  trainloss -1.43937  validloss -1.70132±0.00000  bestvalidloss -1.70132  last_update 0\n",
      "train: iter 76  trainloss -1.46994  validloss -1.72011±0.00000  bestvalidloss -1.72011  last_update 0\n",
      "train: iter 77  trainloss -1.51344  validloss -1.77283±0.00000  bestvalidloss -1.77283  last_update 0\n",
      "train: iter 78  trainloss -1.52555  validloss -1.77888±0.00000  bestvalidloss -1.77888  last_update 0\n",
      "train: iter 79  trainloss -1.55697  validloss -1.77607±0.00000  bestvalidloss -1.77888  last_update 1\n",
      "train: iter 80  trainloss -1.58834  validloss -1.81726±0.00000  bestvalidloss -1.81726  last_update 0\n",
      "train: iter 81  trainloss -1.59587  validloss -1.82109±0.00000  bestvalidloss -1.82109  last_update 0\n",
      "train: iter 82  trainloss -1.59142  validloss -1.88953±0.00000  bestvalidloss -1.88953  last_update 0\n",
      "train: iter 83  trainloss -1.61948  validloss -1.92507±0.00000  bestvalidloss -1.92507  last_update 0\n",
      "train: iter 84  trainloss -1.65996  validloss -1.93621±0.00000  bestvalidloss -1.93621  last_update 0\n",
      "train: iter 85  trainloss -1.66052  validloss -1.90266±0.00000  bestvalidloss -1.93621  last_update 1\n",
      "train: iter 86  trainloss -1.68395  validloss -1.95982±0.00000  bestvalidloss -1.95982  last_update 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 87  trainloss -1.69872  validloss -1.95623±0.00000  bestvalidloss -1.95982  last_update 1\n",
      "train: iter 88  trainloss -1.69929  validloss -2.06175±0.00000  bestvalidloss -2.06175  last_update 0\n",
      "train: iter 89  trainloss -1.71023  validloss -2.03189±0.00000  bestvalidloss -2.06175  last_update 1\n",
      "train: iter 90  trainloss -1.72901  validloss -2.01664±0.00000  bestvalidloss -2.06175  last_update 2\n",
      "train: iter 91  trainloss -1.72643  validloss -2.08970±0.00000  bestvalidloss -2.08970  last_update 0\n",
      "train: iter 92  trainloss -1.75885  validloss -1.98157±0.00000  bestvalidloss -2.08970  last_update 1\n",
      "train: iter 93  trainloss -1.75066  validloss -2.12486±0.00000  bestvalidloss -2.12486  last_update 0\n",
      "train: iter 94  trainloss -1.76477  validloss -2.07095±0.00000  bestvalidloss -2.12486  last_update 1\n",
      "train: iter 95  trainloss -1.76869  validloss -2.11651±0.00000  bestvalidloss -2.12486  last_update 2\n",
      "train: iter 96  trainloss -1.76648  validloss -2.12361±0.00000  bestvalidloss -2.12486  last_update 3\n",
      "train: iter 97  trainloss -1.80982  validloss -2.10548±0.00000  bestvalidloss -2.12486  last_update 4\n",
      "train: iter 98  trainloss -1.78129  validloss -2.15531±0.00000  bestvalidloss -2.15531  last_update 0\n",
      "train: iter 99  trainloss -1.77971  validloss -2.16789±0.00000  bestvalidloss -2.16789  last_update 0\n",
      "train: iter 100  trainloss -1.79432  validloss -2.19620±0.00000  bestvalidloss -2.19620  last_update 0\n",
      "train: iter 101  trainloss -1.78681  validloss -2.22205±0.00000  bestvalidloss -2.22205  last_update 0\n",
      "train: iter 102  trainloss -1.80075  validloss -2.15800±0.00000  bestvalidloss -2.22205  last_update 1\n",
      "train: iter 103  trainloss -1.84175  validloss -2.13050±0.00000  bestvalidloss -2.22205  last_update 2\n",
      "train: iter 104  trainloss -1.83149  validloss -2.17562±0.00000  bestvalidloss -2.22205  last_update 3\n",
      "train: iter 105  trainloss -1.81475  validloss -2.17868±0.00000  bestvalidloss -2.22205  last_update 4\n",
      "train: iter 106  trainloss -1.78346  validloss -2.19868±0.00000  bestvalidloss -2.22205  last_update 5\n",
      "train: iter 107  trainloss -1.78151  validloss -2.16026±0.00000  bestvalidloss -2.22205  last_update 6\n",
      "train: iter 108  trainloss -1.80967  validloss -2.14643±0.00000  bestvalidloss -2.22205  last_update 7\n",
      "train: iter 109  trainloss -1.79758  validloss -2.17356±0.00000  bestvalidloss -2.22205  last_update 8\n",
      "train: iter 110  trainloss -1.81717  validloss -2.18441±0.00000  bestvalidloss -2.22205  last_update 9\n",
      "train: iter 111  trainloss -1.78703  validloss -2.26818±0.00000  bestvalidloss -2.26818  last_update 0\n",
      "train: iter 112  trainloss -1.83731  validloss -2.08168±0.00000  bestvalidloss -2.26818  last_update 1\n",
      "train: iter 113  trainloss -1.81314  validloss -2.21423±0.00000  bestvalidloss -2.26818  last_update 2\n",
      "train: iter 114  trainloss -1.79295  validloss -2.17193±0.00000  bestvalidloss -2.26818  last_update 3\n",
      "train: iter 115  trainloss -1.74678  validloss -2.22175±0.00000  bestvalidloss -2.26818  last_update 4\n",
      "train: iter 116  trainloss -1.85624  validloss -2.27358±0.00000  bestvalidloss -2.27358  last_update 0\n",
      "train: iter 117  trainloss -1.77927  validloss -2.28429±0.00000  bestvalidloss -2.28429  last_update 0\n",
      "train: iter 118  trainloss -1.79673  validloss -2.13649±0.00000  bestvalidloss -2.28429  last_update 1\n",
      "train: iter 119  trainloss -1.71426  validloss -2.18949±0.00000  bestvalidloss -2.28429  last_update 2\n",
      "train: iter 120  trainloss -1.74612  validloss -2.26581±0.00000  bestvalidloss -2.28429  last_update 3\n",
      "train: iter 121  trainloss -1.79766  validloss -2.19306±0.00000  bestvalidloss -2.28429  last_update 4\n",
      "train: iter 122  trainloss -1.76441  validloss -2.26108±0.00000  bestvalidloss -2.28429  last_update 5\n",
      "train: iter 123  trainloss -1.74847  validloss -2.21880±0.00000  bestvalidloss -2.28429  last_update 6\n",
      "train: iter 124  trainloss -1.81073  validloss -2.19017±0.00000  bestvalidloss -2.28429  last_update 7\n",
      "train: iter 125  trainloss -1.80203  validloss -2.20556±0.00000  bestvalidloss -2.28429  last_update 8\n",
      "train: iter 126  trainloss -1.79047  validloss -2.26035±0.00000  bestvalidloss -2.28429  last_update 9\n",
      "train: iter 127  trainloss -1.79041  validloss -2.27834±0.00000  bestvalidloss -2.28429  last_update 10\n",
      "train: iter 128  trainloss -1.82238  validloss -2.18186±0.00000  bestvalidloss -2.28429  last_update 11\n",
      "train: iter 129  trainloss -1.82365  validloss -2.22218±0.00000  bestvalidloss -2.28429  last_update 12\n",
      "train: iter 130  trainloss -1.82208  validloss -2.24726±0.00000  bestvalidloss -2.28429  last_update 13\n",
      "train: iter 131  trainloss -1.77724  validloss -2.22753±0.00000  bestvalidloss -2.28429  last_update 14\n",
      "train: iter 132  trainloss -1.85043  validloss -2.26832±0.00000  bestvalidloss -2.28429  last_update 15\n",
      "train: iter 133  trainloss -1.81779  validloss -2.27456±0.00000  bestvalidloss -2.28429  last_update 16\n",
      "train: iter 134  trainloss -1.81959  validloss -2.14056±0.00000  bestvalidloss -2.28429  last_update 17\n",
      "train: iter 135  trainloss -1.78742  validloss -2.29042±0.00000  bestvalidloss -2.29042  last_update 0\n",
      "train: iter 136  trainloss -1.81561  validloss -2.25984±0.00000  bestvalidloss -2.29042  last_update 1\n",
      "train: iter 137  trainloss -1.88430  validloss -2.21235±0.00000  bestvalidloss -2.29042  last_update 2\n",
      "train: iter 138  trainloss -1.80083  validloss -2.18599±0.00000  bestvalidloss -2.29042  last_update 3\n",
      "train: iter 139  trainloss -1.82754  validloss -2.18761±0.00000  bestvalidloss -2.29042  last_update 4\n",
      "train: iter 140  trainloss -1.84829  validloss -2.14774±0.00000  bestvalidloss -2.29042  last_update 5\n",
      "train: iter 141  trainloss -1.79489  validloss -2.15644±0.00000  bestvalidloss -2.29042  last_update 6\n",
      "train: iter 142  trainloss -1.80408  validloss -2.23787±0.00000  bestvalidloss -2.29042  last_update 7\n",
      "train: iter 143  trainloss -1.83234  validloss -2.21949±0.00000  bestvalidloss -2.29042  last_update 8\n",
      "train: iter 144  trainloss -1.81780  validloss -2.21545±0.00000  bestvalidloss -2.29042  last_update 9\n",
      "train: iter 145  trainloss -1.78049  validloss -2.21224±0.00000  bestvalidloss -2.29042  last_update 10\n",
      "train: iter 146  trainloss -1.81174  validloss -2.22133±0.00000  bestvalidloss -2.29042  last_update 11\n",
      "train: iter 147  trainloss -1.78618  validloss -2.15700±0.00000  bestvalidloss -2.29042  last_update 12\n",
      "train: iter 148  trainloss -1.74482  validloss -2.16891±0.00000  bestvalidloss -2.29042  last_update 13\n",
      "train: iter 149  trainloss -1.81370  validloss -2.25674±0.00000  bestvalidloss -2.29042  last_update 14\n",
      "train: iter 150  trainloss -1.84280  validloss -2.17577±0.00000  bestvalidloss -2.29042  last_update 15\n",
      "train: iter 151  trainloss -1.77997  validloss -2.21647±0.00000  bestvalidloss -2.29042  last_update 16\n",
      "train: iter 152  trainloss -1.80343  validloss -2.16750±0.00000  bestvalidloss -2.29042  last_update 17\n",
      "train: iter 153  trainloss -1.81552  validloss -2.14639±0.00000  bestvalidloss -2.29042  last_update 18\n",
      "train: iter 154  trainloss -1.84111  validloss -2.16845±0.00000  bestvalidloss -2.29042  last_update 19\n",
      "train: iter 155  trainloss -1.73114  validloss -2.27706±0.00000  bestvalidloss -2.29042  last_update 20\n",
      "train: iter 156  trainloss -1.81489  validloss -2.29788±0.00000  bestvalidloss -2.29788  last_update 0\n",
      "train: iter 157  trainloss -1.84679  validloss -2.21364±0.00000  bestvalidloss -2.29788  last_update 1\n",
      "train: iter 158  trainloss -1.78217  validloss -2.17232±0.00000  bestvalidloss -2.29788  last_update 2\n",
      "train: iter 159  trainloss -1.76884  validloss -2.17998±0.00000  bestvalidloss -2.29788  last_update 3\n",
      "train: iter 160  trainloss -1.81002  validloss -2.23224±0.00000  bestvalidloss -2.29788  last_update 4\n",
      "train: iter 161  trainloss -1.77158  validloss -2.15281±0.00000  bestvalidloss -2.29788  last_update 5\n",
      "train: iter 162  trainloss -1.77145  validloss -2.25185±0.00000  bestvalidloss -2.29788  last_update 6\n",
      "train: iter 163  trainloss -1.82259  validloss -2.23571±0.00000  bestvalidloss -2.29788  last_update 7\n",
      "train: iter 164  trainloss -1.82068  validloss -2.15172±0.00000  bestvalidloss -2.29788  last_update 8\n",
      "train: iter 165  trainloss -1.75105  validloss -2.17549±0.00000  bestvalidloss -2.29788  last_update 9\n",
      "train: iter 166  trainloss -1.83327  validloss -2.21175±0.00000  bestvalidloss -2.29788  last_update 10\n",
      "train: iter 167  trainloss -1.85783  validloss -2.20482±0.00000  bestvalidloss -2.29788  last_update 11\n",
      "train: iter 168  trainloss -1.84015  validloss -2.30066±0.00000  bestvalidloss -2.30066  last_update 0\n",
      "train: iter 169  trainloss -1.82948  validloss -2.25711±0.00000  bestvalidloss -2.30066  last_update 1\n",
      "train: iter 170  trainloss -1.77581  validloss -2.27392±0.00000  bestvalidloss -2.30066  last_update 2\n",
      "train: iter 171  trainloss -1.79093  validloss -2.21723±0.00000  bestvalidloss -2.30066  last_update 3\n",
      "train: iter 172  trainloss -1.76746  validloss -2.24034±0.00000  bestvalidloss -2.30066  last_update 4\n",
      "train: iter 173  trainloss -1.76863  validloss -2.30343±0.00000  bestvalidloss -2.30343  last_update 0\n",
      "train: iter 174  trainloss -1.79985  validloss -2.12177±0.00000  bestvalidloss -2.30343  last_update 1\n",
      "train: iter 175  trainloss -1.80756  validloss -2.22591±0.00000  bestvalidloss -2.30343  last_update 2\n",
      "train: iter 176  trainloss -1.83728  validloss -2.20965±0.00000  bestvalidloss -2.30343  last_update 3\n",
      "train: iter 177  trainloss -1.73587  validloss -2.22489±0.00000  bestvalidloss -2.30343  last_update 4\n",
      "train: iter 178  trainloss -1.79008  validloss -2.21511±0.00000  bestvalidloss -2.30343  last_update 5\n",
      "train: iter 179  trainloss -1.80111  validloss -2.23955±0.00000  bestvalidloss -2.30343  last_update 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 180  trainloss -1.82786  validloss -2.24777±0.00000  bestvalidloss -2.30343  last_update 7\n",
      "train: iter 181  trainloss -1.77056  validloss -2.23525±0.00000  bestvalidloss -2.30343  last_update 8\n",
      "train: iter 182  trainloss -1.80774  validloss -2.27929±0.00000  bestvalidloss -2.30343  last_update 9\n",
      "train: iter 183  trainloss -1.76786  validloss -2.19431±0.00000  bestvalidloss -2.30343  last_update 10\n",
      "train: iter 184  trainloss -1.81860  validloss -2.22563±0.00000  bestvalidloss -2.30343  last_update 11\n",
      "train: iter 185  trainloss -1.79686  validloss -2.17019±0.00000  bestvalidloss -2.30343  last_update 12\n",
      "train: iter 186  trainloss -1.78748  validloss -2.22481±0.00000  bestvalidloss -2.30343  last_update 13\n",
      "train: iter 187  trainloss -1.83421  validloss -2.18591±0.00000  bestvalidloss -2.30343  last_update 14\n",
      "train: iter 188  trainloss -1.79956  validloss -2.18247±0.00000  bestvalidloss -2.30343  last_update 15\n",
      "train: iter 189  trainloss -1.77715  validloss -2.23764±0.00000  bestvalidloss -2.30343  last_update 16\n",
      "train: iter 190  trainloss -1.85649  validloss -2.21295±0.00000  bestvalidloss -2.30343  last_update 17\n",
      "train: iter 191  trainloss -1.73962  validloss -2.17368±0.00000  bestvalidloss -2.30343  last_update 18\n",
      "train: iter 192  trainloss -1.81087  validloss -2.15957±0.00000  bestvalidloss -2.30343  last_update 19\n",
      "train: iter 193  trainloss -1.81410  validloss -2.29086±0.00000  bestvalidloss -2.30343  last_update 20\n",
      "train: iter 194  trainloss -1.76865  validloss -2.14188±0.00000  bestvalidloss -2.30343  last_update 21\n",
      "train: iter 195  trainloss -1.80819  validloss -2.20233±0.00000  bestvalidloss -2.30343  last_update 22\n",
      "train: iter 196  trainloss -1.85849  validloss -2.21285±0.00000  bestvalidloss -2.30343  last_update 23\n",
      "train: iter 197  trainloss -1.83141  validloss -2.19500±0.00000  bestvalidloss -2.30343  last_update 24\n",
      "train: iter 198  trainloss -1.81190  validloss -2.21191±0.00000  bestvalidloss -2.30343  last_update 25\n",
      "train: iter 199  trainloss -1.80002  validloss -2.19244±0.00000  bestvalidloss -2.30343  last_update 26\n",
      "train: iter 200  trainloss -1.86841  validloss -2.19951±0.00000  bestvalidloss -2.30343  last_update 27\n",
      "train: iter 201  trainloss -1.85728  validloss -2.24981±0.00000  bestvalidloss -2.30343  last_update 28\n",
      "train: iter 202  trainloss -1.83158  validloss -2.18106±0.00000  bestvalidloss -2.30343  last_update 29\n",
      "train: iter 203  trainloss -1.84858  validloss -2.14844±0.00000  bestvalidloss -2.30343  last_update 30\n",
      "train: iter 204  trainloss -1.79489  validloss -2.24398±0.00000  bestvalidloss -2.30343  last_update 31\n",
      "train: iter 205  trainloss -1.77710  validloss -2.23207±0.00000  bestvalidloss -2.30343  last_update 32\n",
      "train: iter 206  trainloss -1.77880  validloss -2.22628±0.00000  bestvalidloss -2.30343  last_update 33\n",
      "train: iter 207  trainloss -1.78309  validloss -2.27099±0.00000  bestvalidloss -2.30343  last_update 34\n",
      "train: iter 208  trainloss -1.82066  validloss -2.32036±0.00000  bestvalidloss -2.32036  last_update 0\n",
      "train: iter 209  trainloss -1.78230  validloss -2.20158±0.00000  bestvalidloss -2.32036  last_update 1\n",
      "train: iter 210  trainloss -1.82041  validloss -2.20465±0.00000  bestvalidloss -2.32036  last_update 2\n",
      "train: iter 211  trainloss -1.81239  validloss -2.20090±0.00000  bestvalidloss -2.32036  last_update 3\n",
      "train: iter 212  trainloss -1.83485  validloss -2.19769±0.00000  bestvalidloss -2.32036  last_update 4\n",
      "train: iter 213  trainloss -1.82521  validloss -2.18962±0.00000  bestvalidloss -2.32036  last_update 5\n",
      "train: iter 214  trainloss -1.79153  validloss -2.29654±0.00000  bestvalidloss -2.32036  last_update 6\n",
      "train: iter 215  trainloss -1.80762  validloss -2.13209±0.00000  bestvalidloss -2.32036  last_update 7\n",
      "train: iter 216  trainloss -1.82351  validloss -2.18069±0.00000  bestvalidloss -2.32036  last_update 8\n",
      "train: iter 217  trainloss -1.83854  validloss -2.18973±0.00000  bestvalidloss -2.32036  last_update 9\n",
      "train: iter 218  trainloss -1.80854  validloss -2.13883±0.00000  bestvalidloss -2.32036  last_update 10\n",
      "train: iter 219  trainloss -1.81990  validloss -2.23202±0.00000  bestvalidloss -2.32036  last_update 11\n",
      "train: iter 220  trainloss -1.78736  validloss -2.29318±0.00000  bestvalidloss -2.32036  last_update 12\n",
      "train: iter 221  trainloss -1.79956  validloss -2.20306±0.00000  bestvalidloss -2.32036  last_update 13\n",
      "train: iter 222  trainloss -1.78055  validloss -2.16663±0.00000  bestvalidloss -2.32036  last_update 14\n",
      "train: iter 223  trainloss -1.79817  validloss -2.21620±0.00000  bestvalidloss -2.32036  last_update 15\n",
      "train: iter 224  trainloss -1.84323  validloss -2.25791±0.00000  bestvalidloss -2.32036  last_update 16\n",
      "train: iter 225  trainloss -1.83192  validloss -2.24988±0.00000  bestvalidloss -2.32036  last_update 17\n",
      "train: iter 226  trainloss -1.73953  validloss -2.24672±0.00000  bestvalidloss -2.32036  last_update 18\n",
      "train: iter 227  trainloss -1.77887  validloss -2.28095±0.00000  bestvalidloss -2.32036  last_update 19\n",
      "train: iter 228  trainloss -1.79794  validloss -2.19262±0.00000  bestvalidloss -2.32036  last_update 20\n",
      "train: iter 229  trainloss -1.80445  validloss -2.20907±0.00000  bestvalidloss -2.32036  last_update 21\n",
      "train: iter 230  trainloss -1.81531  validloss -2.12848±0.00000  bestvalidloss -2.32036  last_update 22\n",
      "train: iter 231  trainloss -1.82088  validloss -2.24190±0.00000  bestvalidloss -2.32036  last_update 23\n",
      "train: iter 232  trainloss -1.75289  validloss -2.25539±0.00000  bestvalidloss -2.32036  last_update 24\n",
      "train: iter 233  trainloss -1.81735  validloss -2.20815±0.00000  bestvalidloss -2.32036  last_update 25\n",
      "train: iter 234  trainloss -1.82194  validloss -2.25784±0.00000  bestvalidloss -2.32036  last_update 26\n",
      "train: iter 235  trainloss -1.85445  validloss -2.24589±0.00000  bestvalidloss -2.32036  last_update 27\n",
      "train: iter 236  trainloss -1.83623  validloss -2.31180±0.00000  bestvalidloss -2.32036  last_update 28\n",
      "train: iter 237  trainloss -1.80755  validloss -2.19684±0.00000  bestvalidloss -2.32036  last_update 29\n",
      "train: iter 238  trainloss -1.77401  validloss -2.24918±0.00000  bestvalidloss -2.32036  last_update 30\n",
      "train: iter 239  trainloss -1.80707  validloss -2.13617±0.00000  bestvalidloss -2.32036  last_update 31\n",
      "train: iter 240  trainloss -1.81776  validloss -2.32280±0.00000  bestvalidloss -2.32280  last_update 0\n",
      "train: iter 241  trainloss -1.81021  validloss -2.17353±0.00000  bestvalidloss -2.32280  last_update 1\n",
      "train: iter 242  trainloss -1.83184  validloss -2.22179±0.00000  bestvalidloss -2.32280  last_update 2\n",
      "train: iter 243  trainloss -1.79434  validloss -2.21195±0.00000  bestvalidloss -2.32280  last_update 3\n",
      "train: iter 244  trainloss -1.88649  validloss -2.20376±0.00000  bestvalidloss -2.32280  last_update 4\n",
      "train: iter 245  trainloss -1.79413  validloss -2.24790±0.00000  bestvalidloss -2.32280  last_update 5\n",
      "train: iter 246  trainloss -1.81831  validloss -2.21762±0.00000  bestvalidloss -2.32280  last_update 6\n",
      "train: iter 247  trainloss -1.80601  validloss -2.19081±0.00000  bestvalidloss -2.32280  last_update 7\n",
      "train: iter 248  trainloss -1.81953  validloss -2.28358±0.00000  bestvalidloss -2.32280  last_update 8\n",
      "train: iter 249  trainloss -1.82381  validloss -2.22827±0.00000  bestvalidloss -2.32280  last_update 9\n",
      "train: iter 250  trainloss -1.81492  validloss -2.23599±0.00000  bestvalidloss -2.32280  last_update 10\n",
      "train: iter 251  trainloss -1.82680  validloss -2.23842±0.00000  bestvalidloss -2.32280  last_update 11\n",
      "train: iter 252  trainloss -1.77436  validloss -2.21887±0.00000  bestvalidloss -2.32280  last_update 12\n",
      "train: iter 253  trainloss -1.85198  validloss -2.20902±0.00000  bestvalidloss -2.32280  last_update 13\n",
      "train: iter 254  trainloss -1.83276  validloss -2.16913±0.00000  bestvalidloss -2.32280  last_update 14\n",
      "train: iter 255  trainloss -1.83285  validloss -2.20366±0.00000  bestvalidloss -2.32280  last_update 15\n",
      "train: iter 256  trainloss -1.77856  validloss -2.20051±0.00000  bestvalidloss -2.32280  last_update 16\n",
      "train: iter 257  trainloss -1.83712  validloss -2.18841±0.00000  bestvalidloss -2.32280  last_update 17\n",
      "train: iter 258  trainloss -1.84408  validloss -2.22882±0.00000  bestvalidloss -2.32280  last_update 18\n",
      "train: iter 259  trainloss -1.80820  validloss -2.20007±0.00000  bestvalidloss -2.32280  last_update 19\n",
      "train: iter 260  trainloss -1.77416  validloss -2.17436±0.00000  bestvalidloss -2.32280  last_update 20\n",
      "train: iter 261  trainloss -1.86579  validloss -2.26259±0.00000  bestvalidloss -2.32280  last_update 21\n",
      "train: iter 262  trainloss -1.82332  validloss -2.19163±0.00000  bestvalidloss -2.32280  last_update 22\n",
      "train: iter 263  trainloss -1.82342  validloss -2.18187±0.00000  bestvalidloss -2.32280  last_update 23\n",
      "train: iter 264  trainloss -1.82024  validloss -2.12387±0.00000  bestvalidloss -2.32280  last_update 24\n",
      "train: iter 265  trainloss -1.76223  validloss -2.26115±0.00000  bestvalidloss -2.32280  last_update 25\n",
      "train: iter 266  trainloss -1.76353  validloss -2.25537±0.00000  bestvalidloss -2.32280  last_update 26\n",
      "train: iter 267  trainloss -1.79524  validloss -2.21016±0.00000  bestvalidloss -2.32280  last_update 27\n",
      "train: iter 268  trainloss -1.77058  validloss -2.19798±0.00000  bestvalidloss -2.32280  last_update 28\n",
      "train: iter 269  trainloss -1.82507  validloss -2.15589±0.00000  bestvalidloss -2.32280  last_update 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 270  trainloss -1.78646  validloss -2.30314±0.00000  bestvalidloss -2.32280  last_update 30\n",
      "train: iter 271  trainloss -1.79946  validloss -2.19381±0.00000  bestvalidloss -2.32280  last_update 31\n",
      "train: iter 272  trainloss -1.78773  validloss -2.18096±0.00000  bestvalidloss -2.32280  last_update 32\n",
      "train: iter 273  trainloss -1.80817  validloss -2.18655±0.00000  bestvalidloss -2.32280  last_update 33\n",
      "train: iter 274  trainloss -1.83863  validloss -2.18295±0.00000  bestvalidloss -2.32280  last_update 34\n",
      "train: iter 275  trainloss -1.81658  validloss -2.19558±0.00000  bestvalidloss -2.32280  last_update 35\n",
      "train: iter 276  trainloss -1.81858  validloss -2.24124±0.00000  bestvalidloss -2.32280  last_update 36\n",
      "train: iter 277  trainloss -1.78536  validloss -2.27958±0.00000  bestvalidloss -2.32280  last_update 37\n",
      "train: iter 278  trainloss -1.77280  validloss -2.24187±0.00000  bestvalidloss -2.32280  last_update 38\n",
      "train: iter 279  trainloss -1.82626  validloss -2.18319±0.00000  bestvalidloss -2.32280  last_update 39\n",
      "train: iter 280  trainloss -1.79856  validloss -2.23210±0.00000  bestvalidloss -2.32280  last_update 40\n",
      "train: iter 281  trainloss -1.74581  validloss -2.21861±0.00000  bestvalidloss -2.32280  last_update 41\n",
      "train: iter 282  trainloss -1.78910  validloss -2.28804±0.00000  bestvalidloss -2.32280  last_update 42\n",
      "train: iter 283  trainloss -1.75845  validloss -2.12756±0.00000  bestvalidloss -2.32280  last_update 43\n",
      "train: iter 284  trainloss -1.77981  validloss -2.15722±0.00000  bestvalidloss -2.32280  last_update 44\n",
      "train: iter 285  trainloss -1.79276  validloss -2.23194±0.00000  bestvalidloss -2.32280  last_update 45\n",
      "train: iter 286  trainloss -1.78603  validloss -2.14770±0.00000  bestvalidloss -2.32280  last_update 46\n",
      "train: iter 287  trainloss -1.84182  validloss -2.22293±0.00000  bestvalidloss -2.32280  last_update 47\n",
      "train: iter 288  trainloss -1.79424  validloss -2.20384±0.00000  bestvalidloss -2.32280  last_update 48\n",
      "train: iter 289  trainloss -1.85214  validloss -2.17685±0.00000  bestvalidloss -2.32280  last_update 49\n",
      "train: iter 290  trainloss -1.82411  validloss -2.21319±0.00000  bestvalidloss -2.32280  last_update 50\n",
      "train: iter 291  trainloss -1.79967  validloss -2.25318±0.00000  bestvalidloss -2.32280  last_update 51\n",
      "train: iter 292  trainloss -1.80285  validloss -2.22086±0.00000  bestvalidloss -2.32280  last_update 52\n",
      "train: iter 293  trainloss -1.86284  validloss -2.20414±0.00000  bestvalidloss -2.32280  last_update 53\n",
      "train: iter 294  trainloss -1.81768  validloss -2.22286±0.00000  bestvalidloss -2.32280  last_update 54\n",
      "train: iter 295  trainloss -1.76057  validloss -2.25548±0.00000  bestvalidloss -2.32280  last_update 55\n",
      "train: iter 296  trainloss -1.81254  validloss -2.27332±0.00000  bestvalidloss -2.32280  last_update 56\n",
      "train: iter 297  trainloss -1.78135  validloss -2.28528±0.00000  bestvalidloss -2.32280  last_update 57\n",
      "train: iter 298  trainloss -1.83885  validloss -2.28482±0.00000  bestvalidloss -2.32280  last_update 58\n",
      "train: iter 299  trainloss -1.84303  validloss -2.25810±0.00000  bestvalidloss -2.32280  last_update 59\n",
      "train: iter 300  trainloss -1.82309  validloss -2.20661±0.00000  bestvalidloss -2.32280  last_update 60\n",
      "train: iter 301  trainloss -1.80588  validloss -2.26428±0.00000  bestvalidloss -2.32280  last_update 61\n",
      "train: iter 302  trainloss -1.84021  validloss -2.25062±0.00000  bestvalidloss -2.32280  last_update 62\n",
      "train: iter 303  trainloss -1.86739  validloss -2.28590±0.00000  bestvalidloss -2.32280  last_update 63\n",
      "train: iter 304  trainloss -1.80264  validloss -2.24398±0.00000  bestvalidloss -2.32280  last_update 64\n",
      "train: iter 305  trainloss -1.79804  validloss -2.24755±0.00000  bestvalidloss -2.32280  last_update 65\n",
      "train: iter 306  trainloss -1.84141  validloss -2.28649±0.00000  bestvalidloss -2.32280  last_update 66\n",
      "train: iter 307  trainloss -1.83664  validloss -2.22543±0.00000  bestvalidloss -2.32280  last_update 67\n",
      "train: iter 308  trainloss -1.84987  validloss -2.27339±0.00000  bestvalidloss -2.32280  last_update 68\n",
      "train: iter 309  trainloss -1.84330  validloss -2.24695±0.00000  bestvalidloss -2.32280  last_update 69\n",
      "train: iter 310  trainloss -1.79122  validloss -2.27877±0.00000  bestvalidloss -2.32280  last_update 70\n",
      "train: iter 311  trainloss -1.82948  validloss -2.24780±0.00000  bestvalidloss -2.32280  last_update 71\n",
      "train: iter 312  trainloss -1.76786  validloss -2.20690±0.00000  bestvalidloss -2.32280  last_update 72\n",
      "train: iter 313  trainloss -1.76418  validloss -2.24060±0.00000  bestvalidloss -2.32280  last_update 73\n",
      "train: iter 314  trainloss -1.83689  validloss -2.24854±0.00000  bestvalidloss -2.32280  last_update 74\n",
      "train: iter 315  trainloss -1.77609  validloss -2.18412±0.00000  bestvalidloss -2.32280  last_update 75\n",
      "train: iter 316  trainloss -1.79749  validloss -2.24543±0.00000  bestvalidloss -2.32280  last_update 76\n",
      "train: iter 317  trainloss -1.83645  validloss -2.21794±0.00000  bestvalidloss -2.32280  last_update 77\n",
      "train: iter 318  trainloss -1.86128  validloss -2.26576±0.00000  bestvalidloss -2.32280  last_update 78\n",
      "train: iter 319  trainloss -1.77598  validloss -2.25536±0.00000  bestvalidloss -2.32280  last_update 79\n",
      "train: iter 320  trainloss -1.76216  validloss -2.21711±0.00000  bestvalidloss -2.32280  last_update 80\n",
      "train: iter 321  trainloss -1.81810  validloss -2.20670±0.00000  bestvalidloss -2.32280  last_update 81\n",
      "train: iter 322  trainloss -1.77680  validloss -2.21208±0.00000  bestvalidloss -2.32280  last_update 82\n",
      "train: iter 323  trainloss -1.85931  validloss -2.16802±0.00000  bestvalidloss -2.32280  last_update 83\n",
      "train: iter 324  trainloss -1.83036  validloss -2.20972±0.00000  bestvalidloss -2.32280  last_update 84\n",
      "train: iter 325  trainloss -1.81897  validloss -2.25113±0.00000  bestvalidloss -2.32280  last_update 85\n",
      "train: iter 326  trainloss -1.82248  validloss -2.24009±0.00000  bestvalidloss -2.32280  last_update 86\n",
      "train: iter 327  trainloss -1.76703  validloss -2.23340±0.00000  bestvalidloss -2.32280  last_update 87\n",
      "train: iter 328  trainloss -1.77545  validloss -2.22963±0.00000  bestvalidloss -2.32280  last_update 88\n",
      "train: iter 329  trainloss -1.76672  validloss -2.24094±0.00000  bestvalidloss -2.32280  last_update 89\n",
      "train: iter 330  trainloss -1.81612  validloss -2.17708±0.00000  bestvalidloss -2.32280  last_update 90\n",
      "train: iter 331  trainloss -1.80090  validloss -2.18317±0.00000  bestvalidloss -2.32280  last_update 91\n",
      "train: iter 332  trainloss -1.82994  validloss -2.26168±0.00000  bestvalidloss -2.32280  last_update 92\n",
      "train: iter 333  trainloss -1.83072  validloss -2.32271±0.00000  bestvalidloss -2.32280  last_update 93\n",
      "train: iter 334  trainloss -1.80874  validloss -2.29724±0.00000  bestvalidloss -2.32280  last_update 94\n",
      "train: iter 335  trainloss -1.79208  validloss -2.22849±0.00000  bestvalidloss -2.32280  last_update 95\n",
      "train: iter 336  trainloss -1.79879  validloss -2.14347±0.00000  bestvalidloss -2.32280  last_update 96\n",
      "train: iter 337  trainloss -1.81827  validloss -2.24049±0.00000  bestvalidloss -2.32280  last_update 97\n",
      "train: iter 338  trainloss -1.83845  validloss -2.27119±0.00000  bestvalidloss -2.32280  last_update 98\n",
      "train: iter 339  trainloss -1.84550  validloss -2.19400±0.00000  bestvalidloss -2.32280  last_update 99\n",
      "train: iter 340  trainloss -1.79653  validloss -2.29114±0.00000  bestvalidloss -2.32280  last_update 100\n",
      "train: fin\n"
     ]
    }
   ],
   "source": [
    "train_curve, valid_curve = vi.train_initial_belief(num_iter=100000, lr=1e-3, early_stop_step=default_early)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117822aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-1.3128, -2.3334, -4.3603, -4.9416], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(vi.initial_belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f505e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90db667c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 0  trainloss 124.79229  validloss 131.17855±0.00000  bestvalidloss 131.17855  last_update 0\n",
      "train: iter 1  trainloss 94.12099  validloss 105.55231±0.00000  bestvalidloss 105.55231  last_update 0\n",
      "train: iter 2  trainloss 69.88589  validloss 75.59444±0.00000  bestvalidloss 75.59444  last_update 0\n",
      "train: iter 3  trainloss 55.41998  validloss 58.52966±0.00000  bestvalidloss 58.52966  last_update 0\n",
      "train: iter 4  trainloss 44.33200  validloss 46.58771±0.00000  bestvalidloss 46.58771  last_update 0\n",
      "train: iter 5  trainloss 35.61826  validloss 36.94901±0.00000  bestvalidloss 36.94901  last_update 0\n",
      "train: iter 6  trainloss 28.73119  validloss 29.65998±0.00000  bestvalidloss 29.65998  last_update 0\n",
      "train: iter 7  trainloss 23.04634  validloss 23.76023±0.00000  bestvalidloss 23.76023  last_update 0\n",
      "train: iter 8  trainloss 18.43234  validloss 18.83937±0.00000  bestvalidloss 18.83937  last_update 0\n",
      "train: iter 9  trainloss 14.62110  validloss 15.07819±0.00000  bestvalidloss 15.07819  last_update 0\n",
      "train: iter 10  trainloss 11.59084  validloss 12.01015±0.00000  bestvalidloss 12.01015  last_update 0\n",
      "train: iter 11  trainloss 9.13980  validloss 9.37763±0.00000  bestvalidloss 9.37763  last_update 0\n",
      "train: iter 12  trainloss 7.23184  validloss 7.40038±0.00000  bestvalidloss 7.40038  last_update 0\n",
      "train: iter 13  trainloss 5.69214  validloss 5.94335±0.00000  bestvalidloss 5.94335  last_update 0\n",
      "train: iter 14  trainloss 4.49022  validloss 4.72698±0.00000  bestvalidloss 4.72698  last_update 0\n",
      "train: iter 15  trainloss 3.57953  validloss 3.91380±0.00000  bestvalidloss 3.91380  last_update 0\n",
      "train: iter 16  trainloss 2.91046  validloss 3.28225±0.00000  bestvalidloss 3.28225  last_update 0\n",
      "train: iter 17  trainloss 2.39576  validloss 2.67791±0.00000  bestvalidloss 2.67791  last_update 0\n",
      "train: iter 18  trainloss 2.02687  validloss 2.38910±0.00000  bestvalidloss 2.38910  last_update 0\n",
      "train: iter 19  trainloss 1.78625  validloss 2.14689±0.00000  bestvalidloss 2.14689  last_update 0\n",
      "train: iter 20  trainloss 1.56512  validloss 2.05739±0.00000  bestvalidloss 2.05739  last_update 0\n",
      "train: iter 21  trainloss 1.42533  validloss 2.20442±0.00000  bestvalidloss 2.05739  last_update 1\n",
      "train: iter 22  trainloss 1.35124  validloss 1.92042±0.00000  bestvalidloss 1.92042  last_update 0\n",
      "train: iter 23  trainloss 1.32740  validloss 2.06014±0.00000  bestvalidloss 1.92042  last_update 1\n",
      "train: iter 24  trainloss 1.28372  validloss 1.79702±0.00000  bestvalidloss 1.79702  last_update 0\n",
      "train: iter 25  trainloss 1.25628  validloss 1.80714±0.00000  bestvalidloss 1.79702  last_update 1\n",
      "train: iter 26  trainloss 1.22705  validloss 1.77834±0.00000  bestvalidloss 1.77834  last_update 0\n",
      "train: iter 27  trainloss 1.23079  validloss 1.96322±0.00000  bestvalidloss 1.77834  last_update 1\n",
      "train: iter 28  trainloss 1.18716  validloss 2.07428±0.00000  bestvalidloss 1.77834  last_update 2\n",
      "train: iter 29  trainloss 1.22864  validloss 1.80115±0.00000  bestvalidloss 1.77834  last_update 3\n",
      "train: iter 30  trainloss 1.23259  validloss 1.92143±0.00000  bestvalidloss 1.77834  last_update 4\n",
      "train: iter 31  trainloss 1.22004  validloss 2.14562±0.00000  bestvalidloss 1.77834  last_update 5\n",
      "train: iter 32  trainloss 1.22075  validloss 1.78846±0.00000  bestvalidloss 1.77834  last_update 6\n",
      "train: iter 33  trainloss 1.18995  validloss 1.85549±0.00000  bestvalidloss 1.77834  last_update 7\n",
      "train: iter 34  trainloss 1.18266  validloss 2.16074±0.00000  bestvalidloss 1.77834  last_update 8\n",
      "train: iter 35  trainloss 1.25308  validloss 2.04038±0.00000  bestvalidloss 1.77834  last_update 9\n",
      "train: iter 36  trainloss 1.25859  validloss 1.89377±0.00000  bestvalidloss 1.77834  last_update 10\n",
      "train: iter 37  trainloss 1.20495  validloss 2.30533±0.00000  bestvalidloss 1.77834  last_update 11\n",
      "train: iter 38  trainloss 1.19867  validloss 1.99885±0.00000  bestvalidloss 1.77834  last_update 12\n",
      "train: iter 39  trainloss 1.23774  validloss 1.92700±0.00000  bestvalidloss 1.77834  last_update 13\n",
      "train: iter 40  trainloss 1.18085  validloss 2.03970±0.00000  bestvalidloss 1.77834  last_update 14\n",
      "train: iter 41  trainloss 1.19614  validloss 2.02371±0.00000  bestvalidloss 1.77834  last_update 15\n",
      "train: iter 42  trainloss 1.18867  validloss 1.81352±0.00000  bestvalidloss 1.77834  last_update 16\n",
      "train: iter 43  trainloss 1.19658  validloss 1.76794±0.00000  bestvalidloss 1.76794  last_update 0\n",
      "train: iter 44  trainloss 1.24580  validloss 2.06679±0.00000  bestvalidloss 1.76794  last_update 1\n",
      "train: iter 45  trainloss 1.21239  validloss 1.82848±0.00000  bestvalidloss 1.76794  last_update 2\n",
      "train: iter 46  trainloss 1.18028  validloss 2.11529±0.00000  bestvalidloss 1.76794  last_update 3\n",
      "train: iter 47  trainloss 1.23840  validloss 1.83835±0.00000  bestvalidloss 1.76794  last_update 4\n",
      "train: iter 48  trainloss 1.20283  validloss 1.99533±0.00000  bestvalidloss 1.76794  last_update 5\n",
      "train: iter 49  trainloss 1.20989  validloss 2.08119±0.00000  bestvalidloss 1.76794  last_update 6\n",
      "train: iter 50  trainloss 1.18815  validloss 1.98857±0.00000  bestvalidloss 1.76794  last_update 7\n",
      "train: iter 51  trainloss 1.19848  validloss 2.06276±0.00000  bestvalidloss 1.76794  last_update 8\n",
      "train: iter 52  trainloss 1.20466  validloss 2.02869±0.00000  bestvalidloss 1.76794  last_update 9\n",
      "train: iter 53  trainloss 1.16647  validloss 1.94491±0.00000  bestvalidloss 1.76794  last_update 10\n",
      "train: iter 54  trainloss 1.03986  validloss 1.81898±0.00000  bestvalidloss 1.76794  last_update 11\n",
      "train: iter 55  trainloss 0.98499  validloss 1.62150±0.00000  bestvalidloss 1.62150  last_update 0\n",
      "train: iter 56  trainloss 0.99157  validloss 1.76425±0.00000  bestvalidloss 1.62150  last_update 1\n",
      "train: iter 57  trainloss 0.95405  validloss 1.81592±0.00000  bestvalidloss 1.62150  last_update 2\n",
      "train: iter 58  trainloss 0.97431  validloss 1.85435±0.00000  bestvalidloss 1.62150  last_update 3\n",
      "train: iter 59  trainloss 0.97175  validloss 1.66814±0.00000  bestvalidloss 1.62150  last_update 4\n",
      "train: iter 60  trainloss 0.95841  validloss 1.78760±0.00000  bestvalidloss 1.62150  last_update 5\n",
      "train: iter 61  trainloss 0.96560  validloss 1.77393±0.00000  bestvalidloss 1.62150  last_update 6\n",
      "train: iter 62  trainloss 0.98697  validloss 2.10349±0.00000  bestvalidloss 1.62150  last_update 7\n",
      "train: iter 63  trainloss 0.97127  validloss 1.67022±0.00000  bestvalidloss 1.62150  last_update 8\n",
      "train: iter 64  trainloss 0.95446  validloss 1.80931±0.00000  bestvalidloss 1.62150  last_update 9\n",
      "train: iter 65  trainloss 0.95616  validloss 1.87160±0.00000  bestvalidloss 1.62150  last_update 10\n",
      "train: iter 66  trainloss 0.95998  validloss 1.74519±0.00000  bestvalidloss 1.62150  last_update 11\n",
      "train: iter 67  trainloss 0.97044  validloss 1.82361±0.00000  bestvalidloss 1.62150  last_update 12\n",
      "train: iter 68  trainloss 0.92775  validloss 1.75113±0.00000  bestvalidloss 1.62150  last_update 13\n",
      "train: iter 69  trainloss 0.94863  validloss 1.75737±0.00000  bestvalidloss 1.62150  last_update 14\n",
      "train: iter 70  trainloss 0.92587  validloss 1.66119±0.00000  bestvalidloss 1.62150  last_update 15\n",
      "train: iter 71  trainloss 0.93943  validloss 1.72896±0.00000  bestvalidloss 1.62150  last_update 16\n",
      "train: iter 72  trainloss 0.94755  validloss 2.04214±0.00000  bestvalidloss 1.62150  last_update 17\n",
      "train: iter 73  trainloss 0.96007  validloss 1.71987±0.00000  bestvalidloss 1.62150  last_update 18\n",
      "train: iter 74  trainloss 0.95737  validloss 1.73029±0.00000  bestvalidloss 1.62150  last_update 19\n",
      "train: iter 75  trainloss 0.92849  validloss 2.00770±0.00000  bestvalidloss 1.62150  last_update 20\n",
      "train: iter 76  trainloss 0.93579  validloss 1.92189±0.00000  bestvalidloss 1.62150  last_update 21\n",
      "train: iter 77  trainloss 0.93654  validloss 1.78642±0.00000  bestvalidloss 1.62150  last_update 22\n",
      "train: iter 78  trainloss 0.92780  validloss 1.71463±0.00000  bestvalidloss 1.62150  last_update 23\n",
      "train: iter 79  trainloss 0.94970  validloss 1.70448±0.00000  bestvalidloss 1.62150  last_update 24\n",
      "train: iter 80  trainloss 0.93180  validloss 1.84629±0.00000  bestvalidloss 1.62150  last_update 25\n",
      "train: iter 81  trainloss 0.95921  validloss 1.91199±0.00000  bestvalidloss 1.62150  last_update 26\n",
      "train: iter 82  trainloss 0.95134  validloss 1.80169±0.00000  bestvalidloss 1.62150  last_update 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 83  trainloss 0.93512  validloss 1.83065±0.00000  bestvalidloss 1.62150  last_update 28\n",
      "train: iter 84  trainloss 0.92019  validloss 1.73881±0.00000  bestvalidloss 1.62150  last_update 29\n",
      "train: iter 85  trainloss 0.92971  validloss 1.70916±0.00000  bestvalidloss 1.62150  last_update 30\n",
      "train: iter 86  trainloss 0.94701  validloss 1.78423±0.00000  bestvalidloss 1.62150  last_update 31\n",
      "train: iter 87  trainloss 0.92201  validloss 1.84744±0.00000  bestvalidloss 1.62150  last_update 32\n",
      "train: iter 88  trainloss 0.95328  validloss 1.62892±0.00000  bestvalidloss 1.62150  last_update 33\n",
      "train: iter 89  trainloss 0.93950  validloss 1.65073±0.00000  bestvalidloss 1.62150  last_update 34\n",
      "train: iter 90  trainloss 0.92938  validloss 1.67066±0.00000  bestvalidloss 1.62150  last_update 35\n",
      "train: iter 91  trainloss 0.94650  validloss 1.67181±0.00000  bestvalidloss 1.62150  last_update 36\n",
      "train: iter 92  trainloss 0.90541  validloss 1.62355±0.00000  bestvalidloss 1.62150  last_update 37\n",
      "train: iter 93  trainloss 0.96209  validloss 1.82630±0.00000  bestvalidloss 1.62150  last_update 38\n",
      "train: iter 94  trainloss 0.93593  validloss 1.76537±0.00000  bestvalidloss 1.62150  last_update 39\n",
      "train: iter 95  trainloss 0.93726  validloss 1.89499±0.00000  bestvalidloss 1.62150  last_update 40\n",
      "train: iter 96  trainloss 0.95381  validloss 1.67963±0.00000  bestvalidloss 1.62150  last_update 41\n",
      "train: iter 97  trainloss 0.95052  validloss 1.75940±0.00000  bestvalidloss 1.62150  last_update 42\n",
      "train: iter 98  trainloss 0.92718  validloss 1.63728±0.00000  bestvalidloss 1.62150  last_update 43\n",
      "train: iter 99  trainloss 0.92200  validloss 1.86756±0.00000  bestvalidloss 1.62150  last_update 44\n",
      "train: iter 100  trainloss 0.91505  validloss 1.72355±0.00000  bestvalidloss 1.62150  last_update 45\n",
      "train: iter 101  trainloss 0.94733  validloss 1.85130±0.00000  bestvalidloss 1.62150  last_update 46\n",
      "train: iter 102  trainloss 0.90242  validloss 1.74707±0.00000  bestvalidloss 1.62150  last_update 47\n",
      "train: iter 103  trainloss 0.93954  validloss 1.76126±0.00000  bestvalidloss 1.62150  last_update 48\n",
      "train: iter 104  trainloss 0.89266  validloss 1.60966±0.00000  bestvalidloss 1.60966  last_update 0\n",
      "train: iter 105  trainloss 0.91962  validloss 1.56888±0.00000  bestvalidloss 1.56888  last_update 0\n",
      "train: iter 106  trainloss 0.88694  validloss 1.93991±0.00000  bestvalidloss 1.56888  last_update 1\n",
      "train: iter 107  trainloss 0.90323  validloss 1.73727±0.00000  bestvalidloss 1.56888  last_update 2\n",
      "train: iter 108  trainloss 0.87141  validloss 1.73509±0.00000  bestvalidloss 1.56888  last_update 3\n",
      "train: iter 109  trainloss 0.89129  validloss 1.76038±0.00000  bestvalidloss 1.56888  last_update 4\n",
      "train: iter 110  trainloss 0.87849  validloss 1.55868±0.00000  bestvalidloss 1.55868  last_update 0\n",
      "train: iter 111  trainloss 0.89917  validloss 1.76808±0.00000  bestvalidloss 1.55868  last_update 1\n",
      "train: iter 112  trainloss 0.90279  validloss 1.77841±0.00000  bestvalidloss 1.55868  last_update 2\n",
      "train: iter 113  trainloss 0.85435  validloss 1.61142±0.00000  bestvalidloss 1.55868  last_update 3\n",
      "train: iter 114  trainloss 0.84841  validloss 1.62259±0.00000  bestvalidloss 1.55868  last_update 4\n",
      "train: iter 115  trainloss 0.86837  validloss 1.59760±0.00000  bestvalidloss 1.55868  last_update 5\n",
      "train: iter 116  trainloss 0.86264  validloss 1.55633±0.00000  bestvalidloss 1.55633  last_update 0\n",
      "train: iter 117  trainloss 0.82632  validloss 1.64828±0.00000  bestvalidloss 1.55633  last_update 1\n",
      "train: iter 118  trainloss 0.84085  validloss 1.46697±0.00000  bestvalidloss 1.46697  last_update 0\n",
      "train: iter 119  trainloss 0.84097  validloss 1.73179±0.00000  bestvalidloss 1.46697  last_update 1\n",
      "train: iter 120  trainloss 0.85847  validloss 1.47568±0.00000  bestvalidloss 1.46697  last_update 2\n",
      "train: iter 121  trainloss 0.83746  validloss 1.47348±0.00000  bestvalidloss 1.46697  last_update 3\n",
      "train: iter 122  trainloss 0.86394  validloss 1.52976±0.00000  bestvalidloss 1.46697  last_update 4\n",
      "train: iter 123  trainloss 0.84320  validloss 1.59073±0.00000  bestvalidloss 1.46697  last_update 5\n",
      "train: iter 124  trainloss 0.85221  validloss 1.60864±0.00000  bestvalidloss 1.46697  last_update 6\n",
      "train: iter 125  trainloss 0.82410  validloss 1.60179±0.00000  bestvalidloss 1.46697  last_update 7\n",
      "train: iter 126  trainloss 0.84656  validloss 1.66113±0.00000  bestvalidloss 1.46697  last_update 8\n",
      "train: iter 127  trainloss 0.83432  validloss 1.47678±0.00000  bestvalidloss 1.46697  last_update 9\n",
      "train: iter 128  trainloss 0.83741  validloss 1.47521±0.00000  bestvalidloss 1.46697  last_update 10\n",
      "train: iter 129  trainloss 0.82178  validloss 1.60186±0.00000  bestvalidloss 1.46697  last_update 11\n",
      "train: iter 130  trainloss 0.83487  validloss 1.57013±0.00000  bestvalidloss 1.46697  last_update 12\n",
      "train: iter 131  trainloss 0.85460  validloss 1.61749±0.00000  bestvalidloss 1.46697  last_update 13\n",
      "train: iter 132  trainloss 0.82652  validloss 1.41675±0.00000  bestvalidloss 1.41675  last_update 0\n",
      "train: iter 133  trainloss 0.83879  validloss 1.56931±0.00000  bestvalidloss 1.41675  last_update 1\n",
      "train: iter 134  trainloss 0.79661  validloss 1.67399±0.00000  bestvalidloss 1.41675  last_update 2\n",
      "train: iter 135  trainloss 0.82423  validloss 1.73540±0.00000  bestvalidloss 1.41675  last_update 3\n",
      "train: iter 136  trainloss 0.82596  validloss 1.58763±0.00000  bestvalidloss 1.41675  last_update 4\n",
      "train: iter 137  trainloss 0.81361  validloss 1.57593±0.00000  bestvalidloss 1.41675  last_update 5\n",
      "train: iter 138  trainloss 0.80727  validloss 1.64373±0.00000  bestvalidloss 1.41675  last_update 6\n",
      "train: iter 139  trainloss 0.80018  validloss 1.55063±0.00000  bestvalidloss 1.41675  last_update 7\n",
      "train: iter 140  trainloss 0.81232  validloss 1.54742±0.00000  bestvalidloss 1.41675  last_update 8\n",
      "train: iter 141  trainloss 0.84244  validloss 1.43885±0.00000  bestvalidloss 1.41675  last_update 9\n",
      "train: iter 142  trainloss 0.83730  validloss 1.56939±0.00000  bestvalidloss 1.41675  last_update 10\n",
      "train: iter 143  trainloss 0.82739  validloss 1.62407±0.00000  bestvalidloss 1.41675  last_update 11\n",
      "train: iter 144  trainloss 0.83026  validloss 1.89515±0.00000  bestvalidloss 1.41675  last_update 12\n",
      "train: iter 145  trainloss 0.82112  validloss 1.47327±0.00000  bestvalidloss 1.41675  last_update 13\n",
      "train: iter 146  trainloss 0.82115  validloss 1.63816±0.00000  bestvalidloss 1.41675  last_update 14\n",
      "train: iter 147  trainloss 0.82702  validloss 1.43459±0.00000  bestvalidloss 1.41675  last_update 15\n",
      "train: iter 148  trainloss 0.80606  validloss 1.58269±0.00000  bestvalidloss 1.41675  last_update 16\n",
      "train: iter 149  trainloss 0.83407  validloss 1.65807±0.00000  bestvalidloss 1.41675  last_update 17\n",
      "train: iter 150  trainloss 0.80537  validloss 1.59187±0.00000  bestvalidloss 1.41675  last_update 18\n",
      "train: iter 151  trainloss 0.83074  validloss 1.79287±0.00000  bestvalidloss 1.41675  last_update 19\n",
      "train: iter 152  trainloss 0.79813  validloss 1.53868±0.00000  bestvalidloss 1.41675  last_update 20\n",
      "train: iter 153  trainloss 0.85524  validloss 1.77040±0.00000  bestvalidloss 1.41675  last_update 21\n",
      "train: iter 154  trainloss 0.80678  validloss 1.50571±0.00000  bestvalidloss 1.41675  last_update 22\n",
      "train: iter 155  trainloss 0.80133  validloss 1.36786±0.00000  bestvalidloss 1.36786  last_update 0\n",
      "train: iter 156  trainloss 0.81555  validloss 1.59753±0.00000  bestvalidloss 1.36786  last_update 1\n",
      "train: iter 157  trainloss 0.82704  validloss 1.52153±0.00000  bestvalidloss 1.36786  last_update 2\n",
      "train: iter 158  trainloss 0.82094  validloss 1.68291±0.00000  bestvalidloss 1.36786  last_update 3\n",
      "train: iter 159  trainloss 0.80152  validloss 1.76306±0.00000  bestvalidloss 1.36786  last_update 4\n",
      "train: iter 160  trainloss 0.81575  validloss 1.83689±0.00000  bestvalidloss 1.36786  last_update 5\n",
      "train: iter 161  trainloss 0.80088  validloss 1.72495±0.00000  bestvalidloss 1.36786  last_update 6\n",
      "train: iter 162  trainloss 0.79673  validloss 1.78609±0.00000  bestvalidloss 1.36786  last_update 7\n",
      "train: iter 163  trainloss 0.80954  validloss 1.38046±0.00000  bestvalidloss 1.36786  last_update 8\n",
      "train: iter 164  trainloss 0.82147  validloss 1.55361±0.00000  bestvalidloss 1.36786  last_update 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 165  trainloss 0.79928  validloss 1.48485±0.00000  bestvalidloss 1.36786  last_update 10\n",
      "train: iter 166  trainloss 0.82394  validloss 1.50822±0.00000  bestvalidloss 1.36786  last_update 11\n",
      "train: iter 167  trainloss 0.80641  validloss 1.64102±0.00000  bestvalidloss 1.36786  last_update 12\n",
      "train: iter 168  trainloss 0.80471  validloss 1.56423±0.00000  bestvalidloss 1.36786  last_update 13\n",
      "train: iter 169  trainloss 0.79937  validloss 1.35235±0.00000  bestvalidloss 1.35235  last_update 0\n",
      "train: iter 170  trainloss 0.83145  validloss 1.88128±0.00000  bestvalidloss 1.35235  last_update 1\n",
      "train: iter 171  trainloss 0.82336  validloss 1.45729±0.00000  bestvalidloss 1.35235  last_update 2\n",
      "train: iter 172  trainloss 0.81754  validloss 1.52247±0.00000  bestvalidloss 1.35235  last_update 3\n",
      "train: iter 173  trainloss 0.82139  validloss 1.50836±0.00000  bestvalidloss 1.35235  last_update 4\n",
      "train: iter 174  trainloss 0.81225  validloss 1.56230±0.00000  bestvalidloss 1.35235  last_update 5\n",
      "train: iter 175  trainloss 0.80453  validloss 1.51249±0.00000  bestvalidloss 1.35235  last_update 6\n",
      "train: iter 176  trainloss 0.83105  validloss 1.75086±0.00000  bestvalidloss 1.35235  last_update 7\n",
      "train: iter 177  trainloss 0.81767  validloss 1.59733±0.00000  bestvalidloss 1.35235  last_update 8\n",
      "train: iter 178  trainloss 0.80662  validloss 1.56601±0.00000  bestvalidloss 1.35235  last_update 9\n",
      "train: iter 179  trainloss 0.79717  validloss 1.68745±0.00000  bestvalidloss 1.35235  last_update 10\n",
      "train: iter 180  trainloss 0.81082  validloss 1.59242±0.00000  bestvalidloss 1.35235  last_update 11\n",
      "train: iter 181  trainloss 0.80823  validloss 1.67804±0.00000  bestvalidloss 1.35235  last_update 12\n",
      "train: iter 182  trainloss 0.82912  validloss 1.51548±0.00000  bestvalidloss 1.35235  last_update 13\n",
      "train: iter 183  trainloss 0.80205  validloss 1.54138±0.00000  bestvalidloss 1.35235  last_update 14\n",
      "train: iter 184  trainloss 0.82769  validloss 1.46862±0.00000  bestvalidloss 1.35235  last_update 15\n",
      "train: iter 185  trainloss 0.80780  validloss 1.63713±0.00000  bestvalidloss 1.35235  last_update 16\n",
      "train: iter 186  trainloss 0.79481  validloss 1.62864±0.00000  bestvalidloss 1.35235  last_update 17\n",
      "train: iter 187  trainloss 0.82077  validloss 1.52892±0.00000  bestvalidloss 1.35235  last_update 18\n",
      "train: iter 188  trainloss 0.80907  validloss 1.54348±0.00000  bestvalidloss 1.35235  last_update 19\n",
      "train: iter 189  trainloss 0.81883  validloss 1.55029±0.00000  bestvalidloss 1.35235  last_update 20\n",
      "train: iter 190  trainloss 0.82385  validloss 1.64664±0.00000  bestvalidloss 1.35235  last_update 21\n",
      "train: iter 191  trainloss 0.79379  validloss 1.72712±0.00000  bestvalidloss 1.35235  last_update 22\n",
      "train: iter 192  trainloss 0.80923  validloss 1.54556±0.00000  bestvalidloss 1.35235  last_update 23\n",
      "train: iter 193  trainloss 0.80871  validloss 1.63671±0.00000  bestvalidloss 1.35235  last_update 24\n",
      "train: iter 194  trainloss 0.78743  validloss 1.77805±0.00000  bestvalidloss 1.35235  last_update 25\n",
      "train: iter 195  trainloss 0.79359  validloss 1.62702±0.00000  bestvalidloss 1.35235  last_update 26\n",
      "train: iter 196  trainloss 0.80345  validloss 1.66104±0.00000  bestvalidloss 1.35235  last_update 27\n",
      "train: iter 197  trainloss 0.79570  validloss 1.51779±0.00000  bestvalidloss 1.35235  last_update 28\n",
      "train: iter 198  trainloss 0.81650  validloss 1.51819±0.00000  bestvalidloss 1.35235  last_update 29\n",
      "train: iter 199  trainloss 0.82373  validloss 1.47665±0.00000  bestvalidloss 1.35235  last_update 30\n",
      "train: iter 200  trainloss 0.80588  validloss 1.47570±0.00000  bestvalidloss 1.35235  last_update 31\n",
      "train: iter 201  trainloss 0.81001  validloss 1.58537±0.00000  bestvalidloss 1.35235  last_update 32\n",
      "train: iter 202  trainloss 0.83348  validloss 1.79823±0.00000  bestvalidloss 1.35235  last_update 33\n",
      "train: iter 203  trainloss 0.81848  validloss 1.67958±0.00000  bestvalidloss 1.35235  last_update 34\n",
      "train: iter 204  trainloss 0.82833  validloss 1.77740±0.00000  bestvalidloss 1.35235  last_update 35\n",
      "train: iter 205  trainloss 0.83184  validloss 1.64471±0.00000  bestvalidloss 1.35235  last_update 36\n",
      "train: iter 206  trainloss 0.81247  validloss 1.63071±0.00000  bestvalidloss 1.35235  last_update 37\n",
      "train: iter 207  trainloss 0.81848  validloss 1.60099±0.00000  bestvalidloss 1.35235  last_update 38\n",
      "train: iter 208  trainloss 0.78993  validloss 1.65306±0.00000  bestvalidloss 1.35235  last_update 39\n",
      "train: iter 209  trainloss 0.80571  validloss 1.66425±0.00000  bestvalidloss 1.35235  last_update 40\n",
      "train: iter 210  trainloss 0.80956  validloss 1.85600±0.00000  bestvalidloss 1.35235  last_update 41\n",
      "train: iter 211  trainloss 0.84198  validloss 1.93911±0.00000  bestvalidloss 1.35235  last_update 42\n",
      "train: iter 212  trainloss 0.80871  validloss 1.81963±0.00000  bestvalidloss 1.35235  last_update 43\n",
      "train: iter 213  trainloss 0.78434  validloss 1.63968±0.00000  bestvalidloss 1.35235  last_update 44\n",
      "train: iter 214  trainloss 0.77837  validloss 1.62009±0.00000  bestvalidloss 1.35235  last_update 45\n",
      "train: iter 215  trainloss 0.78727  validloss 1.64924±0.00000  bestvalidloss 1.35235  last_update 46\n",
      "train: iter 216  trainloss 0.81850  validloss 1.36697±0.00000  bestvalidloss 1.35235  last_update 47\n",
      "train: iter 217  trainloss 0.78879  validloss 1.79756±0.00000  bestvalidloss 1.35235  last_update 48\n",
      "train: iter 218  trainloss 0.79297  validloss 1.54252±0.00000  bestvalidloss 1.35235  last_update 49\n",
      "train: iter 219  trainloss 0.80755  validloss 1.73942±0.00000  bestvalidloss 1.35235  last_update 50\n",
      "train: iter 220  trainloss 0.79498  validloss 1.67608±0.00000  bestvalidloss 1.35235  last_update 51\n",
      "train: iter 221  trainloss 0.79185  validloss 1.53028±0.00000  bestvalidloss 1.35235  last_update 52\n",
      "train: iter 222  trainloss 0.79627  validloss 1.60898±0.00000  bestvalidloss 1.35235  last_update 53\n",
      "train: iter 223  trainloss 0.80349  validloss 1.75808±0.00000  bestvalidloss 1.35235  last_update 54\n",
      "train: iter 224  trainloss 0.83850  validloss 1.69237±0.00000  bestvalidloss 1.35235  last_update 55\n",
      "train: iter 225  trainloss 0.82718  validloss 1.65357±0.00000  bestvalidloss 1.35235  last_update 56\n",
      "train: iter 226  trainloss 0.81561  validloss 1.62199±0.00000  bestvalidloss 1.35235  last_update 57\n",
      "train: iter 227  trainloss 0.77942  validloss 1.74583±0.00000  bestvalidloss 1.35235  last_update 58\n",
      "train: iter 228  trainloss 0.79647  validloss 1.66164±0.00000  bestvalidloss 1.35235  last_update 59\n",
      "train: iter 229  trainloss 0.79670  validloss 1.61178±0.00000  bestvalidloss 1.35235  last_update 60\n",
      "train: iter 230  trainloss 0.77366  validloss 1.57532±0.00000  bestvalidloss 1.35235  last_update 61\n",
      "train: iter 231  trainloss 0.77107  validloss 1.80571±0.00000  bestvalidloss 1.35235  last_update 62\n",
      "train: iter 232  trainloss 0.79645  validloss 1.68006±0.00000  bestvalidloss 1.35235  last_update 63\n",
      "train: iter 233  trainloss 0.80294  validloss 1.59981±0.00000  bestvalidloss 1.35235  last_update 64\n",
      "train: iter 234  trainloss 0.79473  validloss 1.77129±0.00000  bestvalidloss 1.35235  last_update 65\n",
      "train: iter 235  trainloss 0.78897  validloss 1.80158±0.00000  bestvalidloss 1.35235  last_update 66\n",
      "train: iter 236  trainloss 0.79468  validloss 1.51538±0.00000  bestvalidloss 1.35235  last_update 67\n",
      "train: iter 237  trainloss 0.77358  validloss 1.55886±0.00000  bestvalidloss 1.35235  last_update 68\n",
      "train: iter 238  trainloss 0.77885  validloss 1.65985±0.00000  bestvalidloss 1.35235  last_update 69\n",
      "train: iter 239  trainloss 0.78022  validloss 1.79235±0.00000  bestvalidloss 1.35235  last_update 70\n",
      "train: iter 240  trainloss 0.81064  validloss 1.69520±0.00000  bestvalidloss 1.35235  last_update 71\n",
      "train: iter 241  trainloss 0.78439  validloss 1.49336±0.00000  bestvalidloss 1.35235  last_update 72\n",
      "train: iter 242  trainloss 0.81350  validloss 1.83846±0.00000  bestvalidloss 1.35235  last_update 73\n",
      "train: iter 243  trainloss 0.83027  validloss 1.44934±0.00000  bestvalidloss 1.35235  last_update 74\n",
      "train: iter 244  trainloss 0.78004  validloss 1.57006±0.00000  bestvalidloss 1.35235  last_update 75\n",
      "train: iter 245  trainloss 0.78636  validloss 1.49164±0.00000  bestvalidloss 1.35235  last_update 76\n",
      "train: iter 246  trainloss 0.76700  validloss 1.64152±0.00000  bestvalidloss 1.35235  last_update 77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: iter 247  trainloss 0.78055  validloss 1.53534±0.00000  bestvalidloss 1.35235  last_update 78\n",
      "train: iter 248  trainloss 0.82542  validloss 1.42558±0.00000  bestvalidloss 1.35235  last_update 79\n",
      "train: iter 249  trainloss 0.82489  validloss 1.53993±0.00000  bestvalidloss 1.35235  last_update 80\n",
      "train: iter 250  trainloss 0.80035  validloss 1.50875±0.00000  bestvalidloss 1.35235  last_update 81\n",
      "train: iter 251  trainloss 0.76972  validloss 1.57638±0.00000  bestvalidloss 1.35235  last_update 82\n",
      "train: iter 252  trainloss 0.79318  validloss 1.48740±0.00000  bestvalidloss 1.35235  last_update 83\n",
      "train: iter 253  trainloss 0.78783  validloss 1.52722±0.00000  bestvalidloss 1.35235  last_update 84\n",
      "train: iter 254  trainloss 0.79238  validloss 1.56348±0.00000  bestvalidloss 1.35235  last_update 85\n",
      "train: iter 255  trainloss 0.79511  validloss 1.74273±0.00000  bestvalidloss 1.35235  last_update 86\n",
      "train: iter 256  trainloss 0.78472  validloss 1.59455±0.00000  bestvalidloss 1.35235  last_update 87\n",
      "train: iter 257  trainloss 0.80197  validloss 1.49905±0.00000  bestvalidloss 1.35235  last_update 88\n",
      "train: iter 258  trainloss 0.79450  validloss 1.65687±0.00000  bestvalidloss 1.35235  last_update 89\n",
      "train: iter 259  trainloss 0.78107  validloss 1.37600±0.00000  bestvalidloss 1.35235  last_update 90\n",
      "train: iter 260  trainloss 0.77368  validloss 1.67304±0.00000  bestvalidloss 1.35235  last_update 91\n",
      "train: iter 261  trainloss 0.81577  validloss 1.61330±0.00000  bestvalidloss 1.35235  last_update 92\n",
      "train: iter 262  trainloss 0.80191  validloss 1.70404±0.00000  bestvalidloss 1.35235  last_update 93\n",
      "train: iter 263  trainloss 0.79787  validloss 1.68423±0.00000  bestvalidloss 1.35235  last_update 94\n",
      "train: iter 264  trainloss 0.79558  validloss 1.48713±0.00000  bestvalidloss 1.35235  last_update 95\n",
      "train: iter 265  trainloss 0.81222  validloss 1.70330±0.00000  bestvalidloss 1.35235  last_update 96\n",
      "train: iter 266  trainloss 0.78872  validloss 1.74646±0.00000  bestvalidloss 1.35235  last_update 97\n",
      "train: iter 267  trainloss 0.78890  validloss 1.58842±0.00000  bestvalidloss 1.35235  last_update 98\n",
      "train: iter 268  trainloss 0.77943  validloss 1.60457±0.00000  bestvalidloss 1.35235  last_update 99\n",
      "train: iter 269  trainloss 0.79908  validloss 1.55033±0.00000  bestvalidloss 1.35235  last_update 100\n",
      "train: fin\n"
     ]
    }
   ],
   "source": [
    "train_curve, valid_curve = vi.train_penalty(num_iter=num_iter_max, lr=default_lr, early_stop_step=default_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ae20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78af8ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base save ckpt ckpt_basevi_cartpole_unweighted\n",
      "base load self.initial_belief.data.sum() tensor(-12.9482)\n",
      "base load dec.state_dict()['net_phat.0.weight'].sum() tensor(0.1062)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vi.save(ckpt_key=\"unweighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672208e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c8f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0865761186866223\n",
      "tensor([1.3300])\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility check\n",
    "print(np.random.randn())\n",
    "print(torch.randn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ef7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732366f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24744d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce713d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727f804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c69747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85609792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e850eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d322b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd8a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b9abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde31bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bbee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee0bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
